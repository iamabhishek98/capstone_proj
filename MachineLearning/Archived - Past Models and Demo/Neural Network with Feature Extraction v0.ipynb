{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.neural_network as nn\n",
    "import pickle\n",
    "from glob import glob\n",
    "from timeit import default_timer as timer\n",
    "import sys\n",
    "from GraphingAndTransfer.extractDataset import *\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "from keras.utils import np_utils,to_categorical\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import TextBox, Button, RadioButtons\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Definitions\n",
    "BASEPATH = os.getcwd()\n",
    "# SAVEFOLDER = ''\n",
    "# TESTFOLDER = ''\n",
    "# PROCESSEDFOLDER = 'processed/train/'\n",
    "# PROCESSEDFOLDER = 'processed/test/XY'\n",
    "DANCEMOVENAMES = (\"dab\",\"elbowkick\",\"gun\",\"hair\",\"listen\",\"pointhigh\",\"sidepump\",\"wipetable\")\n",
    "IS_RETURN_DATAFRAME = True\n",
    "\n",
    "#Changeable Parameters\n",
    "NUMBER_OF_AFTER_SAMPLES = 3 #Number of samples to include in dance move after end detected\n",
    "NUMBER_OF_BEFORE_SAMPLES = 5 #Number of samples to include in dance move before start detected\n",
    "MINIMUM_MOVE_TIME = 25 #Minimum number of samples to be considered a move. Set this too low and you will get garbage dance samples\n",
    "\n",
    "IS_PAD = True\n",
    "PAD_NUM =20\n",
    "\n",
    "USE_MAX_SAMPLES = True\n",
    "MAX_SAMPLES = 100\n",
    "USETEMP = True\n",
    "USEACCEL= True\n",
    "TEMPORALDATAAUGNUM = 5\n",
    "NUMRANDOMSHIFTSACCEL = 5\n",
    "Z_RAND_MAX = 0.05\n",
    "\n",
    "\n",
    "class rawDataset():\n",
    "    def __init__(self, device, movename, timestamp, dataset):\n",
    "        self.device = device\n",
    "        self.movename = movename\n",
    "        self.timestamp = timestamp\n",
    "        self.dataset = dataset\n",
    "    def plot(self):\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(3, 1, width_ratios=[1], height_ratios=[0.2,1,1])\n",
    "        index = [ x for x in range(len(self.dataset['activation_List']))]\n",
    "        # print(index)\n",
    "        # TODO plot the activation\n",
    "        activation = fig.add_subplot(gs[0])\n",
    "        accel = fig.add_subplot(gs[1])\n",
    "        gyro = fig.add_subplot(gs[2])\n",
    "\n",
    "        activation.set_title(\"Device \" + str(self.device) + \"\\nActivation\" )\n",
    "        accel.set_title(\"Accel\" )\n",
    "        gyro.set_title(\"Gyro\" )\n",
    "\n",
    "\n",
    "        ax1, = accel.plot(index, self.dataset['a_xList'], label = \"X\")\n",
    "        ax2, = accel.plot(index, self.dataset['a_yList'], label = \"Y\")\n",
    "        ax3, = accel.plot(index, self.dataset['a_zList'], label = \"Z\")\n",
    "\n",
    "        ax4, = gyro.plot(index, self.dataset['g_xList'], label = \"X\")\n",
    "        ax5, = gyro.plot(index, self.dataset['g_yList'], label = \"Y\")\n",
    "        ax6, = gyro.plot(index, self.dataset['g_zList'], label = \"Z\")\n",
    "\n",
    "        ax7, = activation.plot(index, self.dataset['activation_List'], label = \"R\")\n",
    "        \n",
    "        displaylen = len(self.dataset['activation_List'])\n",
    "\n",
    "        activation.set_xlim(xmin = 0 , xmax = displaylen )\n",
    "        activation.set_ylim(ymin = 0.2 , ymax = 2.2 )\n",
    "\n",
    "        accel.set_xlim(xmin = 0 , xmax = displaylen )\n",
    "        accel.set_ylim(ymin = -2 , ymax = 2 )\n",
    "\n",
    "        gyro.set_xlim(xmin = 0 , xmax = displaylen )\n",
    "        gyro.set_ylim(ymin = -250 , ymax = 250 )\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "class dancemove():\n",
    "    def __init__(self, device, movename, timestamp,a_xList,a_yList,a_zList,g_xList,g_yList,g_zList,activation_List ):\n",
    "        self.device = device\n",
    "        self.movename = movename\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "        self.a_xList = a_xList\n",
    "        self.a_yList = a_yList\n",
    "        self.a_zList = a_zList\n",
    "\n",
    "        self.g_xList = g_xList\n",
    "        self.g_yList = g_yList\n",
    "        self.g_zList = g_zList\n",
    "        \n",
    "        self.activation_List = activation_List\n",
    "\n",
    "    def toDict(self):\n",
    "        d = dict()\n",
    "        d['movename'] = self.movename\n",
    "        d['a_xList'] = self.a_xList\n",
    "        d['a_yList'] = self.a_yList\n",
    "        d['a_zList'] = self.a_zList\n",
    "        d['g_xList'] = self.g_xList\n",
    "        d['g_yList'] = self.g_yList\n",
    "        d['g_zList'] = self.g_zList\n",
    "        d['activation_List'] = self.activation_List\n",
    "        return d\n",
    "\n",
    "    def writeThisFile(self,moveid):\n",
    "        fname = \"{}_{}_{}_{}\".format(self.device, self.movename, self.timestamp, str(moveid))\n",
    "        f = os.path.join(BASEPATH,PROCESSEDFOLDER,fname ) \n",
    "        if not os.path.exists(os.path.dirname(f)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(f))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "        with open(f, 'w', newline='') as csvfile:\n",
    "            row = [\n",
    "                    'a_xList',\n",
    "                    'a_yList',\n",
    "                    'a_zList',\n",
    "                    'g_xList',\n",
    "                    'g_yList',\n",
    "                    'g_zList',\n",
    "                    'activation_List'\n",
    "                ]\n",
    "            csvWriter = csv.writer(csvfile, delimiter=',')\n",
    "            csvWriter.writerow(row)\n",
    "            for idx in range(len(self.a_xList)):\n",
    "                row = [\n",
    "                    self.a_xList[idx],\n",
    "                    self.a_yList[idx],\n",
    "                    self.a_zList[idx],\n",
    "                    self.g_xList[idx],\n",
    "                    self.g_yList[idx],\n",
    "                    self.g_zList[idx],\n",
    "                    self.activation_List[idx]\n",
    "                ] \n",
    "                csvWriter.writerow(row)\n",
    "\n",
    "    def plotNorm(self, show =True):\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(3, 1, width_ratios=[1], height_ratios=[0.2,1,1])\n",
    "        index = [ x for x in range(len(self.activation_List))]\n",
    "        # print(index)\n",
    "        # TODO plot the activation\n",
    "        activation = fig.add_subplot(gs[0])\n",
    "        accel = fig.add_subplot(gs[1])\n",
    "        gyro = fig.add_subplot(gs[2])\n",
    "\n",
    "        activation.set_title(\"{} {} {}\".format(self.device, self.movename, self.timestamp) )\n",
    "        accel.set_title(\"Accel\" )\n",
    "        gyro.set_title(\"Gyro\" )\n",
    "        # print( self.a_xList)\n",
    "\n",
    "        d = self.getDataAsNumpyArray( norm = True)\n",
    "        ax1, = accel.plot(index, d[0], label = \"X\")\n",
    "        ax2, = accel.plot(index, d[1], label = \"Y\")\n",
    "        ax3, = accel.plot(index, d[2], label = \"Z\")\n",
    "\n",
    "        ax4, = gyro.plot(index, d[3], label = \"X\")\n",
    "        ax5, = gyro.plot(index, d[4], label = \"Y\")\n",
    "        ax6, = gyro.plot(index, d[5], label = \"Z\")\n",
    "\n",
    "        ax7, = activation.plot(index, self.activation_List, label = \"R\")\n",
    "        \n",
    "\n",
    "        activation.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        activation.set_ylim(ymin = 0 , ymax = 2.2 )\n",
    "\n",
    "        accel.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        accel.set_ylim(ymin = 0 , ymax = 1.1 )\n",
    "\n",
    "        gyro.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        gyro.set_ylim(ymin = 0 , ymax = 1.1 )\n",
    "\n",
    "        if show:\n",
    "            plt.show(block = True)\n",
    "            plt.clf()\n",
    "\n",
    "    def plot(self, show =True):\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(3, 1, width_ratios=[1], height_ratios=[0.2,1,1])\n",
    "        index = [ x for x in range(len(self.activation_List))]\n",
    "        # print(index)\n",
    "        # TODO plot the activation\n",
    "        activation = fig.add_subplot(gs[0])\n",
    "        accel = fig.add_subplot(gs[1])\n",
    "        gyro = fig.add_subplot(gs[2])\n",
    "\n",
    "        activation.set_title(\"{} {} {}\".format(self.device, self.movename, self.timestamp) )\n",
    "        accel.set_title(\"Accel\" )\n",
    "        gyro.set_title(\"Gyro\" )\n",
    "        # print( self.a_xList)\n",
    "\n",
    "        ax1, = accel.plot(index, self.a_xList, label = \"X\")\n",
    "        ax2, = accel.plot(index, self.a_yList, label = \"Y\")\n",
    "        ax3, = accel.plot(index, self.a_zList, label = \"Z\")\n",
    "\n",
    "        ax4, = gyro.plot(index, self.g_xList, label = \"X\")\n",
    "        ax5, = gyro.plot(index, self.g_yList, label = \"Y\")\n",
    "        ax6, = gyro.plot(index, self.g_zList, label = \"Z\")\n",
    "\n",
    "        ax7, = activation.plot(index, self.activation_List, label = \"R\")\n",
    "        \n",
    "\n",
    "        activation.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        activation.set_ylim(ymin = 0.2 , ymax = 2.2 )\n",
    "\n",
    "        accel.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        accel.set_ylim(ymin = -2 , ymax = 2 )\n",
    "\n",
    "        gyro.set_xlim(xmin = 0 , xmax = len(self.activation_List) )\n",
    "        gyro.set_ylim(ymin = -250 , ymax = 250 )\n",
    "\n",
    "        if show:\n",
    "            plt.show(block = True)\n",
    "            plt.clf()\n",
    "\n",
    "    def print_Data(self):\n",
    "        print(self.activation_List)\n",
    "\n",
    "    def get_label(self):\n",
    "        return self.movename\n",
    "\n",
    "    def get_data(self):\n",
    "        return [\n",
    "            self.a_xList,\n",
    "            self.a_yList,\n",
    "            self.a_zList,\n",
    "            self.g_xList,\n",
    "            self.g_yList,\n",
    "            self.g_zList,\n",
    "            self.activation_List\n",
    "        ] \n",
    "\n",
    "    def get_data_len(self):\n",
    "        mydata = self.get_data()\n",
    "        assert all(len(x)==len(mydata[0]) for x in mydata)\n",
    "        return len(mydata[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    def getDataAsNumpyArray(self, norm = True):\n",
    "            x = np.array(self.a_xList)\n",
    "            y = np.array(self.a_yList)\n",
    "            z = np.array(self.a_zList)\n",
    "            x2 = np.array(self.g_xList)\n",
    "            y2 = np.array(self.g_yList)\n",
    "            z2 = np.array(self.g_zList)\n",
    "            a = np.array(self.activation_List)\n",
    "\n",
    "            if norm:\n",
    "                NormLimit = 1\n",
    "                x = (x+2)/4 * NormLimit\n",
    "                y = (y+2)/4 * NormLimit\n",
    "                z = (z+2)/4 * NormLimit\n",
    "\n",
    "                x2 = (x2+250)/ (250 * 2) * NormLimit\n",
    "                y2 = (y2+250)/ (250 * 2) * NormLimit\n",
    "                z2 = (z2+250)/ (250 * 2) * NormLimit\n",
    "\n",
    "                x = np.clip(x,0,NormLimit)\n",
    "                y = np.clip(y,0,NormLimit)\n",
    "                z = np.clip(z,0,NormLimit)\n",
    "\n",
    "                x2 = np.clip(x2,0,NormLimit)\n",
    "                y2 = np.clip(y2,0,NormLimit)\n",
    "                z2 = np.clip(z2,0,NormLimit)\n",
    "\n",
    "\n",
    "            \n",
    "            f = [x,y,z,x2,y2,z2]\n",
    "            return np.array(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "def listFiles(savepath):\n",
    "    filepath = os.path.join(BASEPATH, savepath)\n",
    "    return os.listdir(filepath)\n",
    "\n",
    "def extractFileMetadata(fname):\n",
    "    device, movename, timestamp = fname.split(\"_\")\n",
    "    return (device, movename, timestamp)\n",
    "\n",
    "def readRawDataset(fname, savepath):\n",
    "    filepath = os.path.join(BASEPATH, savepath, fname)\n",
    "\n",
    "    dataset = {\n",
    "        'a_xList': [],\n",
    "        'a_yList': [],\n",
    "        'a_zList': [],\n",
    "        'g_xList': [],\n",
    "        'g_yList': [],\n",
    "        'g_zList': [],\n",
    "        'activation_List': []\n",
    "    }\n",
    "    with open(filepath, 'r', newline='') as csvfile:\n",
    "        csvReader = csv.reader(csvfile, delimiter=',')\n",
    "        count = 0\n",
    "        for row in csvReader:\n",
    "            # print(row)\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            dataset['a_xList'].append(float(row[0]))\n",
    "            dataset['a_yList'].append(float(row[1]))\n",
    "            dataset['a_zList'].append(float(row[2]))\n",
    "            dataset['g_xList'].append(float(row[3]))\n",
    "            dataset['g_yList'].append(float(row[4]))\n",
    "            dataset['g_zList'].append(float(row[5]))\n",
    "            dataset['activation_List'].append(int(row[6]))\n",
    "            count += 1\n",
    "        if IS_PAD:\n",
    "            for i in range(PAD_NUM):\n",
    "                dataset['a_xList'].insert(0,dataset['a_xList'][0])\n",
    "                dataset['a_yList'].insert(0,dataset['a_yList'][0])\n",
    "                dataset['a_zList'].insert(0,dataset['a_zList'][0])\n",
    "                dataset['g_xList'].insert(0,dataset['g_xList'][0])\n",
    "                dataset['g_yList'].insert(0,dataset['g_yList'][0])\n",
    "                dataset['g_zList'].insert(0,dataset['g_zList'][0])\n",
    "                dataset['activation_List'].insert(0,dataset['activation_List'][0])\n",
    "\n",
    "                dataset['a_xList'].append(dataset['a_xList'][-1])\n",
    "                dataset['a_yList'].append(dataset['a_yList'][-1])\n",
    "                dataset['a_zList'].append(dataset['a_zList'][-1])\n",
    "                dataset['g_xList'].append(dataset['g_xList'][-1])\n",
    "                dataset['g_yList'].append(dataset['g_yList'][-1])\n",
    "                dataset['g_zList'].append(dataset['g_zList'][-1])\n",
    "                dataset['activation_List'].append(dataset['activation_List'][-1])\n",
    "\n",
    "\n",
    "        device, movename, timestamp = extractFileMetadata(fname)\n",
    "        print(\"Recording from {} with move {} at {} opened with {} samples\".format(device, movename,timestamp ,count - 1))\n",
    "\n",
    "        r = rawDataset(device, movename, timestamp, dataset)\n",
    "        return r\n",
    "\n",
    "def isolateSequences(rawdata, useAccelBaseValueAugmentation =True , useTemporalAugmentation =True):\n",
    "    moveIdxs = []\n",
    "    device = rawdata.device\n",
    "    movename = rawdata.movename\n",
    "    timestamp = rawdata.timestamp\n",
    "    d = rawdata.dataset\n",
    "\n",
    "    numberOfSamples = len(d['a_xList'])\n",
    "    isInMove=False\n",
    "    startIdx = None\n",
    "    endIdx = None\n",
    "    for idx in range(numberOfSamples):\n",
    "        currentActivation = d['activation_List'][idx]\n",
    "        if (currentActivation == 2) and (isInMove == False):\n",
    "            isInMove = True\n",
    "            startIdx = idx\n",
    "            cooldown = MINIMUM_MOVE_TIME\n",
    "        elif (isInMove == True) and (not currentActivation == 2):\n",
    "            isInMove = False\n",
    "            endIdx = idx\n",
    "            moveIdxs.append( (startIdx,endIdx ) )\n",
    "\n",
    "    movesData = []\n",
    "    for start,end in moveIdxs:\n",
    "        \n",
    "        if (end - start) < MINIMUM_MOVE_TIME:\n",
    "            continue\n",
    "\n",
    "        if USE_MAX_SAMPLES:\n",
    "            if useTemporalAugmentation:\n",
    "                for i in range(-TEMPORALDATAAUGNUM*2,(TEMPORALDATAAUGNUM+1)*2,2 ):\n",
    "                    localStart = start - NUMBER_OF_BEFORE_SAMPLES\n",
    "                    requiresPad = 0\n",
    "                    if localStart<0:\n",
    "                        requiresPad = abs(localStart)\n",
    "                        localStart = 0\n",
    "                    localEnd = localStart + MAX_SAMPLES\n",
    "                    a_xList = d['a_xList'][localStart+i:localEnd+i]\n",
    "                    a_yList = d['a_yList'][localStart+i:localEnd+i]\n",
    "                    a_zList = d['a_zList'][localStart+i:localEnd+i]\n",
    "                    g_xList = d['g_xList'][localStart+i:localEnd+i]\n",
    "                    g_yList = d['g_yList'][localStart+i:localEnd+i]\n",
    "                    g_zList = d['g_zList'][localStart+i:localEnd+i]\n",
    "                    activation_List = d['activation_List'][localStart+i:localEnd+i]\n",
    "\n",
    "                    if len(a_xList)==0:\n",
    "                        print(\"DATAERROR\")\n",
    "                        print(len(a_xList),len(a_yList),len(a_zList),len(g_xList),len(g_yList),len(g_zList) )\n",
    "                        print(len(d['a_xList']),len( d['a_yList']),len(d['a_zList']),len(d['g_xList']),len(d['g_yList']),len(d['g_zList']) )\n",
    "                        print(localStart+i,localEnd+i)\n",
    "\n",
    "                        assert False\n",
    "                    dm = dancemove(device, movename, timestamp,a_xList,a_yList,a_zList,g_xList,g_yList,g_zList,activation_List)\n",
    "                    movesData.append(dm)\n",
    "                    \n",
    "                if useAccelBaseValueAugmentation:\n",
    "                    for i in range(NUMRANDOMSHIFTSACCEL):\n",
    "                        localStart = start - NUMBER_OF_BEFORE_SAMPLES\n",
    "                        localEnd = localStart + MAX_SAMPLES\n",
    "                        z_rand = random.uniform(0, Z_RAND_MAX)\n",
    "                        y_rand = random.uniform(0, z_rand)\n",
    "                        x_rand = z_rand - y_rand\n",
    "                        a_xList = list( map( lambda x: x + x_rand, a_xList) )\n",
    "                        a_yList = list( map( lambda x: x + y_rand, a_yList) )\n",
    "                        a_zList = list( map( lambda x: x - z_rand, a_zList) )\n",
    "                        # g_xList = d['g_xList'][localStart:localEnd]\n",
    "                        # g_yList = d['g_yList'][localStart:localEnd]\n",
    "                        # g_zList = d['g_zList'][localStart:localEnd]\n",
    "                        # activation_List = d['activation_List'][localStart:localEnd]\n",
    "                        dm = dancemove(device, movename, timestamp,a_xList,a_yList,a_zList,g_xList,g_yList,g_zList,activation_List)\n",
    "                        movesData.append(dm)\n",
    "\n",
    "            if not useAccelBaseValueAugmentation and not useTemporalAugmentation:\n",
    "                    localStart = start - NUMBER_OF_BEFORE_SAMPLES\n",
    "                    localEnd = localStart + MAX_SAMPLES\n",
    "                    a_xList = d['a_xList'][localStart:localEnd]\n",
    "                    a_yList = d['a_yList'][localStart:localEnd]\n",
    "                    a_zList = d['a_zList'][localStart:localEnd]\n",
    "                    g_xList = d['g_xList'][localStart:localEnd]\n",
    "                    g_yList = d['g_yList'][localStart:localEnd]\n",
    "                    g_zList = d['g_zList'][localStart:localEnd]\n",
    "                    activation_List = d['activation_List'][localStart:localEnd]\n",
    "                    dm = dancemove(device, movename, timestamp,a_xList,a_yList,a_zList,g_xList,g_yList,g_zList,activation_List)\n",
    "                    movesData.append(dm)\n",
    "\n",
    "        else:\n",
    "            a_xList = d['a_xList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            a_yList = d['a_yList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            a_zList = d['a_zList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            g_xList = d['g_xList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            g_yList = d['g_yList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            g_zList = d['g_zList'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "            activation_List = d['activation_List'][start - NUMBER_OF_BEFORE_SAMPLES: end + NUMBER_OF_AFTER_SAMPLES ]\n",
    "\n",
    "\n",
    "            dm = dancemove(device, movename, timestamp,a_xList,a_yList,a_zList,g_xList,g_yList,g_zList,activation_List)\n",
    "            movesData.append(dm)\n",
    "    return movesData\n",
    "\n",
    "\n",
    "\n",
    "def processData(dancer, testset=False):\n",
    "    \n",
    "    if not testset:\n",
    "        SAVEFOLDER = 'dataset/raw/train/' + dancer\n",
    "        raws = list(map(lambda x:readRawDataset(x, SAVEFOLDER), listFiles(SAVEFOLDER)))\n",
    "    else:\n",
    "        TESTFOLDER = 'dataset/raw/test/' + dancer\n",
    "        raws = list(map(lambda x:readRawDataset(x ,savepath= TESTFOLDER), listFiles(savepath= TESTFOLDER)))\n",
    "\n",
    "    combinedList = []\n",
    "    numberOfMoves = 0\n",
    "    for item in raws:\n",
    "        # item.plot()\n",
    "        moves = isolateSequences(item, useTemporalAugmentation = USETEMP, useAccelBaseValueAugmentation= USEACCEL )\n",
    "        numberOfMoves += len(moves)\n",
    "        combinedList.extend(moves)\n",
    "    print(\"Done\")\n",
    "    print(\"Extracted {} moves from {} raw data Sequences.\".format(numberOfMoves, len(raws)))\n",
    "\n",
    "    numberOfEachMoves = dict.fromkeys(DANCEMOVENAMES ,0)\n",
    "    numberOfEachMoves[\"defaultMove\"] = 0\n",
    "    for item in combinedList:\n",
    "        numberOfEachMoves[item.movename] += 1\n",
    "        \n",
    "    for k,v in numberOfEachMoves.items():\n",
    "        print(\"{}  {}\".format(v, k))\n",
    "\n",
    "    # for item in combinedList:\n",
    "    #     item.plot()\n",
    "\n",
    "#     for idx,item in enumerate(combinedList):\n",
    "#         item.writeThisFile(idx)   \n",
    "  \n",
    "    if IS_RETURN_DATAFRAME:\n",
    "        frame = pd.DataFrame()\n",
    "        c = list(map(lambda x : x.toDict(),combinedList ))\n",
    "        for df in c:\n",
    "            frame = frame.append(df, ignore_index=True)\n",
    "        return frame\n",
    "\n",
    "    return combinedList\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     danceMoveDataset = processData(testset=False)\n",
    "#     print(danceMoveDataset)\n",
    "    # danceMoveDataset[0].plot()\n",
    "    # danceMoveDataset[0].plotNorm()\n",
    "\n",
    "#     a = list(filter(lambda x: x.movename == \"gun\" ,danceMoveDataset ))\n",
    "#     b = list(filter(lambda x: x.movename == \"sidepump\" ,danceMoveDataset ))\n",
    "#     c = list(filter(lambda x: x.movename == \"hair\" ,danceMoveDataset ))\n",
    "\n",
    "\n",
    "#     for i in range(5):\n",
    "#         a[i].plotNorm(show = False)\n",
    "#         b[i].plotNorm(show = False)\n",
    "#         c[i].plotNorm(show = False)\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "\n",
    "#     TODO: take not that ML guys. danceMoveDataset contains the data you want. If you want to, you can pickle this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Alex's raw signals, generate a processed and augmented dictionary dataset for model <font color=green>_training_</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", 100)\n",
    "np.set_printoptions(threshold=100) # 'sys.maxsize' for max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording from dev1 with move sidepump at 16164027138856 opened with 2108 samples\n",
      "Recording from dev1 with move gun at 16164021898786 opened with 1945 samples\n",
      "Recording from dev1 with move hair at 16164023666271 opened with 1836 samples\n",
      "Done\n",
      "Extracted 672 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "272  gun\n",
      "192  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "208  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "      <th>activation_List</th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "      <th>movename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3469256626080526, 0.33315539756483153, 0.32...</td>\n",
       "      <td>[-0.27917871527082117, -0.2800072291624927, -0...</td>\n",
       "      <td>[-0.9677316655029782, -0.9556389993017869, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1.953125, -7.8125, -9.765625, -7.8125, -1.953...</td>\n",
       "      <td>[7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...</td>\n",
       "      <td>[1.953125, -3.90625, -7.8125, -11.71875, -7.81...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.32489323853889895, 0.3199359431233394, 0.31...</td>\n",
       "      <td>[-0.29300433749749566, -0.3070526024984974, -0...</td>\n",
       "      <td>[-0.9608833995810722, -0.9640300397486432, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-9.765625, -7.8125, -1.953125, 1.953125, 3.90...</td>\n",
       "      <td>[1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...</td>\n",
       "      <td>[-7.8125, -11.71875, -7.8125, -3.90625, -3.906...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3169615658740036, 0.3151769395244022, 0.326...</td>\n",
       "      <td>[-0.31548156149909845, -0.32053893689945906, -...</td>\n",
       "      <td>[-0.972168023849186, -0.9770508143095116, -0.9...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...</td>\n",
       "      <td>[0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...</td>\n",
       "      <td>[-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.32660616371464135, 0.33971369822878483, 0.3...</td>\n",
       "      <td>[-0.31732336213967544, -0.3028940172838053, -0...</td>\n",
       "      <td>[-0.979980488585707, -0.9754882931514242, -0.9...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...</td>\n",
       "      <td>[-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...</td>\n",
       "      <td>[-3.90625, -3.90625, 1.953125, -1.953125, -5.8...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.3475782189372709, 0.3585469313623626, 0.371...</td>\n",
       "      <td>[-0.28798641037028316, -0.3040418462221699, -0...</td>\n",
       "      <td>[-0.9727929758908544, -0.9774257855345126, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...</td>\n",
       "      <td>[0.0, 1.953125, 1.953125, -1.953125, 1.953125,...</td>\n",
       "      <td>[1.953125, -1.953125, -5.859375, -21.484375, -...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>[0.7764664298905232, 0.709326574643564, 0.6127...</td>\n",
       "      <td>[-0.9541845438012653, -0.8967853472478579, -0....</td>\n",
       "      <td>[-0.9980556128599315, -1.0055054634581104, -1....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>[0.7933736931635599, 0.7262338379166007, 0.629...</td>\n",
       "      <td>[-0.9420443655173032, -0.8846451689638958, -0....</td>\n",
       "      <td>[-1.0271030544169302, -1.0345529050151092, -1....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>[0.7944767928417193, 0.7273369375947601, 0.630...</td>\n",
       "      <td>[-0.9199204988174902, -0.8625213022640827, -0....</td>\n",
       "      <td>[-1.0503300207949027, -1.0577798713930817, -1....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>[0.8002941167041476, 0.7331542614571884, 0.636...</td>\n",
       "      <td>[-0.9030247398603959, -0.8456255433069885, -0....</td>\n",
       "      <td>[-1.0730431036144252, -1.0804929542126043, -1....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>[0.8021724945760584, 0.7350326393290992, 0.638...</td>\n",
       "      <td>[-0.9024063923742267, -0.8450071958208193, -0....</td>\n",
       "      <td>[-1.0755398289725053, -1.0829896795706844, -1....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "      <td>hair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a_xList  \\\n",
       "0    [0.3469256626080526, 0.33315539756483153, 0.32...   \n",
       "1    [0.32489323853889895, 0.3199359431233394, 0.31...   \n",
       "2    [0.3169615658740036, 0.3151769395244022, 0.326...   \n",
       "3    [0.32660616371464135, 0.33971369822878483, 0.3...   \n",
       "4    [0.3475782189372709, 0.3585469313623626, 0.371...   \n",
       "..                                                 ...   \n",
       "667  [0.7764664298905232, 0.709326574643564, 0.6127...   \n",
       "668  [0.7933736931635599, 0.7262338379166007, 0.629...   \n",
       "669  [0.7944767928417193, 0.7273369375947601, 0.630...   \n",
       "670  [0.8002941167041476, 0.7331542614571884, 0.636...   \n",
       "671  [0.8021724945760584, 0.7350326393290992, 0.638...   \n",
       "\n",
       "                                               a_yList  \\\n",
       "0    [-0.27917871527082117, -0.2800072291624927, -0...   \n",
       "1    [-0.29300433749749566, -0.3070526024984974, -0...   \n",
       "2    [-0.31548156149909845, -0.32053893689945906, -...   \n",
       "3    [-0.31732336213967544, -0.3028940172838053, -0...   \n",
       "4    [-0.28798641037028316, -0.3040418462221699, -0...   \n",
       "..                                                 ...   \n",
       "667  [-0.9541845438012653, -0.8967853472478579, -0....   \n",
       "668  [-0.9420443655173032, -0.8846451689638958, -0....   \n",
       "669  [-0.9199204988174902, -0.8625213022640827, -0....   \n",
       "670  [-0.9030247398603959, -0.8456255433069885, -0....   \n",
       "671  [-0.9024063923742267, -0.8450071958208193, -0....   \n",
       "\n",
       "                                               a_zList  \\\n",
       "0    [-0.9677316655029782, -0.9556389993017869, -0....   \n",
       "1    [-0.9608833995810722, -0.9640300397486432, -0....   \n",
       "2    [-0.972168023849186, -0.9770508143095116, -0.9...   \n",
       "3    [-0.979980488585707, -0.9754882931514242, -0.9...   \n",
       "4    [-0.9727929758908544, -0.9774257855345126, -0....   \n",
       "..                                                 ...   \n",
       "667  [-0.9980556128599315, -1.0055054634581104, -1....   \n",
       "668  [-1.0271030544169302, -1.0345529050151092, -1....   \n",
       "669  [-1.0503300207949027, -1.0577798713930817, -1....   \n",
       "670  [-1.0730431036144252, -1.0804929542126043, -1....   \n",
       "671  [-1.0755398289725053, -1.0829896795706844, -1....   \n",
       "\n",
       "                                       activation_List  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "667  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "668  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "669  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "670  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "671  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                               g_xList  \\\n",
       "0    [1.953125, -7.8125, -9.765625, -7.8125, -1.953...   \n",
       "1    [-9.765625, -7.8125, -1.953125, 1.953125, 3.90...   \n",
       "2    [-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...   \n",
       "3    [3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...   \n",
       "4    [5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...   \n",
       "..                                                 ...   \n",
       "667  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "668  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "669  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "670  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "671  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "\n",
       "                                               g_yList  \\\n",
       "0    [7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...   \n",
       "1    [1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...   \n",
       "2    [0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...   \n",
       "3    [-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...   \n",
       "4    [0.0, 1.953125, 1.953125, -1.953125, 1.953125,...   \n",
       "..                                                 ...   \n",
       "667  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "668  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "669  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "670  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "671  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "\n",
       "                                               g_zList  movename  \n",
       "0    [1.953125, -3.90625, -7.8125, -11.71875, -7.81...  sidepump  \n",
       "1    [-7.8125, -11.71875, -7.8125, -3.90625, -3.906...  sidepump  \n",
       "2    [-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...  sidepump  \n",
       "3    [-3.90625, -3.90625, 1.953125, -1.953125, -5.8...  sidepump  \n",
       "4    [1.953125, -1.953125, -5.859375, -21.484375, -...  sidepump  \n",
       "..                                                 ...       ...  \n",
       "667  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...      hair  \n",
       "668  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...      hair  \n",
       "669  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...      hair  \n",
       "670  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...      hair  \n",
       "671  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...      hair  \n",
       "\n",
       "[672 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_danceMove_Alex = processData(dancer='Alex', testset=False)\n",
    "train_danceMove_Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 672 entries, 0 to 671\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   a_xList          672 non-null    object\n",
      " 1   a_yList          672 non-null    object\n",
      " 2   a_zList          672 non-null    object\n",
      " 3   activation_List  672 non-null    object\n",
      " 4   g_xList          672 non-null    object\n",
      " 5   g_yList          672 non-null    object\n",
      " 6   g_zList          672 non-null    object\n",
      " 7   movename         672 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 42.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is no NaN value in the dataset\n",
    "train_danceMove_Alex.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sense of the data\n",
    "\n",
    "For example, in <code>train_danceMove_Alex</code> above, the dataset consists of 672 training samples for three dance moves, namely, sidepump, gun and hair.\n",
    "\n",
    "In each sample, there are a total of 6 sensor signals: x-, y-, z-acceleration and x-, y-, z-gyroscope. Each of these 6 sensor signals further consists of 100 signal values saved in a <code>list</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range (1, len(train_danceMove_Alex)):\n",
    "#     train_danceMove_Alex[i].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Alex's raw signals, generate a processed and augmented dictionary dataset for model <font color=orange>_testing_</color>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording from dev1 with move gun at 16164022461072 opened with 641 samples\n",
      "Recording from dev1 with move hair at 16164024790779 opened with 822 samples\n",
      "Recording from dev1 with move sidepump at 16164027729986 opened with 811 samples\n",
      "Done\n",
      "Extracted 240 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "80  gun\n",
      "80  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "80  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "      <th>activation_List</th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "      <th>movename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.27363811114079917, 0.2829328666844795, 0.28...</td>\n",
       "      <td>[-0.2602651178071856, -0.2874090706843113, -0....</td>\n",
       "      <td>[-1.0212616738656028, -1.0190070043193615, -1....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1.953125, 7.8125, 11.71875, 17.578125, 21.484...</td>\n",
       "      <td>[5.859375, 11.71875, 13.671875, 13.671875, 13....</td>\n",
       "      <td>[-11.71875, -9.765625, -3.90625, 0.0, 1.953125...</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.28225972001068766, 0.27560583200641264, 0.2...</td>\n",
       "      <td>[-0.2974454424105868, -0.30346726544635205, -0...</td>\n",
       "      <td>[-1.0176542025916169, -1.0168425215549701, -1....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[11.71875, 17.578125, 21.484375, 21.484375, 19...</td>\n",
       "      <td>[13.671875, 13.671875, 13.671875, 9.765625, 9....</td>\n",
       "      <td>[-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2716134992038476, 0.2692180995223086, 0.267...</td>\n",
       "      <td>[-0.3008303592678112, -0.2929982155606867, -0....</td>\n",
       "      <td>[-1.0101055129329821, -0.9998133077597893, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...</td>\n",
       "      <td>[21.484375, 21.484375, 19.53125, 15.625, 9.765...</td>\n",
       "      <td>[13.671875, 9.765625, 9.765625, 5.859375, 1.95...</td>\n",
       "      <td>[1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.26778085971338517, 0.2606685158280311, 0.26...</td>\n",
       "      <td>[-0.282048929336412, -0.2692293576018472, -0.2...</td>\n",
       "      <td>[-0.9936379846558736, -0.9836827907935242, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....</td>\n",
       "      <td>[9.765625, 5.859375, 1.953125, -1.953125, -3.9...</td>\n",
       "      <td>[3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2626511094968187, 0.2700906656980912, 0.280...</td>\n",
       "      <td>[-0.2615376145611083, -0.25692256873666497, -0...</td>\n",
       "      <td>[-0.9777096744761145, -0.9678758046856686, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...</td>\n",
       "      <td>[1.953125, -1.953125, -3.90625, -5.859375, -7....</td>\n",
       "      <td>[0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[-0.11852609676780412, -0.23467214399944458, -...</td>\n",
       "      <td>[-0.8340191654520122, -0.7045961220849668, -0....</td>\n",
       "      <td>[-0.4687442141370746, -0.21975541972972312, -0...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[-0.10166211560699885, -0.21780816283863932, -...</td>\n",
       "      <td>[-0.8038048410379223, -0.6743817976708769, -0....</td>\n",
       "      <td>[-0.5158225197119698, -0.2668337253046183, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[-0.07240524239023793, -0.18855128962187842, -...</td>\n",
       "      <td>[-0.7836634691730494, -0.6542404258060041, -0....</td>\n",
       "      <td>[-0.5652207647936036, -0.3162319703862521, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[-0.0615218332282159, -0.17766788045985638, -0...</td>\n",
       "      <td>[-0.7609838072263673, -0.6315607638593219, -0....</td>\n",
       "      <td>[-0.5987838359023078, -0.34979504149495627, -0...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[-0.05984476020802341, -0.1759908074396639, -0...</td>\n",
       "      <td>[-0.7551887778703449, -0.6257657345032995, -0....</td>\n",
       "      <td>[-0.6062559382785226, -0.3572671438711712, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a_xList  \\\n",
       "0    [0.27363811114079917, 0.2829328666844795, 0.28...   \n",
       "1    [0.28225972001068766, 0.27560583200641264, 0.2...   \n",
       "2    [0.2716134992038476, 0.2692180995223086, 0.267...   \n",
       "3    [0.26778085971338517, 0.2606685158280311, 0.26...   \n",
       "4    [0.2626511094968187, 0.2700906656980912, 0.280...   \n",
       "..                                                 ...   \n",
       "235  [-0.11852609676780412, -0.23467214399944458, -...   \n",
       "236  [-0.10166211560699885, -0.21780816283863932, -...   \n",
       "237  [-0.07240524239023793, -0.18855128962187842, -...   \n",
       "238  [-0.0615218332282159, -0.17766788045985638, -0...   \n",
       "239  [-0.05984476020802341, -0.1759908074396639, -0...   \n",
       "\n",
       "                                               a_yList  \\\n",
       "0    [-0.2602651178071856, -0.2874090706843113, -0....   \n",
       "1    [-0.2974454424105868, -0.30346726544635205, -0...   \n",
       "2    [-0.3008303592678112, -0.2929982155606867, -0....   \n",
       "3    [-0.282048929336412, -0.2692293576018472, -0.2...   \n",
       "4    [-0.2615376145611083, -0.25692256873666497, -0...   \n",
       "..                                                 ...   \n",
       "235  [-0.8340191654520122, -0.7045961220849668, -0....   \n",
       "236  [-0.8038048410379223, -0.6743817976708769, -0....   \n",
       "237  [-0.7836634691730494, -0.6542404258060041, -0....   \n",
       "238  [-0.7609838072263673, -0.6315607638593219, -0....   \n",
       "239  [-0.7551887778703449, -0.6257657345032995, -0....   \n",
       "\n",
       "                                               a_zList  \\\n",
       "0    [-1.0212616738656028, -1.0190070043193615, -1....   \n",
       "1    [-1.0176542025916169, -1.0168425215549701, -1....   \n",
       "2    [-1.0101055129329821, -0.9998133077597893, -0....   \n",
       "3    [-0.9936379846558736, -0.9836827907935242, -0....   \n",
       "4    [-0.9777096744761145, -0.9678758046856686, -0....   \n",
       "..                                                 ...   \n",
       "235  [-0.4687442141370746, -0.21975541972972312, -0...   \n",
       "236  [-0.5158225197119698, -0.2668337253046183, -0....   \n",
       "237  [-0.5652207647936036, -0.3162319703862521, -0....   \n",
       "238  [-0.5987838359023078, -0.34979504149495627, -0...   \n",
       "239  [-0.6062559382785226, -0.3572671438711712, -0....   \n",
       "\n",
       "                                       activation_List  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "..                                                 ...   \n",
       "235  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "236  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "237  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "238  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "239  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                               g_xList  \\\n",
       "0    [1.953125, 7.8125, 11.71875, 17.578125, 21.484...   \n",
       "1    [11.71875, 17.578125, 21.484375, 21.484375, 19...   \n",
       "2    [21.484375, 21.484375, 19.53125, 15.625, 9.765...   \n",
       "3    [19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....   \n",
       "4    [9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...   \n",
       "..                                                 ...   \n",
       "235  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "236  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "237  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "238  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "239  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "\n",
       "                                               g_yList  \\\n",
       "0    [5.859375, 11.71875, 13.671875, 13.671875, 13....   \n",
       "1    [13.671875, 13.671875, 13.671875, 9.765625, 9....   \n",
       "2    [13.671875, 9.765625, 9.765625, 5.859375, 1.95...   \n",
       "3    [9.765625, 5.859375, 1.953125, -1.953125, -3.9...   \n",
       "4    [1.953125, -1.953125, -3.90625, -5.859375, -7....   \n",
       "..                                                 ...   \n",
       "235  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "236  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "237  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "238  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "239  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "\n",
       "                                               g_zList  movename  \n",
       "0    [-11.71875, -9.765625, -3.90625, 0.0, 1.953125...       gun  \n",
       "1    [-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...       gun  \n",
       "2    [1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...       gun  \n",
       "3    [3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...       gun  \n",
       "4    [0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...       gun  \n",
       "..                                                 ...       ...  \n",
       "235  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  sidepump  \n",
       "236  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  sidepump  \n",
       "237  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  sidepump  \n",
       "238  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  sidepump  \n",
       "239  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  sidepump  \n",
       "\n",
       "[240 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_danceMove_Alex = processData(dancer='Alex', testset=True)\n",
    "test_danceMove_Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, generate training and testing datasets from other dancers within the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "dancers = ['Alex', 'Abi', 'CJ', 'Ryan', 'XY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dancer_data_train(dancers):\n",
    "    df = pd.DataFrame()\n",
    "    for dancer in dancers:\n",
    "        print(\"\\nProcessing {}'s dance moves as training set:\" .format(dancer))\n",
    "        df_temp = processData(dancer, testset=False)\n",
    "        df_temp['Dancer'] = dancer\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)\n",
    "    return df\n",
    "    \n",
    "def dancer_data_test(dancers):\n",
    "    df = pd.DataFrame()\n",
    "    for dancer in dancers:\n",
    "        print(\"\\nProcessing {}'s dance moves as test set:\" .format(dancer))\n",
    "        df_temp = processData(dancer, testset=True)\n",
    "        df_temp['Dancer'] = dancer\n",
    "        df = pd.concat([df, df_temp], axis=0, ignore_index=True)\n",
    "    return df        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Alex's dance moves as training set:\n",
      "Recording from dev1 with move sidepump at 16164027138856 opened with 2108 samples\n",
      "Recording from dev1 with move gun at 16164021898786 opened with 1945 samples\n",
      "Recording from dev1 with move hair at 16164023666271 opened with 1836 samples\n",
      "Done\n",
      "Extracted 672 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "272  gun\n",
      "192  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "208  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing Abi's dance moves as training set:\n",
      "Recording from dev3 with move gun at 16164008859156 opened with 3107 samples\n",
      "Recording from dev3 with move sidepump at 16164005943414 opened with 3423 samples\n",
      "Recording from dev3 with move hair at 16164010930352 opened with 2308 samples\n",
      "Done\n",
      "Extracted 848 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "368  gun\n",
      "192  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "288  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing CJ's dance moves as training set:\n",
      "Recording from dev2 with move gun at 16164001747106 opened with 5092 samples\n",
      "Recording from dev2 with move sidepump at 16163990842770 opened with 2949 samples\n",
      "Recording from dev2 with move hair at 16163995267247 opened with 2769 samples\n",
      "Done\n",
      "Extracted 800 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "336  gun\n",
      "208  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "256  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing Ryan's dance moves as training set:\n",
      "Recording from dev3 with move hair at 16164017785153 opened with 2394 samples\n",
      "Recording from dev3 with move sidepump at 16164015933177 opened with 2186 samples\n",
      "Recording from dev3 with move gun at 16164013754852 opened with 1936 samples\n",
      "Done\n",
      "Extracted 592 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "176  gun\n",
      "208  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "208  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing XY's dance moves as training set:\n",
      "Recording from dev2 with move gun at 16163979052089 opened with 1875 samples\n",
      "Recording from dev2 with move hair at 16163985037577 opened with 2687 samples\n",
      "Recording from dev2 with move sidepump at 16163982054267 opened with 3233 samples\n",
      "Done\n",
      "Extracted 944 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "256  gun\n",
      "336  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "352  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n"
     ]
    }
   ],
   "source": [
    "dance_move_training = dancer_data_train(dancers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "      <th>activation_List</th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "      <th>movename</th>\n",
       "      <th>Dancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3469256626080526, 0.33315539756483153, 0.32...</td>\n",
       "      <td>[-0.27917871527082117, -0.2800072291624927, -0...</td>\n",
       "      <td>[-0.9677316655029782, -0.9556389993017869, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1.953125, -7.8125, -9.765625, -7.8125, -1.953...</td>\n",
       "      <td>[7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...</td>\n",
       "      <td>[1.953125, -3.90625, -7.8125, -11.71875, -7.81...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.32489323853889895, 0.3199359431233394, 0.31...</td>\n",
       "      <td>[-0.29300433749749566, -0.3070526024984974, -0...</td>\n",
       "      <td>[-0.9608833995810722, -0.9640300397486432, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-9.765625, -7.8125, -1.953125, 1.953125, 3.90...</td>\n",
       "      <td>[1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...</td>\n",
       "      <td>[-7.8125, -11.71875, -7.8125, -3.90625, -3.906...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3169615658740036, 0.3151769395244022, 0.326...</td>\n",
       "      <td>[-0.31548156149909845, -0.32053893689945906, -...</td>\n",
       "      <td>[-0.972168023849186, -0.9770508143095116, -0.9...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...</td>\n",
       "      <td>[0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...</td>\n",
       "      <td>[-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.32660616371464135, 0.33971369822878483, 0.3...</td>\n",
       "      <td>[-0.31732336213967544, -0.3028940172838053, -0...</td>\n",
       "      <td>[-0.979980488585707, -0.9754882931514242, -0.9...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...</td>\n",
       "      <td>[-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...</td>\n",
       "      <td>[-3.90625, -3.90625, 1.953125, -1.953125, -5.8...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.3475782189372709, 0.3585469313623626, 0.371...</td>\n",
       "      <td>[-0.28798641037028316, -0.3040418462221699, -0...</td>\n",
       "      <td>[-0.9727929758908544, -0.9774257855345126, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...</td>\n",
       "      <td>[0.0, 1.953125, 1.953125, -1.953125, 1.953125,...</td>\n",
       "      <td>[1.953125, -1.953125, -5.859375, -21.484375, -...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>[-0.018744869448308804, 0.08910740113660003, 0...</td>\n",
       "      <td>[-0.4189193352107859, -0.10859668823549677, 0....</td>\n",
       "      <td>[-1.1777913020647814, -0.8310340169354289, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-39.0625, -119.140625, -203.125, -248.046875,...</td>\n",
       "      <td>[-39.0625, 48.828125, 85.9375, 78.125, 52.7343...</td>\n",
       "      <td>[-191.40625, -144.53125, -74.21875, -11.71875,...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>[-0.012212607935742036, 0.09563966264916679, 0...</td>\n",
       "      <td>[-0.40129222100280115, -0.09096957402751202, 0...</td>\n",
       "      <td>[-1.201950677785333, -0.8551933926559804, -0.6...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-39.0625, -119.140625, -203.125, -248.046875,...</td>\n",
       "      <td>[-39.0625, 48.828125, 85.9375, 78.125, 52.7343...</td>\n",
       "      <td>[-191.40625, -144.53125, -74.21875, -11.71875,...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>[-0.01036614235073114, 0.09748612823417768, 0....</td>\n",
       "      <td>[-0.3754487348417188, -0.06512608786642968, 0....</td>\n",
       "      <td>[-1.2296406295314264, -0.8828833444020736, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-39.0625, -119.140625, -203.125, -248.046875,...</td>\n",
       "      <td>[-39.0625, 48.828125, 85.9375, 78.125, 52.7343...</td>\n",
       "      <td>[-191.40625, -144.53125, -74.21875, -11.71875,...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>[0.00650501274209591, 0.11435728332700473, 0.0...</td>\n",
       "      <td>[-0.35082232260593094, -0.04049967563064181, 0...</td>\n",
       "      <td>[-1.2711381968600413, -0.9243809117306885, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-39.0625, -119.140625, -203.125, -248.046875,...</td>\n",
       "      <td>[-39.0625, 48.828125, 85.9375, 78.125, 52.7343...</td>\n",
       "      <td>[-191.40625, -144.53125, -74.21875, -11.71875,...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>[0.02031399839605552, 0.12816626898096434, 0.0...</td>\n",
       "      <td>[-0.3317359124593369, -0.02141326548404781, 0....</td>\n",
       "      <td>[-1.3040335926605948, -0.957276307531242, -0.7...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[-39.0625, -119.140625, -203.125, -248.046875,...</td>\n",
       "      <td>[-39.0625, 48.828125, 85.9375, 78.125, 52.7343...</td>\n",
       "      <td>[-191.40625, -144.53125, -74.21875, -11.71875,...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3856 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                a_xList  \\\n",
       "0     [0.3469256626080526, 0.33315539756483153, 0.32...   \n",
       "1     [0.32489323853889895, 0.3199359431233394, 0.31...   \n",
       "2     [0.3169615658740036, 0.3151769395244022, 0.326...   \n",
       "3     [0.32660616371464135, 0.33971369822878483, 0.3...   \n",
       "4     [0.3475782189372709, 0.3585469313623626, 0.371...   \n",
       "...                                                 ...   \n",
       "3851  [-0.018744869448308804, 0.08910740113660003, 0...   \n",
       "3852  [-0.012212607935742036, 0.09563966264916679, 0...   \n",
       "3853  [-0.01036614235073114, 0.09748612823417768, 0....   \n",
       "3854  [0.00650501274209591, 0.11435728332700473, 0.0...   \n",
       "3855  [0.02031399839605552, 0.12816626898096434, 0.0...   \n",
       "\n",
       "                                                a_yList  \\\n",
       "0     [-0.27917871527082117, -0.2800072291624927, -0...   \n",
       "1     [-0.29300433749749566, -0.3070526024984974, -0...   \n",
       "2     [-0.31548156149909845, -0.32053893689945906, -...   \n",
       "3     [-0.31732336213967544, -0.3028940172838053, -0...   \n",
       "4     [-0.28798641037028316, -0.3040418462221699, -0...   \n",
       "...                                                 ...   \n",
       "3851  [-0.4189193352107859, -0.10859668823549677, 0....   \n",
       "3852  [-0.40129222100280115, -0.09096957402751202, 0...   \n",
       "3853  [-0.3754487348417188, -0.06512608786642968, 0....   \n",
       "3854  [-0.35082232260593094, -0.04049967563064181, 0...   \n",
       "3855  [-0.3317359124593369, -0.02141326548404781, 0....   \n",
       "\n",
       "                                                a_zList  \\\n",
       "0     [-0.9677316655029782, -0.9556389993017869, -0....   \n",
       "1     [-0.9608833995810722, -0.9640300397486432, -0....   \n",
       "2     [-0.972168023849186, -0.9770508143095116, -0.9...   \n",
       "3     [-0.979980488585707, -0.9754882931514242, -0.9...   \n",
       "4     [-0.9727929758908544, -0.9774257855345126, -0....   \n",
       "...                                                 ...   \n",
       "3851  [-1.1777913020647814, -0.8310340169354289, -0....   \n",
       "3852  [-1.201950677785333, -0.8551933926559804, -0.6...   \n",
       "3853  [-1.2296406295314264, -0.8828833444020736, -0....   \n",
       "3854  [-1.2711381968600413, -0.9243809117306885, -0....   \n",
       "3855  [-1.3040335926605948, -0.957276307531242, -0.7...   \n",
       "\n",
       "                                        activation_List  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                 ...   \n",
       "3851  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3852  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3853  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3854  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "3855  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                g_xList  \\\n",
       "0     [1.953125, -7.8125, -9.765625, -7.8125, -1.953...   \n",
       "1     [-9.765625, -7.8125, -1.953125, 1.953125, 3.90...   \n",
       "2     [-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...   \n",
       "3     [3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...   \n",
       "4     [5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...   \n",
       "...                                                 ...   \n",
       "3851  [-39.0625, -119.140625, -203.125, -248.046875,...   \n",
       "3852  [-39.0625, -119.140625, -203.125, -248.046875,...   \n",
       "3853  [-39.0625, -119.140625, -203.125, -248.046875,...   \n",
       "3854  [-39.0625, -119.140625, -203.125, -248.046875,...   \n",
       "3855  [-39.0625, -119.140625, -203.125, -248.046875,...   \n",
       "\n",
       "                                                g_yList  \\\n",
       "0     [7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...   \n",
       "1     [1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...   \n",
       "2     [0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...   \n",
       "3     [-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...   \n",
       "4     [0.0, 1.953125, 1.953125, -1.953125, 1.953125,...   \n",
       "...                                                 ...   \n",
       "3851  [-39.0625, 48.828125, 85.9375, 78.125, 52.7343...   \n",
       "3852  [-39.0625, 48.828125, 85.9375, 78.125, 52.7343...   \n",
       "3853  [-39.0625, 48.828125, 85.9375, 78.125, 52.7343...   \n",
       "3854  [-39.0625, 48.828125, 85.9375, 78.125, 52.7343...   \n",
       "3855  [-39.0625, 48.828125, 85.9375, 78.125, 52.7343...   \n",
       "\n",
       "                                                g_zList  movename Dancer  \n",
       "0     [1.953125, -3.90625, -7.8125, -11.71875, -7.81...  sidepump   Alex  \n",
       "1     [-7.8125, -11.71875, -7.8125, -3.90625, -3.906...  sidepump   Alex  \n",
       "2     [-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...  sidepump   Alex  \n",
       "3     [-3.90625, -3.90625, 1.953125, -1.953125, -5.8...  sidepump   Alex  \n",
       "4     [1.953125, -1.953125, -5.859375, -21.484375, -...  sidepump   Alex  \n",
       "...                                                 ...       ...    ...  \n",
       "3851  [-191.40625, -144.53125, -74.21875, -11.71875,...  sidepump     XY  \n",
       "3852  [-191.40625, -144.53125, -74.21875, -11.71875,...  sidepump     XY  \n",
       "3853  [-191.40625, -144.53125, -74.21875, -11.71875,...  sidepump     XY  \n",
       "3854  [-191.40625, -144.53125, -74.21875, -11.71875,...  sidepump     XY  \n",
       "3855  [-191.40625, -144.53125, -74.21875, -11.71875,...  sidepump     XY  \n",
       "\n",
       "[3856 rows x 9 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dance_move_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Alex's dance moves as testing set:\n",
      "Recording from dev1 with move gun at 16164022461072 opened with 641 samples\n",
      "Recording from dev1 with move hair at 16164024790779 opened with 822 samples\n",
      "Recording from dev1 with move sidepump at 16164027729986 opened with 811 samples\n",
      "Done\n",
      "Extracted 240 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "80  gun\n",
      "80  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "80  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing Abi's dance moves as testing set:\n",
      "Recording from dev3 with move sidepump at 16164007065778 opened with 1453 samples\n",
      "Recording from dev3 with move hair at 16164011636886 opened with 1120 samples\n",
      "Recording from dev3 with move gun at 16164009335203 opened with 644 samples\n",
      "Done\n",
      "Extracted 304 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "80  gun\n",
      "80  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "144  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing CJ's dance moves as testing set:\n",
      "Recording from dev2 with move sidepump at 16163993461918 opened with 2586 samples\n",
      "Recording from dev2 with move hair at 16163995995852 opened with 1270 samples\n",
      "Recording from dev2 with move gun at 16164002269679 opened with 870 samples\n",
      "Done\n",
      "Extracted 400 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "80  gun\n",
      "96  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "224  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing Ryan's dance moves as testing set:\n",
      "Recording from dev3 with move gun at 16164014283207 opened with 830 samples\n",
      "Recording from dev3 with move hair at 16164018903415 opened with 1001 samples\n",
      "Recording from dev3 with move sidepump at 16164016446985 opened with 849 samples\n",
      "Done\n",
      "Extracted 256 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "96  gun\n",
      "80  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "80  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n",
      "\n",
      "Processing XY's dance moves as testing set:\n",
      "Recording from dev2 with move gun at 16163979870987 opened with 691 samples\n",
      "Recording from dev2 with move hair at 16163985977325 opened with 1389 samples\n",
      "Recording from dev2 with move sidepump at 16163987678445 opened with 1666 samples\n",
      "Done\n",
      "Extracted 400 moves from 3 raw data Sequences.\n",
      "0  dab\n",
      "0  elbowkick\n",
      "96  gun\n",
      "144  hair\n",
      "0  listen\n",
      "0  pointhigh\n",
      "160  sidepump\n",
      "0  wipetable\n",
      "0  defaultMove\n"
     ]
    }
   ],
   "source": [
    "dance_move_testing = dancer_data_test(dancers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "      <th>activation_List</th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "      <th>movename</th>\n",
       "      <th>Dancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.27363811114079917, 0.2829328666844795, 0.28...</td>\n",
       "      <td>[-0.2602651178071856, -0.2874090706843113, -0....</td>\n",
       "      <td>[-1.0212616738656028, -1.0190070043193615, -1....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1.953125, 7.8125, 11.71875, 17.578125, 21.484...</td>\n",
       "      <td>[5.859375, 11.71875, 13.671875, 13.671875, 13....</td>\n",
       "      <td>[-11.71875, -9.765625, -3.90625, 0.0, 1.953125...</td>\n",
       "      <td>gun</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.28225972001068766, 0.27560583200641264, 0.2...</td>\n",
       "      <td>[-0.2974454424105868, -0.30346726544635205, -0...</td>\n",
       "      <td>[-1.0176542025916169, -1.0168425215549701, -1....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[11.71875, 17.578125, 21.484375, 21.484375, 19...</td>\n",
       "      <td>[13.671875, 13.671875, 13.671875, 9.765625, 9....</td>\n",
       "      <td>[-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...</td>\n",
       "      <td>gun</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2716134992038476, 0.2692180995223086, 0.267...</td>\n",
       "      <td>[-0.3008303592678112, -0.2929982155606867, -0....</td>\n",
       "      <td>[-1.0101055129329821, -0.9998133077597893, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...</td>\n",
       "      <td>[21.484375, 21.484375, 19.53125, 15.625, 9.765...</td>\n",
       "      <td>[13.671875, 9.765625, 9.765625, 5.859375, 1.95...</td>\n",
       "      <td>[1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...</td>\n",
       "      <td>gun</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.26778085971338517, 0.2606685158280311, 0.26...</td>\n",
       "      <td>[-0.282048929336412, -0.2692293576018472, -0.2...</td>\n",
       "      <td>[-0.9936379846558736, -0.9836827907935242, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....</td>\n",
       "      <td>[9.765625, 5.859375, 1.953125, -1.953125, -3.9...</td>\n",
       "      <td>[3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...</td>\n",
       "      <td>gun</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2626511094968187, 0.2700906656980912, 0.280...</td>\n",
       "      <td>[-0.2615376145611083, -0.25692256873666497, -0...</td>\n",
       "      <td>[-0.9777096744761145, -0.9678758046856686, -0....</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...</td>\n",
       "      <td>[1.953125, -1.953125, -3.90625, -5.859375, -7....</td>\n",
       "      <td>[0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...</td>\n",
       "      <td>gun</td>\n",
       "      <td>Alex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>[0.4766771639013293, 0.256322255217873, 0.0491...</td>\n",
       "      <td>[-0.2779346523634733, -0.049464193225406114, 0...</td>\n",
       "      <td>[-1.12661334495663, -0.9323305620437313, -0.70...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[103.515625, 35.15625, -56.640625, -132.8125, ...</td>\n",
       "      <td>[-39.0625, -27.34375, -52.734375, -42.96875, -...</td>\n",
       "      <td>[-35.15625, -23.4375, -25.390625, 5.859375, 19...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>[0.4863106675836003, 0.265955758900144, 0.0587...</td>\n",
       "      <td>[-0.2772625026058786, -0.04879204346781138, 0....</td>\n",
       "      <td>[-1.1369189983964958, -0.9426362154835971, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[103.515625, 35.15625, -56.640625, -132.8125, ...</td>\n",
       "      <td>[-39.0625, -27.34375, -52.734375, -42.96875, -...</td>\n",
       "      <td>[-35.15625, -23.4375, -25.390625, 5.859375, 19...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>[0.49950056008079885, 0.27914565139734254, 0.0...</td>\n",
       "      <td>[-0.25542161210694647, -0.026951152968879285, ...</td>\n",
       "      <td>[-1.1719497813926265, -0.9776669984797277, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[103.515625, 35.15625, -56.640625, -132.8125, ...</td>\n",
       "      <td>[-39.0625, -27.34375, -52.734375, -42.96875, -...</td>\n",
       "      <td>[-35.15625, -23.4375, -25.390625, 5.859375, 19...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>[0.5107319227968634, 0.29037701411340705, 0.08...</td>\n",
       "      <td>[-0.2548802820690409, -0.0264098229309737, 0.1...</td>\n",
       "      <td>[-1.1837224741465966, -0.9894396912336978, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[103.515625, 35.15625, -56.640625, -132.8125, ...</td>\n",
       "      <td>[-39.0625, -27.34375, -52.734375, -42.96875, -...</td>\n",
       "      <td>[-35.15625, -23.4375, -25.390625, 5.859375, 19...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>[0.5203908005227751, 0.30003589183931867, 0.09...</td>\n",
       "      <td>[-0.246422089661973, -0.01795163052390579, 0.1...</td>\n",
       "      <td>[-1.2018395442795762, -1.0075567613666774, -0....</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[103.515625, 35.15625, -56.640625, -132.8125, ...</td>\n",
       "      <td>[-39.0625, -27.34375, -52.734375, -42.96875, -...</td>\n",
       "      <td>[-35.15625, -23.4375, -25.390625, 5.859375, 19...</td>\n",
       "      <td>sidepump</td>\n",
       "      <td>XY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                a_xList  \\\n",
       "0     [0.27363811114079917, 0.2829328666844795, 0.28...   \n",
       "1     [0.28225972001068766, 0.27560583200641264, 0.2...   \n",
       "2     [0.2716134992038476, 0.2692180995223086, 0.267...   \n",
       "3     [0.26778085971338517, 0.2606685158280311, 0.26...   \n",
       "4     [0.2626511094968187, 0.2700906656980912, 0.280...   \n",
       "...                                                 ...   \n",
       "1595  [0.4766771639013293, 0.256322255217873, 0.0491...   \n",
       "1596  [0.4863106675836003, 0.265955758900144, 0.0587...   \n",
       "1597  [0.49950056008079885, 0.27914565139734254, 0.0...   \n",
       "1598  [0.5107319227968634, 0.29037701411340705, 0.08...   \n",
       "1599  [0.5203908005227751, 0.30003589183931867, 0.09...   \n",
       "\n",
       "                                                a_yList  \\\n",
       "0     [-0.2602651178071856, -0.2874090706843113, -0....   \n",
       "1     [-0.2974454424105868, -0.30346726544635205, -0...   \n",
       "2     [-0.3008303592678112, -0.2929982155606867, -0....   \n",
       "3     [-0.282048929336412, -0.2692293576018472, -0.2...   \n",
       "4     [-0.2615376145611083, -0.25692256873666497, -0...   \n",
       "...                                                 ...   \n",
       "1595  [-0.2779346523634733, -0.049464193225406114, 0...   \n",
       "1596  [-0.2772625026058786, -0.04879204346781138, 0....   \n",
       "1597  [-0.25542161210694647, -0.026951152968879285, ...   \n",
       "1598  [-0.2548802820690409, -0.0264098229309737, 0.1...   \n",
       "1599  [-0.246422089661973, -0.01795163052390579, 0.1...   \n",
       "\n",
       "                                                a_zList  \\\n",
       "0     [-1.0212616738656028, -1.0190070043193615, -1....   \n",
       "1     [-1.0176542025916169, -1.0168425215549701, -1....   \n",
       "2     [-1.0101055129329821, -0.9998133077597893, -0....   \n",
       "3     [-0.9936379846558736, -0.9836827907935242, -0....   \n",
       "4     [-0.9777096744761145, -0.9678758046856686, -0....   \n",
       "...                                                 ...   \n",
       "1595  [-1.12661334495663, -0.9323305620437313, -0.70...   \n",
       "1596  [-1.1369189983964958, -0.9426362154835971, -0....   \n",
       "1597  [-1.1719497813926265, -0.9776669984797277, -0....   \n",
       "1598  [-1.1837224741465966, -0.9894396912336978, -0....   \n",
       "1599  [-1.2018395442795762, -1.0075567613666774, -0....   \n",
       "\n",
       "                                        activation_List  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "...                                                 ...   \n",
       "1595  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1596  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1597  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1598  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "1599  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "\n",
       "                                                g_xList  \\\n",
       "0     [1.953125, 7.8125, 11.71875, 17.578125, 21.484...   \n",
       "1     [11.71875, 17.578125, 21.484375, 21.484375, 19...   \n",
       "2     [21.484375, 21.484375, 19.53125, 15.625, 9.765...   \n",
       "3     [19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....   \n",
       "4     [9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...   \n",
       "...                                                 ...   \n",
       "1595  [103.515625, 35.15625, -56.640625, -132.8125, ...   \n",
       "1596  [103.515625, 35.15625, -56.640625, -132.8125, ...   \n",
       "1597  [103.515625, 35.15625, -56.640625, -132.8125, ...   \n",
       "1598  [103.515625, 35.15625, -56.640625, -132.8125, ...   \n",
       "1599  [103.515625, 35.15625, -56.640625, -132.8125, ...   \n",
       "\n",
       "                                                g_yList  \\\n",
       "0     [5.859375, 11.71875, 13.671875, 13.671875, 13....   \n",
       "1     [13.671875, 13.671875, 13.671875, 9.765625, 9....   \n",
       "2     [13.671875, 9.765625, 9.765625, 5.859375, 1.95...   \n",
       "3     [9.765625, 5.859375, 1.953125, -1.953125, -3.9...   \n",
       "4     [1.953125, -1.953125, -3.90625, -5.859375, -7....   \n",
       "...                                                 ...   \n",
       "1595  [-39.0625, -27.34375, -52.734375, -42.96875, -...   \n",
       "1596  [-39.0625, -27.34375, -52.734375, -42.96875, -...   \n",
       "1597  [-39.0625, -27.34375, -52.734375, -42.96875, -...   \n",
       "1598  [-39.0625, -27.34375, -52.734375, -42.96875, -...   \n",
       "1599  [-39.0625, -27.34375, -52.734375, -42.96875, -...   \n",
       "\n",
       "                                                g_zList  movename Dancer  \n",
       "0     [-11.71875, -9.765625, -3.90625, 0.0, 1.953125...       gun   Alex  \n",
       "1     [-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...       gun   Alex  \n",
       "2     [1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...       gun   Alex  \n",
       "3     [3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...       gun   Alex  \n",
       "4     [0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...       gun   Alex  \n",
       "...                                                 ...       ...    ...  \n",
       "1595  [-35.15625, -23.4375, -25.390625, 5.859375, 19...  sidepump     XY  \n",
       "1596  [-35.15625, -23.4375, -25.390625, 5.859375, 19...  sidepump     XY  \n",
       "1597  [-35.15625, -23.4375, -25.390625, 5.859375, 19...  sidepump     XY  \n",
       "1598  [-35.15625, -23.4375, -25.390625, 5.859375, 19...  sidepump     XY  \n",
       "1599  [-35.15625, -23.4375, -25.390625, 5.859375, 19...  sidepump     XY  \n",
       "\n",
       "[1600 rows x 9 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dance_move_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_danceMove_Abi = processData(dancer='Abi', testset=False)\n",
    "# train_danceMove_CJ = processData(dancer='CJ', testset=False)\n",
    "# train_danceMove_Ryan = processData(dancer='Ryan', testset=False)\n",
    "# train_danceMove_XY = processData(dancer='XY', testset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_danceMove_Abi = processData(dancer='Abi', testset=True)\n",
    "# test_danceMove_CJ = processData(dancer='CJ', testset=True)\n",
    "# test_danceMove_Ryan = processData(dancer='Ryan', testset=True)\n",
    "# test_danceMove_XY = processData(dancer='XY', testset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.3469256626080526, 0.33315539756483153, 0.32...</td>\n",
       "      <td>[-0.27917871527082117, -0.2800072291624927, -0...</td>\n",
       "      <td>[-0.9677316655029782, -0.9556389993017869, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.32489323853889895, 0.3199359431233394, 0.31...</td>\n",
       "      <td>[-0.29300433749749566, -0.3070526024984974, -0...</td>\n",
       "      <td>[-0.9608833995810722, -0.9640300397486432, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3169615658740036, 0.3151769395244022, 0.326...</td>\n",
       "      <td>[-0.31548156149909845, -0.32053893689945906, -...</td>\n",
       "      <td>[-0.972168023849186, -0.9770508143095116, -0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.32660616371464135, 0.33971369822878483, 0.3...</td>\n",
       "      <td>[-0.31732336213967544, -0.3028940172838053, -0...</td>\n",
       "      <td>[-0.979980488585707, -0.9754882931514242, -0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.3475782189372709, 0.3585469313623626, 0.371...</td>\n",
       "      <td>[-0.28798641037028316, -0.3040418462221699, -0...</td>\n",
       "      <td>[-0.9727929758908544, -0.9774257855345126, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>[0.7764664298905232, 0.709326574643564, 0.6127...</td>\n",
       "      <td>[-0.9541845438012653, -0.8967853472478579, -0....</td>\n",
       "      <td>[-0.9980556128599315, -1.0055054634581104, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>[0.7933736931635599, 0.7262338379166007, 0.629...</td>\n",
       "      <td>[-0.9420443655173032, -0.8846451689638958, -0....</td>\n",
       "      <td>[-1.0271030544169302, -1.0345529050151092, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>[0.7944767928417193, 0.7273369375947601, 0.630...</td>\n",
       "      <td>[-0.9199204988174902, -0.8625213022640827, -0....</td>\n",
       "      <td>[-1.0503300207949027, -1.0577798713930817, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>[0.8002941167041476, 0.7331542614571884, 0.636...</td>\n",
       "      <td>[-0.9030247398603959, -0.8456255433069885, -0....</td>\n",
       "      <td>[-1.0730431036144252, -1.0804929542126043, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>[0.8021724945760584, 0.7350326393290992, 0.638...</td>\n",
       "      <td>[-0.9024063923742267, -0.8450071958208193, -0....</td>\n",
       "      <td>[-1.0755398289725053, -1.0829896795706844, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a_xList  \\\n",
       "0    [0.3469256626080526, 0.33315539756483153, 0.32...   \n",
       "1    [0.32489323853889895, 0.3199359431233394, 0.31...   \n",
       "2    [0.3169615658740036, 0.3151769395244022, 0.326...   \n",
       "3    [0.32660616371464135, 0.33971369822878483, 0.3...   \n",
       "4    [0.3475782189372709, 0.3585469313623626, 0.371...   \n",
       "..                                                 ...   \n",
       "667  [0.7764664298905232, 0.709326574643564, 0.6127...   \n",
       "668  [0.7933736931635599, 0.7262338379166007, 0.629...   \n",
       "669  [0.7944767928417193, 0.7273369375947601, 0.630...   \n",
       "670  [0.8002941167041476, 0.7331542614571884, 0.636...   \n",
       "671  [0.8021724945760584, 0.7350326393290992, 0.638...   \n",
       "\n",
       "                                               a_yList  \\\n",
       "0    [-0.27917871527082117, -0.2800072291624927, -0...   \n",
       "1    [-0.29300433749749566, -0.3070526024984974, -0...   \n",
       "2    [-0.31548156149909845, -0.32053893689945906, -...   \n",
       "3    [-0.31732336213967544, -0.3028940172838053, -0...   \n",
       "4    [-0.28798641037028316, -0.3040418462221699, -0...   \n",
       "..                                                 ...   \n",
       "667  [-0.9541845438012653, -0.8967853472478579, -0....   \n",
       "668  [-0.9420443655173032, -0.8846451689638958, -0....   \n",
       "669  [-0.9199204988174902, -0.8625213022640827, -0....   \n",
       "670  [-0.9030247398603959, -0.8456255433069885, -0....   \n",
       "671  [-0.9024063923742267, -0.8450071958208193, -0....   \n",
       "\n",
       "                                               a_zList  \n",
       "0    [-0.9677316655029782, -0.9556389993017869, -0....  \n",
       "1    [-0.9608833995810722, -0.9640300397486432, -0....  \n",
       "2    [-0.972168023849186, -0.9770508143095116, -0.9...  \n",
       "3    [-0.979980488585707, -0.9754882931514242, -0.9...  \n",
       "4    [-0.9727929758908544, -0.9774257855345126, -0....  \n",
       "..                                                 ...  \n",
       "667  [-0.9980556128599315, -1.0055054634581104, -1....  \n",
       "668  [-1.0271030544169302, -1.0345529050151092, -1....  \n",
       "669  [-1.0503300207949027, -1.0577798713930817, -1....  \n",
       "670  [-1.0730431036144252, -1.0804929542126043, -1....  \n",
       "671  [-1.0755398289725053, -1.0829896795706844, -1....  \n",
       "\n",
       "[672 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_acc_train = train_danceMove_Alex.iloc[0:, 0:3]\n",
    "xyz_acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_xList</th>\n",
       "      <th>a_yList</th>\n",
       "      <th>a_zList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.27363811114079917, 0.2829328666844795, 0.28...</td>\n",
       "      <td>[-0.2602651178071856, -0.2874090706843113, -0....</td>\n",
       "      <td>[-1.0212616738656028, -1.0190070043193615, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.28225972001068766, 0.27560583200641264, 0.2...</td>\n",
       "      <td>[-0.2974454424105868, -0.30346726544635205, -0...</td>\n",
       "      <td>[-1.0176542025916169, -1.0168425215549701, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.2716134992038476, 0.2692180995223086, 0.267...</td>\n",
       "      <td>[-0.3008303592678112, -0.2929982155606867, -0....</td>\n",
       "      <td>[-1.0101055129329821, -0.9998133077597893, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.26778085971338517, 0.2606685158280311, 0.26...</td>\n",
       "      <td>[-0.282048929336412, -0.2692293576018472, -0.2...</td>\n",
       "      <td>[-0.9936379846558736, -0.9836827907935242, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.2626511094968187, 0.2700906656980912, 0.280...</td>\n",
       "      <td>[-0.2615376145611083, -0.25692256873666497, -0...</td>\n",
       "      <td>[-0.9777096744761145, -0.9678758046856686, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[-0.11852609676780412, -0.23467214399944458, -...</td>\n",
       "      <td>[-0.8340191654520122, -0.7045961220849668, -0....</td>\n",
       "      <td>[-0.4687442141370746, -0.21975541972972312, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[-0.10166211560699885, -0.21780816283863932, -...</td>\n",
       "      <td>[-0.8038048410379223, -0.6743817976708769, -0....</td>\n",
       "      <td>[-0.5158225197119698, -0.2668337253046183, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[-0.07240524239023793, -0.18855128962187842, -...</td>\n",
       "      <td>[-0.7836634691730494, -0.6542404258060041, -0....</td>\n",
       "      <td>[-0.5652207647936036, -0.3162319703862521, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[-0.0615218332282159, -0.17766788045985638, -0...</td>\n",
       "      <td>[-0.7609838072263673, -0.6315607638593219, -0....</td>\n",
       "      <td>[-0.5987838359023078, -0.34979504149495627, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[-0.05984476020802341, -0.1759908074396639, -0...</td>\n",
       "      <td>[-0.7551887778703449, -0.6257657345032995, -0....</td>\n",
       "      <td>[-0.6062559382785226, -0.3572671438711712, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a_xList  \\\n",
       "0    [0.27363811114079917, 0.2829328666844795, 0.28...   \n",
       "1    [0.28225972001068766, 0.27560583200641264, 0.2...   \n",
       "2    [0.2716134992038476, 0.2692180995223086, 0.267...   \n",
       "3    [0.26778085971338517, 0.2606685158280311, 0.26...   \n",
       "4    [0.2626511094968187, 0.2700906656980912, 0.280...   \n",
       "..                                                 ...   \n",
       "235  [-0.11852609676780412, -0.23467214399944458, -...   \n",
       "236  [-0.10166211560699885, -0.21780816283863932, -...   \n",
       "237  [-0.07240524239023793, -0.18855128962187842, -...   \n",
       "238  [-0.0615218332282159, -0.17766788045985638, -0...   \n",
       "239  [-0.05984476020802341, -0.1759908074396639, -0...   \n",
       "\n",
       "                                               a_yList  \\\n",
       "0    [-0.2602651178071856, -0.2874090706843113, -0....   \n",
       "1    [-0.2974454424105868, -0.30346726544635205, -0...   \n",
       "2    [-0.3008303592678112, -0.2929982155606867, -0....   \n",
       "3    [-0.282048929336412, -0.2692293576018472, -0.2...   \n",
       "4    [-0.2615376145611083, -0.25692256873666497, -0...   \n",
       "..                                                 ...   \n",
       "235  [-0.8340191654520122, -0.7045961220849668, -0....   \n",
       "236  [-0.8038048410379223, -0.6743817976708769, -0....   \n",
       "237  [-0.7836634691730494, -0.6542404258060041, -0....   \n",
       "238  [-0.7609838072263673, -0.6315607638593219, -0....   \n",
       "239  [-0.7551887778703449, -0.6257657345032995, -0....   \n",
       "\n",
       "                                               a_zList  \n",
       "0    [-1.0212616738656028, -1.0190070043193615, -1....  \n",
       "1    [-1.0176542025916169, -1.0168425215549701, -1....  \n",
       "2    [-1.0101055129329821, -0.9998133077597893, -0....  \n",
       "3    [-0.9936379846558736, -0.9836827907935242, -0....  \n",
       "4    [-0.9777096744761145, -0.9678758046856686, -0....  \n",
       "..                                                 ...  \n",
       "235  [-0.4687442141370746, -0.21975541972972312, -0...  \n",
       "236  [-0.5158225197119698, -0.2668337253046183, -0....  \n",
       "237  [-0.5652207647936036, -0.3162319703862521, -0....  \n",
       "238  [-0.5987838359023078, -0.34979504149495627, -0...  \n",
       "239  [-0.6062559382785226, -0.3572671438711712, -0....  \n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_acc_test = test_danceMove_Alex.iloc[0:, 0:3]\n",
    "xyz_acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.953125, -7.8125, -9.765625, -7.8125, -1.953...</td>\n",
       "      <td>[7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...</td>\n",
       "      <td>[1.953125, -3.90625, -7.8125, -11.71875, -7.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-9.765625, -7.8125, -1.953125, 1.953125, 3.90...</td>\n",
       "      <td>[1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...</td>\n",
       "      <td>[-7.8125, -11.71875, -7.8125, -3.90625, -3.906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...</td>\n",
       "      <td>[0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...</td>\n",
       "      <td>[-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...</td>\n",
       "      <td>[-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...</td>\n",
       "      <td>[-3.90625, -3.90625, 1.953125, -1.953125, -5.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...</td>\n",
       "      <td>[0.0, 1.953125, 1.953125, -1.953125, 1.953125,...</td>\n",
       "      <td>[1.953125, -1.953125, -5.859375, -21.484375, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>[3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...</td>\n",
       "      <td>[-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....</td>\n",
       "      <td>[-46.875, -23.4375, -3.90625, 5.859375, 3.9062...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               g_xList  \\\n",
       "0    [1.953125, -7.8125, -9.765625, -7.8125, -1.953...   \n",
       "1    [-9.765625, -7.8125, -1.953125, 1.953125, 3.90...   \n",
       "2    [-1.953125, 1.953125, 3.90625, 7.8125, 5.85937...   \n",
       "3    [3.90625, 7.8125, 5.859375, 7.8125, 15.625, 27...   \n",
       "4    [5.859375, 7.8125, 15.625, 27.34375, 46.875, 6...   \n",
       "..                                                 ...   \n",
       "667  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "668  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "669  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "670  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "671  [3.90625, 54.6875, 93.75, 121.09375, 125.0, 12...   \n",
       "\n",
       "                                               g_yList  \\\n",
       "0    [7.8125, 3.90625, 1.953125, 0.0, 0.0, -1.95312...   \n",
       "1    [1.953125, 0.0, 0.0, -1.953125, -3.90625, -1.9...   \n",
       "2    [0.0, -1.953125, -3.90625, -1.953125, 0.0, 1.9...   \n",
       "3    [-3.90625, -1.953125, 0.0, 1.953125, 1.953125,...   \n",
       "4    [0.0, 1.953125, 1.953125, -1.953125, 1.953125,...   \n",
       "..                                                 ...   \n",
       "667  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "668  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "669  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "670  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "671  [-35.15625, 0.0, 7.8125, 21.484375, 15.625, 7....   \n",
       "\n",
       "                                               g_zList  \n",
       "0    [1.953125, -3.90625, -7.8125, -11.71875, -7.81...  \n",
       "1    [-7.8125, -11.71875, -7.8125, -3.90625, -3.906...  \n",
       "2    [-7.8125, -3.90625, -3.90625, -3.90625, 1.9531...  \n",
       "3    [-3.90625, -3.90625, 1.953125, -1.953125, -5.8...  \n",
       "4    [1.953125, -1.953125, -5.859375, -21.484375, -...  \n",
       "..                                                 ...  \n",
       "667  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...  \n",
       "668  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...  \n",
       "669  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...  \n",
       "670  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...  \n",
       "671  [-46.875, -23.4375, -3.90625, 5.859375, 3.9062...  \n",
       "\n",
       "[672 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_gyro_train = train_danceMove_Alex.iloc[0:, 4:7]\n",
    "xyz_gyro_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g_xList</th>\n",
       "      <th>g_yList</th>\n",
       "      <th>g_zList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.953125, 7.8125, 11.71875, 17.578125, 21.484...</td>\n",
       "      <td>[5.859375, 11.71875, 13.671875, 13.671875, 13....</td>\n",
       "      <td>[-11.71875, -9.765625, -3.90625, 0.0, 1.953125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[11.71875, 17.578125, 21.484375, 21.484375, 19...</td>\n",
       "      <td>[13.671875, 13.671875, 13.671875, 9.765625, 9....</td>\n",
       "      <td>[-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[21.484375, 21.484375, 19.53125, 15.625, 9.765...</td>\n",
       "      <td>[13.671875, 9.765625, 9.765625, 5.859375, 1.95...</td>\n",
       "      <td>[1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....</td>\n",
       "      <td>[9.765625, 5.859375, 1.953125, -1.953125, -3.9...</td>\n",
       "      <td>[3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...</td>\n",
       "      <td>[1.953125, -1.953125, -3.90625, -5.859375, -7....</td>\n",
       "      <td>[0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[54.6875, 25.390625, -11.71875, -25.390625, -3...</td>\n",
       "      <td>[-226.5625, -228.515625, -250.0, -250.0, -250....</td>\n",
       "      <td>[87.890625, 7.8125, -23.4375, -27.34375, -1.95...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               g_xList  \\\n",
       "0    [1.953125, 7.8125, 11.71875, 17.578125, 21.484...   \n",
       "1    [11.71875, 17.578125, 21.484375, 21.484375, 19...   \n",
       "2    [21.484375, 21.484375, 19.53125, 15.625, 9.765...   \n",
       "3    [19.53125, 15.625, 9.765625, 3.90625, 0.0, -1....   \n",
       "4    [9.765625, 3.90625, 0.0, -1.953125, 3.90625, 1...   \n",
       "..                                                 ...   \n",
       "235  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "236  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "237  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "238  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "239  [54.6875, 25.390625, -11.71875, -25.390625, -3...   \n",
       "\n",
       "                                               g_yList  \\\n",
       "0    [5.859375, 11.71875, 13.671875, 13.671875, 13....   \n",
       "1    [13.671875, 13.671875, 13.671875, 9.765625, 9....   \n",
       "2    [13.671875, 9.765625, 9.765625, 5.859375, 1.95...   \n",
       "3    [9.765625, 5.859375, 1.953125, -1.953125, -3.9...   \n",
       "4    [1.953125, -1.953125, -3.90625, -5.859375, -7....   \n",
       "..                                                 ...   \n",
       "235  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "236  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "237  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "238  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "239  [-226.5625, -228.515625, -250.0, -250.0, -250....   \n",
       "\n",
       "                                               g_zList  \n",
       "0    [-11.71875, -9.765625, -3.90625, 0.0, 1.953125...  \n",
       "1    [-3.90625, 0.0, 1.953125, 0.0, 3.90625, 5.8593...  \n",
       "2    [1.953125, 0.0, 3.90625, 5.859375, 0.0, -1.953...  \n",
       "3    [3.90625, 5.859375, 0.0, -1.953125, -7.8125, -...  \n",
       "4    [0.0, -1.953125, -7.8125, -7.8125, -9.765625, ...  \n",
       "..                                                 ...  \n",
       "235  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  \n",
       "236  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  \n",
       "237  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  \n",
       "238  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  \n",
       "239  [87.890625, 7.8125, -23.4375, -27.34375, -1.95...  \n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz_gyro_test = test_danceMove_Alex.iloc[0:, 4:7]\n",
    "xyz_gyro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.DataFrame()\n",
    "features_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from spectrum import aryule\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import entropy\n",
    "from math import sqrt\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(x):\n",
    "    data = []\n",
    "    data_temp = []\n",
    "    \n",
    "#     print('len x:', len(x))\n",
    "    for col in range(x.shape[1]):\n",
    "#         print('col = ', col)\n",
    "#         print(data)\n",
    "#         data.extend([])\n",
    "        for row in range(x.shape[0]):\n",
    "#             print('row =' , row)\n",
    "            data_temp.append(np.mean(x.iloc[row, col]))\n",
    "#             print(data_temp)\n",
    "            if row == x.shape[0]-1:\n",
    "#                 print(data_temp)\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "#             print(x.iloc[row, col])\n",
    "#             data.append(np.mean(x.iloc[row, col]))\n",
    "\n",
    "    return np.array(data).T\n",
    "\n",
    "def get_std(x):\n",
    "    data = []\n",
    "    data_temp = []\n",
    "    for col in range(x.shape[1]):\n",
    "        for row in range(x.shape[0]):\n",
    "            data_temp.append(np.std(x.iloc[row, col]))\n",
    "            if row == x.shape[0]-1:\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "    return np.array(data).T\n",
    "\n",
    "\n",
    "def fourier_transform(x):\n",
    "    data = []\n",
    "    data_temp = []\n",
    "    for col in range(x.shape[1]):\n",
    "        for row in range(x.shape[0]):\n",
    "            data_temp.append(np.fft.fft(xyz_acc_mean.values[row][col]))\n",
    "            if row == x.shape[0]-1:\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "    return np.array(data).T\n",
    "\n",
    "def get_mag(val):\n",
    "    data = []\n",
    "    for i in range(len(val)):\n",
    "        x, y, z = val.values[i]\n",
    "        data.append(sqrt(pow(x, 2) + pow(y, 2) + pow(z, 2)))\n",
    "    return np.array(data).T\n",
    "\n",
    "def get_kurtosis(x):\n",
    "    data = []\n",
    "    data_temp = []\n",
    "    for col in range(x.shape[1]):\n",
    "        for row in range(x.shape[0]):\n",
    "            data_temp.append(kurtosis(x.iloc[row, col]))\n",
    "            if row == x.shape[0]-1:\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "    return np.array(data).T\n",
    "\n",
    "def get_skew(x):\n",
    "    data = []\n",
    "    data_temp = []\n",
    "    for col in range(x.shape[1]):\n",
    "        for row in range(x.shape[0]):\n",
    "            data_temp.append(skew(x.iloc[row, col]))\n",
    "            if row == x.shape[0]-1:\n",
    "                data.append(data_temp)\n",
    "                data_temp = []\n",
    "    return np.array(data).T \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset):\n",
    "    xyz_acc = dataset.iloc[0:, 0:3]\n",
    "    xyz_gyro = dataset.iloc[0:, 4:7]\n",
    "    dancer = dataset.iloc[0:, 8]\n",
    "    move = dataset.iloc[0:, 7]\n",
    "    xyz_acc_mean = pd.DataFrame(get_mean(xyz_acc), columns=['acc_X_mean', 'acc_Y_mean', 'acc_Z_mean'])\n",
    "    xyz_acc_std = pd.DataFrame(get_std(xyz_acc), columns=['acc_X_std', 'acc_Y_std', 'acc_Z_std'])\n",
    "    xyz_acc_mag = pd.DataFrame(get_mag(xyz_acc_mean), columns=['acc_mag'])\n",
    "    xyz_acc_kurtosis = pd.DataFrame(get_kurtosis(xyz_acc), columns=['acc_X_kurtosis', 'acc_Y_kurtosis', 'acc_Z_kurtosis'])\n",
    "    xyz_acc_skew = pd.DataFrame(get_skew(xyz_acc), columns=['acc_X_skew', 'acc_Y_skew', 'acc_Z_skew'])\n",
    "    xyz_gyro_mean = pd.DataFrame(get_mean(xyz_gyro), columns=['gyro_X_mean', 'gyro_Y_mean', 'gyro_Z_mean'])\n",
    "    xyz_gyro_std = pd.DataFrame(get_std(xyz_gyro), columns=['gyro_X_std', 'gyro_Y_std', 'gyro_Z_std'])\n",
    "    xyz_gyro_mag = pd.DataFrame(get_mag(xyz_gyro_mean), columns=['gyro_mag'])\n",
    "    xyz_gyro_kurtosis = pd.DataFrame(get_kurtosis(xyz_gyro), columns=['gyro_X_kurtosis', 'gyro_Y_kurtosis', 'gyro_Z_kurtosis'])\n",
    "    xyz_gyro_skew = pd.DataFrame(get_skew(xyz_gyro), columns=['gyro_X_skew', 'gyro_Y_skew', 'gyro_Z_skew'])\n",
    "    \n",
    "    extracted_features = pd.concat([xyz_acc_mean, xyz_acc_std, xyz_acc_mag,\n",
    "                                    xyz_acc_kurtosis, xyz_acc_skew,\n",
    "                                    xyz_gyro_mean, xyz_gyro_std, xyz_gyro_mag,\n",
    "                                    xyz_gyro_kurtosis, xyz_gyro_skew,\n",
    "                                    dancer, move], axis=1)\n",
    "    return extracted_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_training = extract_features(dance_move_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_X_mean</th>\n",
       "      <th>acc_Y_mean</th>\n",
       "      <th>acc_Z_mean</th>\n",
       "      <th>acc_X_std</th>\n",
       "      <th>acc_Y_std</th>\n",
       "      <th>acc_Z_std</th>\n",
       "      <th>acc_mag</th>\n",
       "      <th>acc_X_kurtosis</th>\n",
       "      <th>acc_Y_kurtosis</th>\n",
       "      <th>acc_Z_kurtosis</th>\n",
       "      <th>acc_X_skew</th>\n",
       "      <th>acc_Y_skew</th>\n",
       "      <th>acc_Z_skew</th>\n",
       "      <th>gyro_X_mean</th>\n",
       "      <th>gyro_Y_mean</th>\n",
       "      <th>gyro_Z_mean</th>\n",
       "      <th>gyro_X_std</th>\n",
       "      <th>gyro_Y_std</th>\n",
       "      <th>gyro_Z_std</th>\n",
       "      <th>gyro_mag</th>\n",
       "      <th>gyro_X_kurtosis</th>\n",
       "      <th>gyro_Y_kurtosis</th>\n",
       "      <th>gyro_Z_kurtosis</th>\n",
       "      <th>gyro_X_skew</th>\n",
       "      <th>gyro_Y_skew</th>\n",
       "      <th>gyro_Z_skew</th>\n",
       "      <th>Dancer</th>\n",
       "      <th>movename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.325903</td>\n",
       "      <td>-0.519305</td>\n",
       "      <td>-0.412784</td>\n",
       "      <td>0.549820</td>\n",
       "      <td>0.447848</td>\n",
       "      <td>0.518816</td>\n",
       "      <td>0.739108</td>\n",
       "      <td>-0.354886</td>\n",
       "      <td>-0.378593</td>\n",
       "      <td>-0.715202</td>\n",
       "      <td>-0.746062</td>\n",
       "      <td>-0.279321</td>\n",
       "      <td>0.221182</td>\n",
       "      <td>-17.324219</td>\n",
       "      <td>-10.117188</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>72.215278</td>\n",
       "      <td>75.421676</td>\n",
       "      <td>75.601329</td>\n",
       "      <td>20.077261</td>\n",
       "      <td>-0.274461</td>\n",
       "      <td>1.350727</td>\n",
       "      <td>4.334908</td>\n",
       "      <td>-0.768383</td>\n",
       "      <td>-0.108224</td>\n",
       "      <td>-1.893026</td>\n",
       "      <td>Alex</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325575</td>\n",
       "      <td>-0.519750</td>\n",
       "      <td>-0.413090</td>\n",
       "      <td>0.549821</td>\n",
       "      <td>0.447622</td>\n",
       "      <td>0.519143</td>\n",
       "      <td>0.739447</td>\n",
       "      <td>-0.356695</td>\n",
       "      <td>-0.374915</td>\n",
       "      <td>-0.717553</td>\n",
       "      <td>-0.744268</td>\n",
       "      <td>-0.277540</td>\n",
       "      <td>0.220508</td>\n",
       "      <td>-17.070312</td>\n",
       "      <td>-9.941406</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>72.286536</td>\n",
       "      <td>75.468555</td>\n",
       "      <td>75.633201</td>\n",
       "      <td>19.786622</td>\n",
       "      <td>-0.273977</td>\n",
       "      <td>1.341151</td>\n",
       "      <td>4.357877</td>\n",
       "      <td>-0.775745</td>\n",
       "      <td>-0.114498</td>\n",
       "      <td>-1.904374</td>\n",
       "      <td>Alex</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324332</td>\n",
       "      <td>-0.519810</td>\n",
       "      <td>-0.413312</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.447592</td>\n",
       "      <td>0.519381</td>\n",
       "      <td>0.739067</td>\n",
       "      <td>-0.364883</td>\n",
       "      <td>-0.374436</td>\n",
       "      <td>-0.719263</td>\n",
       "      <td>-0.737206</td>\n",
       "      <td>-0.277289</td>\n",
       "      <td>0.220024</td>\n",
       "      <td>-16.914062</td>\n",
       "      <td>-9.843750</td>\n",
       "      <td>1.464844</td>\n",
       "      <td>72.312706</td>\n",
       "      <td>75.487672</td>\n",
       "      <td>75.623716</td>\n",
       "      <td>19.624747</td>\n",
       "      <td>-0.271151</td>\n",
       "      <td>1.337400</td>\n",
       "      <td>4.395134</td>\n",
       "      <td>-0.781193</td>\n",
       "      <td>-0.118137</td>\n",
       "      <td>-1.918170</td>\n",
       "      <td>Alex</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.323197</td>\n",
       "      <td>-0.519732</td>\n",
       "      <td>-0.413130</td>\n",
       "      <td>0.549975</td>\n",
       "      <td>0.447628</td>\n",
       "      <td>0.519185</td>\n",
       "      <td>0.738413</td>\n",
       "      <td>-0.372393</td>\n",
       "      <td>-0.375015</td>\n",
       "      <td>-0.717869</td>\n",
       "      <td>-0.730745</td>\n",
       "      <td>-0.277638</td>\n",
       "      <td>0.220432</td>\n",
       "      <td>-17.031250</td>\n",
       "      <td>-9.980469</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>72.289945</td>\n",
       "      <td>75.478058</td>\n",
       "      <td>75.660100</td>\n",
       "      <td>19.779680</td>\n",
       "      <td>-0.272820</td>\n",
       "      <td>1.338770</td>\n",
       "      <td>4.359282</td>\n",
       "      <td>-0.777257</td>\n",
       "      <td>-0.112780</td>\n",
       "      <td>-1.907157</td>\n",
       "      <td>Alex</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322363</td>\n",
       "      <td>-0.519991</td>\n",
       "      <td>-0.412989</td>\n",
       "      <td>0.549991</td>\n",
       "      <td>0.447509</td>\n",
       "      <td>0.519033</td>\n",
       "      <td>0.738152</td>\n",
       "      <td>-0.377106</td>\n",
       "      <td>-0.373090</td>\n",
       "      <td>-0.716802</td>\n",
       "      <td>-0.726140</td>\n",
       "      <td>-0.276487</td>\n",
       "      <td>0.220761</td>\n",
       "      <td>-17.285156</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>1.152344</td>\n",
       "      <td>72.230975</td>\n",
       "      <td>75.476736</td>\n",
       "      <td>75.676018</td>\n",
       "      <td>20.002613</td>\n",
       "      <td>-0.274995</td>\n",
       "      <td>1.338957</td>\n",
       "      <td>4.343288</td>\n",
       "      <td>-0.769209</td>\n",
       "      <td>-0.112013</td>\n",
       "      <td>-1.902188</td>\n",
       "      <td>Alex</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>0.500218</td>\n",
       "      <td>-0.054956</td>\n",
       "      <td>-0.610859</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.791446</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>0.506751</td>\n",
       "      <td>-0.037329</td>\n",
       "      <td>-0.635018</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.813288</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>0.508597</td>\n",
       "      <td>-0.011485</td>\n",
       "      <td>-0.662708</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.835455</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>0.525468</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>-0.704206</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.878746</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>0.539277</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>-0.737101</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3856 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acc_X_mean  acc_Y_mean  acc_Z_mean  acc_X_std  acc_Y_std  acc_Z_std  \\\n",
       "0       0.325903   -0.519305   -0.412784   0.549820   0.447848   0.518816   \n",
       "1       0.325575   -0.519750   -0.413090   0.549821   0.447622   0.519143   \n",
       "2       0.324332   -0.519810   -0.413312   0.549899   0.447592   0.519381   \n",
       "3       0.323197   -0.519732   -0.413130   0.549975   0.447628   0.519185   \n",
       "4       0.322363   -0.519991   -0.412989   0.549991   0.447509   0.519033   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "3851    0.500218   -0.054956   -0.610859   0.415969   0.490331   0.523213   \n",
       "3852    0.506751   -0.037329   -0.635018   0.415969   0.490331   0.523213   \n",
       "3853    0.508597   -0.011485   -0.662708   0.415969   0.490331   0.523213   \n",
       "3854    0.525468    0.013141   -0.704206   0.415969   0.490331   0.523213   \n",
       "3855    0.539277    0.032227   -0.737101   0.415969   0.490331   0.523213   \n",
       "\n",
       "       acc_mag  acc_X_kurtosis  acc_Y_kurtosis  acc_Z_kurtosis  acc_X_skew  \\\n",
       "0     0.739108       -0.354886       -0.378593       -0.715202   -0.746062   \n",
       "1     0.739447       -0.356695       -0.374915       -0.717553   -0.744268   \n",
       "2     0.739067       -0.364883       -0.374436       -0.719263   -0.737206   \n",
       "3     0.738413       -0.372393       -0.375015       -0.717869   -0.730745   \n",
       "4     0.738152       -0.377106       -0.373090       -0.716802   -0.726140   \n",
       "...        ...             ...             ...             ...         ...   \n",
       "3851  0.791446        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3852  0.813288        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3853  0.835455        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3854  0.878746        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3855  0.913880        0.662682        4.044871       -1.060848   -0.962199   \n",
       "\n",
       "      acc_Y_skew  acc_Z_skew  gyro_X_mean  gyro_Y_mean  gyro_Z_mean  \\\n",
       "0      -0.279321    0.221182   -17.324219   -10.117188     0.781250   \n",
       "1      -0.277540    0.220508   -17.070312    -9.941406     1.132812   \n",
       "2      -0.277289    0.220024   -16.914062    -9.843750     1.464844   \n",
       "3      -0.277638    0.220432   -17.031250    -9.980469     1.250000   \n",
       "4      -0.276487    0.220761   -17.285156   -10.000000     1.152344   \n",
       "...          ...         ...          ...          ...          ...   \n",
       "3851   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3852   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3853   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3854   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3855   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "\n",
       "      gyro_X_std  gyro_Y_std  gyro_Z_std   gyro_mag  gyro_X_kurtosis  \\\n",
       "0      72.215278   75.421676   75.601329  20.077261        -0.274461   \n",
       "1      72.286536   75.468555   75.633201  19.786622        -0.273977   \n",
       "2      72.312706   75.487672   75.623716  19.624747        -0.271151   \n",
       "3      72.289945   75.478058   75.660100  19.779680        -0.272820   \n",
       "4      72.230975   75.476736   75.676018  20.002613        -0.274995   \n",
       "...          ...         ...         ...        ...              ...   \n",
       "3851   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3852   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3853   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3854   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3855   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "\n",
       "      gyro_Y_kurtosis  gyro_Z_kurtosis  gyro_X_skew  gyro_Y_skew  gyro_Z_skew  \\\n",
       "0            1.350727         4.334908    -0.768383    -0.108224    -1.893026   \n",
       "1            1.341151         4.357877    -0.775745    -0.114498    -1.904374   \n",
       "2            1.337400         4.395134    -0.781193    -0.118137    -1.918170   \n",
       "3            1.338770         4.359282    -0.777257    -0.112780    -1.907157   \n",
       "4            1.338957         4.343288    -0.769209    -0.112013    -1.902188   \n",
       "...               ...              ...          ...          ...          ...   \n",
       "3851         2.393430         1.751305    -1.196690    -1.528542    -0.984029   \n",
       "3852         2.393430         1.751305    -1.196690    -1.528542    -0.984029   \n",
       "3853         2.393430         1.751305    -1.196690    -1.528542    -0.984029   \n",
       "3854         2.393430         1.751305    -1.196690    -1.528542    -0.984029   \n",
       "3855         2.393430         1.751305    -1.196690    -1.528542    -0.984029   \n",
       "\n",
       "     Dancer  movename  \n",
       "0      Alex  sidepump  \n",
       "1      Alex  sidepump  \n",
       "2      Alex  sidepump  \n",
       "3      Alex  sidepump  \n",
       "4      Alex  sidepump  \n",
       "...     ...       ...  \n",
       "3851     XY  sidepump  \n",
       "3852     XY  sidepump  \n",
       "3853     XY  sidepump  \n",
       "3854     XY  sidepump  \n",
       "3855     XY  sidepump  \n",
       "\n",
       "[3856 rows x 28 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_testing = extract_features(dance_move_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_X_mean</th>\n",
       "      <th>acc_Y_mean</th>\n",
       "      <th>acc_Z_mean</th>\n",
       "      <th>acc_X_std</th>\n",
       "      <th>acc_Y_std</th>\n",
       "      <th>acc_Z_std</th>\n",
       "      <th>acc_mag</th>\n",
       "      <th>acc_X_kurtosis</th>\n",
       "      <th>acc_Y_kurtosis</th>\n",
       "      <th>acc_Z_kurtosis</th>\n",
       "      <th>acc_X_skew</th>\n",
       "      <th>acc_Y_skew</th>\n",
       "      <th>acc_Z_skew</th>\n",
       "      <th>gyro_X_mean</th>\n",
       "      <th>gyro_Y_mean</th>\n",
       "      <th>gyro_Z_mean</th>\n",
       "      <th>gyro_X_std</th>\n",
       "      <th>gyro_Y_std</th>\n",
       "      <th>gyro_Z_std</th>\n",
       "      <th>gyro_mag</th>\n",
       "      <th>gyro_X_kurtosis</th>\n",
       "      <th>gyro_Y_kurtosis</th>\n",
       "      <th>gyro_Z_kurtosis</th>\n",
       "      <th>gyro_X_skew</th>\n",
       "      <th>gyro_Y_skew</th>\n",
       "      <th>gyro_Z_skew</th>\n",
       "      <th>Dancer</th>\n",
       "      <th>movename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.256790</td>\n",
       "      <td>-0.462554</td>\n",
       "      <td>-0.784829</td>\n",
       "      <td>0.130087</td>\n",
       "      <td>0.591135</td>\n",
       "      <td>0.371541</td>\n",
       "      <td>0.946495</td>\n",
       "      <td>-0.224492</td>\n",
       "      <td>2.002373</td>\n",
       "      <td>-0.555826</td>\n",
       "      <td>-0.382008</td>\n",
       "      <td>0.578464</td>\n",
       "      <td>0.735185</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>-2.285156</td>\n",
       "      <td>136.164216</td>\n",
       "      <td>27.500832</td>\n",
       "      <td>49.382057</td>\n",
       "      <td>6.376557</td>\n",
       "      <td>-0.071116</td>\n",
       "      <td>3.116190</td>\n",
       "      <td>2.278384</td>\n",
       "      <td>-0.150386</td>\n",
       "      <td>-1.422626</td>\n",
       "      <td>0.155111</td>\n",
       "      <td>Alex</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257769</td>\n",
       "      <td>-0.462569</td>\n",
       "      <td>-0.784113</td>\n",
       "      <td>0.130428</td>\n",
       "      <td>0.591126</td>\n",
       "      <td>0.371121</td>\n",
       "      <td>0.946176</td>\n",
       "      <td>-0.240044</td>\n",
       "      <td>2.002696</td>\n",
       "      <td>-0.551996</td>\n",
       "      <td>-0.398504</td>\n",
       "      <td>0.578550</td>\n",
       "      <td>0.733888</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>-6.152344</td>\n",
       "      <td>-2.109375</td>\n",
       "      <td>136.162653</td>\n",
       "      <td>27.423676</td>\n",
       "      <td>49.367088</td>\n",
       "      <td>6.515626</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>3.138564</td>\n",
       "      <td>2.282634</td>\n",
       "      <td>-0.149532</td>\n",
       "      <td>-1.414567</td>\n",
       "      <td>0.144675</td>\n",
       "      <td>Alex</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.258747</td>\n",
       "      <td>-0.461937</td>\n",
       "      <td>-0.783456</td>\n",
       "      <td>0.130765</td>\n",
       "      <td>0.591316</td>\n",
       "      <td>0.370737</td>\n",
       "      <td>0.945589</td>\n",
       "      <td>-0.254675</td>\n",
       "      <td>1.993933</td>\n",
       "      <td>-0.548460</td>\n",
       "      <td>-0.414879</td>\n",
       "      <td>0.575078</td>\n",
       "      <td>0.732676</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-6.464844</td>\n",
       "      <td>-2.128906</td>\n",
       "      <td>136.146788</td>\n",
       "      <td>27.284667</td>\n",
       "      <td>49.366636</td>\n",
       "      <td>6.807054</td>\n",
       "      <td>-0.071051</td>\n",
       "      <td>3.193697</td>\n",
       "      <td>2.283057</td>\n",
       "      <td>-0.143155</td>\n",
       "      <td>-1.409535</td>\n",
       "      <td>0.145865</td>\n",
       "      <td>Alex</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259899</td>\n",
       "      <td>-0.461397</td>\n",
       "      <td>-0.783044</td>\n",
       "      <td>0.131116</td>\n",
       "      <td>0.591479</td>\n",
       "      <td>0.370501</td>\n",
       "      <td>0.945301</td>\n",
       "      <td>-0.267562</td>\n",
       "      <td>1.986450</td>\n",
       "      <td>-0.546314</td>\n",
       "      <td>-0.434982</td>\n",
       "      <td>0.572115</td>\n",
       "      <td>0.731832</td>\n",
       "      <td>-0.371094</td>\n",
       "      <td>-6.738281</td>\n",
       "      <td>-2.226562</td>\n",
       "      <td>136.112690</td>\n",
       "      <td>27.167927</td>\n",
       "      <td>49.365032</td>\n",
       "      <td>7.106316</td>\n",
       "      <td>-0.070031</td>\n",
       "      <td>3.239446</td>\n",
       "      <td>2.284921</td>\n",
       "      <td>-0.133009</td>\n",
       "      <td>-1.403701</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>Alex</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.261176</td>\n",
       "      <td>-0.461428</td>\n",
       "      <td>-0.782958</td>\n",
       "      <td>0.131461</td>\n",
       "      <td>0.591469</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>-0.277500</td>\n",
       "      <td>1.986906</td>\n",
       "      <td>-0.545886</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>0.572282</td>\n",
       "      <td>0.731636</td>\n",
       "      <td>-0.761719</td>\n",
       "      <td>-6.914062</td>\n",
       "      <td>-2.363281</td>\n",
       "      <td>136.088361</td>\n",
       "      <td>27.100509</td>\n",
       "      <td>49.354425</td>\n",
       "      <td>7.346399</td>\n",
       "      <td>-0.069421</td>\n",
       "      <td>3.263453</td>\n",
       "      <td>2.291185</td>\n",
       "      <td>-0.124516</td>\n",
       "      <td>-1.397798</td>\n",
       "      <td>0.160152</td>\n",
       "      <td>Alex</td>\n",
       "      <td>gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0.359500</td>\n",
       "      <td>-0.306828</td>\n",
       "      <td>-0.581852</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.749623</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>0.369134</td>\n",
       "      <td>-0.306155</td>\n",
       "      <td>-0.592158</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.761998</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0.382324</td>\n",
       "      <td>-0.284315</td>\n",
       "      <td>-0.627188</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.787637</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0.393555</td>\n",
       "      <td>-0.283773</td>\n",
       "      <td>-0.638961</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.802299</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0.403214</td>\n",
       "      <td>-0.275315</td>\n",
       "      <td>-0.657078</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.818616</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "      <td>XY</td>\n",
       "      <td>sidepump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acc_X_mean  acc_Y_mean  acc_Z_mean  acc_X_std  acc_Y_std  acc_Z_std  \\\n",
       "0       0.256790   -0.462554   -0.784829   0.130087   0.591135   0.371541   \n",
       "1       0.257769   -0.462569   -0.784113   0.130428   0.591126   0.371121   \n",
       "2       0.258747   -0.461937   -0.783456   0.130765   0.591316   0.370737   \n",
       "3       0.259899   -0.461397   -0.783044   0.131116   0.591479   0.370501   \n",
       "4       0.261176   -0.461428   -0.782958   0.131461   0.591469   0.370453   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "1595    0.359500   -0.306828   -0.581852   0.521528   0.612409   0.574101   \n",
       "1596    0.369134   -0.306155   -0.592158   0.521528   0.612409   0.574101   \n",
       "1597    0.382324   -0.284315   -0.627188   0.521528   0.612409   0.574101   \n",
       "1598    0.393555   -0.283773   -0.638961   0.521528   0.612409   0.574101   \n",
       "1599    0.403214   -0.275315   -0.657078   0.521528   0.612409   0.574101   \n",
       "\n",
       "       acc_mag  acc_X_kurtosis  acc_Y_kurtosis  acc_Z_kurtosis  acc_X_skew  \\\n",
       "0     0.946495       -0.224492        2.002373       -0.555826   -0.382008   \n",
       "1     0.946176       -0.240044        2.002696       -0.551996   -0.398504   \n",
       "2     0.945589       -0.254675        1.993933       -0.548460   -0.414879   \n",
       "3     0.945301       -0.267562        1.986450       -0.546314   -0.434982   \n",
       "4     0.945596       -0.277500        1.986906       -0.545886   -0.457919   \n",
       "...        ...             ...             ...             ...         ...   \n",
       "1595  0.749623       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1596  0.761998       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1597  0.787637       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1598  0.802299       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1599  0.818616       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "\n",
       "      acc_Y_skew  acc_Z_skew  gyro_X_mean  gyro_Y_mean  gyro_Z_mean  \\\n",
       "0       0.578464    0.735185     0.429688    -5.937500    -2.285156   \n",
       "1       0.578550    0.733888     0.390625    -6.152344    -2.109375   \n",
       "2       0.575078    0.732676     0.097656    -6.464844    -2.128906   \n",
       "3       0.572115    0.731832    -0.371094    -6.738281    -2.226562   \n",
       "4       0.572282    0.731636    -0.761719    -6.914062    -2.363281   \n",
       "...          ...         ...          ...          ...          ...   \n",
       "1595   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1596   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1597   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1598   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1599   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "\n",
       "      gyro_X_std  gyro_Y_std  gyro_Z_std   gyro_mag  gyro_X_kurtosis  \\\n",
       "0     136.164216   27.500832   49.382057   6.376557        -0.071116   \n",
       "1     136.162653   27.423676   49.367088   6.515626        -0.071154   \n",
       "2     136.146788   27.284667   49.366636   6.807054        -0.071051   \n",
       "3     136.112690   27.167927   49.365032   7.106316        -0.070031   \n",
       "4     136.088361   27.100509   49.354425   7.346399        -0.069421   \n",
       "...          ...         ...         ...        ...              ...   \n",
       "1595   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1596   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1597   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1598   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1599   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "\n",
       "      gyro_Y_kurtosis  gyro_Z_kurtosis  gyro_X_skew  gyro_Y_skew  gyro_Z_skew  \\\n",
       "0            3.116190         2.278384    -0.150386    -1.422626     0.155111   \n",
       "1            3.138564         2.282634    -0.149532    -1.414567     0.144675   \n",
       "2            3.193697         2.283057    -0.143155    -1.409535     0.145865   \n",
       "3            3.239446         2.284921    -0.133009    -1.403701     0.151807   \n",
       "4            3.263453         2.291185    -0.124516    -1.397798     0.160152   \n",
       "...               ...              ...          ...          ...          ...   \n",
       "1595         0.457469         1.608749    -0.589082    -0.734662    -0.796596   \n",
       "1596         0.457469         1.608749    -0.589082    -0.734662    -0.796596   \n",
       "1597         0.457469         1.608749    -0.589082    -0.734662    -0.796596   \n",
       "1598         0.457469         1.608749    -0.589082    -0.734662    -0.796596   \n",
       "1599         0.457469         1.608749    -0.589082    -0.734662    -0.796596   \n",
       "\n",
       "     Dancer  movename  \n",
       "0      Alex       gun  \n",
       "1      Alex       gun  \n",
       "2      Alex       gun  \n",
       "3      Alex       gun  \n",
       "4      Alex       gun  \n",
       "...     ...       ...  \n",
       "1595     XY  sidepump  \n",
       "1596     XY  sidepump  \n",
       "1597     XY  sidepump  \n",
       "1598     XY  sidepump  \n",
       "1599     XY  sidepump  \n",
       "\n",
       "[1600 rows x 28 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_X_mean</th>\n",
       "      <th>acc_Y_mean</th>\n",
       "      <th>acc_Z_mean</th>\n",
       "      <th>acc_X_std</th>\n",
       "      <th>acc_Y_std</th>\n",
       "      <th>acc_Z_std</th>\n",
       "      <th>acc_mag</th>\n",
       "      <th>acc_X_kurtosis</th>\n",
       "      <th>acc_Y_kurtosis</th>\n",
       "      <th>acc_Z_kurtosis</th>\n",
       "      <th>acc_X_skew</th>\n",
       "      <th>acc_Y_skew</th>\n",
       "      <th>acc_Z_skew</th>\n",
       "      <th>gyro_X_mean</th>\n",
       "      <th>gyro_Y_mean</th>\n",
       "      <th>gyro_Z_mean</th>\n",
       "      <th>gyro_X_std</th>\n",
       "      <th>gyro_Y_std</th>\n",
       "      <th>gyro_Z_std</th>\n",
       "      <th>gyro_mag</th>\n",
       "      <th>gyro_X_kurtosis</th>\n",
       "      <th>gyro_Y_kurtosis</th>\n",
       "      <th>gyro_Z_kurtosis</th>\n",
       "      <th>gyro_X_skew</th>\n",
       "      <th>gyro_Y_skew</th>\n",
       "      <th>gyro_Z_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.325903</td>\n",
       "      <td>-0.519305</td>\n",
       "      <td>-0.412784</td>\n",
       "      <td>0.549820</td>\n",
       "      <td>0.447848</td>\n",
       "      <td>0.518816</td>\n",
       "      <td>0.739108</td>\n",
       "      <td>-0.354886</td>\n",
       "      <td>-0.378593</td>\n",
       "      <td>-0.715202</td>\n",
       "      <td>-0.746062</td>\n",
       "      <td>-0.279321</td>\n",
       "      <td>0.221182</td>\n",
       "      <td>-17.324219</td>\n",
       "      <td>-10.117188</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>72.215278</td>\n",
       "      <td>75.421676</td>\n",
       "      <td>75.601329</td>\n",
       "      <td>20.077261</td>\n",
       "      <td>-0.274461</td>\n",
       "      <td>1.350727</td>\n",
       "      <td>4.334908</td>\n",
       "      <td>-0.768383</td>\n",
       "      <td>-0.108224</td>\n",
       "      <td>-1.893026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325575</td>\n",
       "      <td>-0.519750</td>\n",
       "      <td>-0.413090</td>\n",
       "      <td>0.549821</td>\n",
       "      <td>0.447622</td>\n",
       "      <td>0.519143</td>\n",
       "      <td>0.739447</td>\n",
       "      <td>-0.356695</td>\n",
       "      <td>-0.374915</td>\n",
       "      <td>-0.717553</td>\n",
       "      <td>-0.744268</td>\n",
       "      <td>-0.277540</td>\n",
       "      <td>0.220508</td>\n",
       "      <td>-17.070312</td>\n",
       "      <td>-9.941406</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>72.286536</td>\n",
       "      <td>75.468555</td>\n",
       "      <td>75.633201</td>\n",
       "      <td>19.786622</td>\n",
       "      <td>-0.273977</td>\n",
       "      <td>1.341151</td>\n",
       "      <td>4.357877</td>\n",
       "      <td>-0.775745</td>\n",
       "      <td>-0.114498</td>\n",
       "      <td>-1.904374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324332</td>\n",
       "      <td>-0.519810</td>\n",
       "      <td>-0.413312</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.447592</td>\n",
       "      <td>0.519381</td>\n",
       "      <td>0.739067</td>\n",
       "      <td>-0.364883</td>\n",
       "      <td>-0.374436</td>\n",
       "      <td>-0.719263</td>\n",
       "      <td>-0.737206</td>\n",
       "      <td>-0.277289</td>\n",
       "      <td>0.220024</td>\n",
       "      <td>-16.914062</td>\n",
       "      <td>-9.843750</td>\n",
       "      <td>1.464844</td>\n",
       "      <td>72.312706</td>\n",
       "      <td>75.487672</td>\n",
       "      <td>75.623716</td>\n",
       "      <td>19.624747</td>\n",
       "      <td>-0.271151</td>\n",
       "      <td>1.337400</td>\n",
       "      <td>4.395134</td>\n",
       "      <td>-0.781193</td>\n",
       "      <td>-0.118137</td>\n",
       "      <td>-1.918170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.323197</td>\n",
       "      <td>-0.519732</td>\n",
       "      <td>-0.413130</td>\n",
       "      <td>0.549975</td>\n",
       "      <td>0.447628</td>\n",
       "      <td>0.519185</td>\n",
       "      <td>0.738413</td>\n",
       "      <td>-0.372393</td>\n",
       "      <td>-0.375015</td>\n",
       "      <td>-0.717869</td>\n",
       "      <td>-0.730745</td>\n",
       "      <td>-0.277638</td>\n",
       "      <td>0.220432</td>\n",
       "      <td>-17.031250</td>\n",
       "      <td>-9.980469</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>72.289945</td>\n",
       "      <td>75.478058</td>\n",
       "      <td>75.660100</td>\n",
       "      <td>19.779680</td>\n",
       "      <td>-0.272820</td>\n",
       "      <td>1.338770</td>\n",
       "      <td>4.359282</td>\n",
       "      <td>-0.777257</td>\n",
       "      <td>-0.112780</td>\n",
       "      <td>-1.907157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.322363</td>\n",
       "      <td>-0.519991</td>\n",
       "      <td>-0.412989</td>\n",
       "      <td>0.549991</td>\n",
       "      <td>0.447509</td>\n",
       "      <td>0.519033</td>\n",
       "      <td>0.738152</td>\n",
       "      <td>-0.377106</td>\n",
       "      <td>-0.373090</td>\n",
       "      <td>-0.716802</td>\n",
       "      <td>-0.726140</td>\n",
       "      <td>-0.276487</td>\n",
       "      <td>0.220761</td>\n",
       "      <td>-17.285156</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>1.152344</td>\n",
       "      <td>72.230975</td>\n",
       "      <td>75.476736</td>\n",
       "      <td>75.676018</td>\n",
       "      <td>20.002613</td>\n",
       "      <td>-0.274995</td>\n",
       "      <td>1.338957</td>\n",
       "      <td>4.343288</td>\n",
       "      <td>-0.769209</td>\n",
       "      <td>-0.112013</td>\n",
       "      <td>-1.902188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>0.500218</td>\n",
       "      <td>-0.054956</td>\n",
       "      <td>-0.610859</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.791446</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>0.506751</td>\n",
       "      <td>-0.037329</td>\n",
       "      <td>-0.635018</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.813288</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>0.508597</td>\n",
       "      <td>-0.011485</td>\n",
       "      <td>-0.662708</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.835455</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>0.525468</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>-0.704206</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.878746</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>0.539277</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>-0.737101</td>\n",
       "      <td>0.415969</td>\n",
       "      <td>0.490331</td>\n",
       "      <td>0.523213</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>4.044871</td>\n",
       "      <td>-1.060848</td>\n",
       "      <td>-0.962199</td>\n",
       "      <td>-2.126935</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-26.582031</td>\n",
       "      <td>-10.625000</td>\n",
       "      <td>8.945312</td>\n",
       "      <td>84.802472</td>\n",
       "      <td>51.425689</td>\n",
       "      <td>94.940030</td>\n",
       "      <td>29.991893</td>\n",
       "      <td>1.980038</td>\n",
       "      <td>2.393430</td>\n",
       "      <td>1.751305</td>\n",
       "      <td>-1.196690</td>\n",
       "      <td>-1.528542</td>\n",
       "      <td>-0.984029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3856 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acc_X_mean  acc_Y_mean  acc_Z_mean  acc_X_std  acc_Y_std  acc_Z_std  \\\n",
       "0       0.325903   -0.519305   -0.412784   0.549820   0.447848   0.518816   \n",
       "1       0.325575   -0.519750   -0.413090   0.549821   0.447622   0.519143   \n",
       "2       0.324332   -0.519810   -0.413312   0.549899   0.447592   0.519381   \n",
       "3       0.323197   -0.519732   -0.413130   0.549975   0.447628   0.519185   \n",
       "4       0.322363   -0.519991   -0.412989   0.549991   0.447509   0.519033   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "3851    0.500218   -0.054956   -0.610859   0.415969   0.490331   0.523213   \n",
       "3852    0.506751   -0.037329   -0.635018   0.415969   0.490331   0.523213   \n",
       "3853    0.508597   -0.011485   -0.662708   0.415969   0.490331   0.523213   \n",
       "3854    0.525468    0.013141   -0.704206   0.415969   0.490331   0.523213   \n",
       "3855    0.539277    0.032227   -0.737101   0.415969   0.490331   0.523213   \n",
       "\n",
       "       acc_mag  acc_X_kurtosis  acc_Y_kurtosis  acc_Z_kurtosis  acc_X_skew  \\\n",
       "0     0.739108       -0.354886       -0.378593       -0.715202   -0.746062   \n",
       "1     0.739447       -0.356695       -0.374915       -0.717553   -0.744268   \n",
       "2     0.739067       -0.364883       -0.374436       -0.719263   -0.737206   \n",
       "3     0.738413       -0.372393       -0.375015       -0.717869   -0.730745   \n",
       "4     0.738152       -0.377106       -0.373090       -0.716802   -0.726140   \n",
       "...        ...             ...             ...             ...         ...   \n",
       "3851  0.791446        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3852  0.813288        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3853  0.835455        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3854  0.878746        0.662682        4.044871       -1.060848   -0.962199   \n",
       "3855  0.913880        0.662682        4.044871       -1.060848   -0.962199   \n",
       "\n",
       "      acc_Y_skew  acc_Z_skew  gyro_X_mean  gyro_Y_mean  gyro_Z_mean  \\\n",
       "0      -0.279321    0.221182   -17.324219   -10.117188     0.781250   \n",
       "1      -0.277540    0.220508   -17.070312    -9.941406     1.132812   \n",
       "2      -0.277289    0.220024   -16.914062    -9.843750     1.464844   \n",
       "3      -0.277638    0.220432   -17.031250    -9.980469     1.250000   \n",
       "4      -0.276487    0.220761   -17.285156   -10.000000     1.152344   \n",
       "...          ...         ...          ...          ...          ...   \n",
       "3851   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3852   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3853   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3854   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "3855   -2.126935   -0.109458   -26.582031   -10.625000     8.945312   \n",
       "\n",
       "      gyro_X_std  gyro_Y_std  gyro_Z_std   gyro_mag  gyro_X_kurtosis  \\\n",
       "0      72.215278   75.421676   75.601329  20.077261        -0.274461   \n",
       "1      72.286536   75.468555   75.633201  19.786622        -0.273977   \n",
       "2      72.312706   75.487672   75.623716  19.624747        -0.271151   \n",
       "3      72.289945   75.478058   75.660100  19.779680        -0.272820   \n",
       "4      72.230975   75.476736   75.676018  20.002613        -0.274995   \n",
       "...          ...         ...         ...        ...              ...   \n",
       "3851   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3852   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3853   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3854   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "3855   84.802472   51.425689   94.940030  29.991893         1.980038   \n",
       "\n",
       "      gyro_Y_kurtosis  gyro_Z_kurtosis  gyro_X_skew  gyro_Y_skew  gyro_Z_skew  \n",
       "0            1.350727         4.334908    -0.768383    -0.108224    -1.893026  \n",
       "1            1.341151         4.357877    -0.775745    -0.114498    -1.904374  \n",
       "2            1.337400         4.395134    -0.781193    -0.118137    -1.918170  \n",
       "3            1.338770         4.359282    -0.777257    -0.112780    -1.907157  \n",
       "4            1.338957         4.343288    -0.769209    -0.112013    -1.902188  \n",
       "...               ...              ...          ...          ...          ...  \n",
       "3851         2.393430         1.751305    -1.196690    -1.528542    -0.984029  \n",
       "3852         2.393430         1.751305    -1.196690    -1.528542    -0.984029  \n",
       "3853         2.393430         1.751305    -1.196690    -1.528542    -0.984029  \n",
       "3854         2.393430         1.751305    -1.196690    -1.528542    -0.984029  \n",
       "3855         2.393430         1.751305    -1.196690    -1.528542    -0.984029  \n",
       "\n",
       "[3856 rows x 26 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = extracted_features_training.iloc[:, :-2]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_X_mean</th>\n",
       "      <th>acc_Y_mean</th>\n",
       "      <th>acc_Z_mean</th>\n",
       "      <th>acc_X_std</th>\n",
       "      <th>acc_Y_std</th>\n",
       "      <th>acc_Z_std</th>\n",
       "      <th>acc_mag</th>\n",
       "      <th>acc_X_kurtosis</th>\n",
       "      <th>acc_Y_kurtosis</th>\n",
       "      <th>acc_Z_kurtosis</th>\n",
       "      <th>acc_X_skew</th>\n",
       "      <th>acc_Y_skew</th>\n",
       "      <th>acc_Z_skew</th>\n",
       "      <th>gyro_X_mean</th>\n",
       "      <th>gyro_Y_mean</th>\n",
       "      <th>gyro_Z_mean</th>\n",
       "      <th>gyro_X_std</th>\n",
       "      <th>gyro_Y_std</th>\n",
       "      <th>gyro_Z_std</th>\n",
       "      <th>gyro_mag</th>\n",
       "      <th>gyro_X_kurtosis</th>\n",
       "      <th>gyro_Y_kurtosis</th>\n",
       "      <th>gyro_Z_kurtosis</th>\n",
       "      <th>gyro_X_skew</th>\n",
       "      <th>gyro_Y_skew</th>\n",
       "      <th>gyro_Z_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.256790</td>\n",
       "      <td>-0.462554</td>\n",
       "      <td>-0.784829</td>\n",
       "      <td>0.130087</td>\n",
       "      <td>0.591135</td>\n",
       "      <td>0.371541</td>\n",
       "      <td>0.946495</td>\n",
       "      <td>-0.224492</td>\n",
       "      <td>2.002373</td>\n",
       "      <td>-0.555826</td>\n",
       "      <td>-0.382008</td>\n",
       "      <td>0.578464</td>\n",
       "      <td>0.735185</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>-2.285156</td>\n",
       "      <td>136.164216</td>\n",
       "      <td>27.500832</td>\n",
       "      <td>49.382057</td>\n",
       "      <td>6.376557</td>\n",
       "      <td>-0.071116</td>\n",
       "      <td>3.116190</td>\n",
       "      <td>2.278384</td>\n",
       "      <td>-0.150386</td>\n",
       "      <td>-1.422626</td>\n",
       "      <td>0.155111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257769</td>\n",
       "      <td>-0.462569</td>\n",
       "      <td>-0.784113</td>\n",
       "      <td>0.130428</td>\n",
       "      <td>0.591126</td>\n",
       "      <td>0.371121</td>\n",
       "      <td>0.946176</td>\n",
       "      <td>-0.240044</td>\n",
       "      <td>2.002696</td>\n",
       "      <td>-0.551996</td>\n",
       "      <td>-0.398504</td>\n",
       "      <td>0.578550</td>\n",
       "      <td>0.733888</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>-6.152344</td>\n",
       "      <td>-2.109375</td>\n",
       "      <td>136.162653</td>\n",
       "      <td>27.423676</td>\n",
       "      <td>49.367088</td>\n",
       "      <td>6.515626</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>3.138564</td>\n",
       "      <td>2.282634</td>\n",
       "      <td>-0.149532</td>\n",
       "      <td>-1.414567</td>\n",
       "      <td>0.144675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.258747</td>\n",
       "      <td>-0.461937</td>\n",
       "      <td>-0.783456</td>\n",
       "      <td>0.130765</td>\n",
       "      <td>0.591316</td>\n",
       "      <td>0.370737</td>\n",
       "      <td>0.945589</td>\n",
       "      <td>-0.254675</td>\n",
       "      <td>1.993933</td>\n",
       "      <td>-0.548460</td>\n",
       "      <td>-0.414879</td>\n",
       "      <td>0.575078</td>\n",
       "      <td>0.732676</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-6.464844</td>\n",
       "      <td>-2.128906</td>\n",
       "      <td>136.146788</td>\n",
       "      <td>27.284667</td>\n",
       "      <td>49.366636</td>\n",
       "      <td>6.807054</td>\n",
       "      <td>-0.071051</td>\n",
       "      <td>3.193697</td>\n",
       "      <td>2.283057</td>\n",
       "      <td>-0.143155</td>\n",
       "      <td>-1.409535</td>\n",
       "      <td>0.145865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259899</td>\n",
       "      <td>-0.461397</td>\n",
       "      <td>-0.783044</td>\n",
       "      <td>0.131116</td>\n",
       "      <td>0.591479</td>\n",
       "      <td>0.370501</td>\n",
       "      <td>0.945301</td>\n",
       "      <td>-0.267562</td>\n",
       "      <td>1.986450</td>\n",
       "      <td>-0.546314</td>\n",
       "      <td>-0.434982</td>\n",
       "      <td>0.572115</td>\n",
       "      <td>0.731832</td>\n",
       "      <td>-0.371094</td>\n",
       "      <td>-6.738281</td>\n",
       "      <td>-2.226562</td>\n",
       "      <td>136.112690</td>\n",
       "      <td>27.167927</td>\n",
       "      <td>49.365032</td>\n",
       "      <td>7.106316</td>\n",
       "      <td>-0.070031</td>\n",
       "      <td>3.239446</td>\n",
       "      <td>2.284921</td>\n",
       "      <td>-0.133009</td>\n",
       "      <td>-1.403701</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.261176</td>\n",
       "      <td>-0.461428</td>\n",
       "      <td>-0.782958</td>\n",
       "      <td>0.131461</td>\n",
       "      <td>0.591469</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>-0.277500</td>\n",
       "      <td>1.986906</td>\n",
       "      <td>-0.545886</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>0.572282</td>\n",
       "      <td>0.731636</td>\n",
       "      <td>-0.761719</td>\n",
       "      <td>-6.914062</td>\n",
       "      <td>-2.363281</td>\n",
       "      <td>136.088361</td>\n",
       "      <td>27.100509</td>\n",
       "      <td>49.354425</td>\n",
       "      <td>7.346399</td>\n",
       "      <td>-0.069421</td>\n",
       "      <td>3.263453</td>\n",
       "      <td>2.291185</td>\n",
       "      <td>-0.124516</td>\n",
       "      <td>-1.397798</td>\n",
       "      <td>0.160152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0.359500</td>\n",
       "      <td>-0.306828</td>\n",
       "      <td>-0.581852</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.749623</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>0.369134</td>\n",
       "      <td>-0.306155</td>\n",
       "      <td>-0.592158</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.761998</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>0.382324</td>\n",
       "      <td>-0.284315</td>\n",
       "      <td>-0.627188</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.787637</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>0.393555</td>\n",
       "      <td>-0.283773</td>\n",
       "      <td>-0.638961</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.802299</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0.403214</td>\n",
       "      <td>-0.275315</td>\n",
       "      <td>-0.657078</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.574101</td>\n",
       "      <td>0.818616</td>\n",
       "      <td>-0.621114</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>-1.484621</td>\n",
       "      <td>-0.392547</td>\n",
       "      <td>-1.443782</td>\n",
       "      <td>-0.121935</td>\n",
       "      <td>-17.128906</td>\n",
       "      <td>-18.906250</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>99.986592</td>\n",
       "      <td>52.921312</td>\n",
       "      <td>89.290183</td>\n",
       "      <td>25.512044</td>\n",
       "      <td>0.551662</td>\n",
       "      <td>0.457469</td>\n",
       "      <td>1.608749</td>\n",
       "      <td>-0.589082</td>\n",
       "      <td>-0.734662</td>\n",
       "      <td>-0.796596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acc_X_mean  acc_Y_mean  acc_Z_mean  acc_X_std  acc_Y_std  acc_Z_std  \\\n",
       "0       0.256790   -0.462554   -0.784829   0.130087   0.591135   0.371541   \n",
       "1       0.257769   -0.462569   -0.784113   0.130428   0.591126   0.371121   \n",
       "2       0.258747   -0.461937   -0.783456   0.130765   0.591316   0.370737   \n",
       "3       0.259899   -0.461397   -0.783044   0.131116   0.591479   0.370501   \n",
       "4       0.261176   -0.461428   -0.782958   0.131461   0.591469   0.370453   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "1595    0.359500   -0.306828   -0.581852   0.521528   0.612409   0.574101   \n",
       "1596    0.369134   -0.306155   -0.592158   0.521528   0.612409   0.574101   \n",
       "1597    0.382324   -0.284315   -0.627188   0.521528   0.612409   0.574101   \n",
       "1598    0.393555   -0.283773   -0.638961   0.521528   0.612409   0.574101   \n",
       "1599    0.403214   -0.275315   -0.657078   0.521528   0.612409   0.574101   \n",
       "\n",
       "       acc_mag  acc_X_kurtosis  acc_Y_kurtosis  acc_Z_kurtosis  acc_X_skew  \\\n",
       "0     0.946495       -0.224492        2.002373       -0.555826   -0.382008   \n",
       "1     0.946176       -0.240044        2.002696       -0.551996   -0.398504   \n",
       "2     0.945589       -0.254675        1.993933       -0.548460   -0.414879   \n",
       "3     0.945301       -0.267562        1.986450       -0.546314   -0.434982   \n",
       "4     0.945596       -0.277500        1.986906       -0.545886   -0.457919   \n",
       "...        ...             ...             ...             ...         ...   \n",
       "1595  0.749623       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1596  0.761998       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1597  0.787637       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1598  0.802299       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "1599  0.818616       -0.621114        0.774047       -1.484621   -0.392547   \n",
       "\n",
       "      acc_Y_skew  acc_Z_skew  gyro_X_mean  gyro_Y_mean  gyro_Z_mean  \\\n",
       "0       0.578464    0.735185     0.429688    -5.937500    -2.285156   \n",
       "1       0.578550    0.733888     0.390625    -6.152344    -2.109375   \n",
       "2       0.575078    0.732676     0.097656    -6.464844    -2.128906   \n",
       "3       0.572115    0.731832    -0.371094    -6.738281    -2.226562   \n",
       "4       0.572282    0.731636    -0.761719    -6.914062    -2.363281   \n",
       "...          ...         ...          ...          ...          ...   \n",
       "1595   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1596   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1597   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1598   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "1599   -1.443782   -0.121935   -17.128906   -18.906250    -0.136719   \n",
       "\n",
       "      gyro_X_std  gyro_Y_std  gyro_Z_std   gyro_mag  gyro_X_kurtosis  \\\n",
       "0     136.164216   27.500832   49.382057   6.376557        -0.071116   \n",
       "1     136.162653   27.423676   49.367088   6.515626        -0.071154   \n",
       "2     136.146788   27.284667   49.366636   6.807054        -0.071051   \n",
       "3     136.112690   27.167927   49.365032   7.106316        -0.070031   \n",
       "4     136.088361   27.100509   49.354425   7.346399        -0.069421   \n",
       "...          ...         ...         ...        ...              ...   \n",
       "1595   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1596   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1597   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1598   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "1599   99.986592   52.921312   89.290183  25.512044         0.551662   \n",
       "\n",
       "      gyro_Y_kurtosis  gyro_Z_kurtosis  gyro_X_skew  gyro_Y_skew  gyro_Z_skew  \n",
       "0            3.116190         2.278384    -0.150386    -1.422626     0.155111  \n",
       "1            3.138564         2.282634    -0.149532    -1.414567     0.144675  \n",
       "2            3.193697         2.283057    -0.143155    -1.409535     0.145865  \n",
       "3            3.239446         2.284921    -0.133009    -1.403701     0.151807  \n",
       "4            3.263453         2.291185    -0.124516    -1.397798     0.160152  \n",
       "...               ...              ...          ...          ...          ...  \n",
       "1595         0.457469         1.608749    -0.589082    -0.734662    -0.796596  \n",
       "1596         0.457469         1.608749    -0.589082    -0.734662    -0.796596  \n",
       "1597         0.457469         1.608749    -0.589082    -0.734662    -0.796596  \n",
       "1598         0.457469         1.608749    -0.589082    -0.734662    -0.796596  \n",
       "1599         0.457469         1.608749    -0.589082    -0.734662    -0.796596  \n",
       "\n",
       "[1600 rows x 26 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_val = extracted_features_testing.iloc[:, :-2]\n",
    "# X_val\n",
    "X_test = extracted_features_testing.iloc[:, :-2]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Alex\n",
       "1       Alex\n",
       "2       Alex\n",
       "3       Alex\n",
       "4       Alex\n",
       "        ... \n",
       "3851      XY\n",
       "3852      XY\n",
       "3853      XY\n",
       "3854      XY\n",
       "3855      XY\n",
       "Name: Dancer, Length: 3856, dtype: object"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dancer = extracted_features_training.iloc[:, -2]\n",
    "y_dancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Alex\n",
       "1       Alex\n",
       "2       Alex\n",
       "3       Alex\n",
       "4       Alex\n",
       "        ... \n",
       "1595      XY\n",
       "1596      XY\n",
       "1597      XY\n",
       "1598      XY\n",
       "1599      XY\n",
       "Name: Dancer, Length: 1600, dtype: object"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dancer_test = extracted_features_testing.iloc[:, -2]\n",
    "y_dancer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dancer = preprocessing.LabelEncoder()\n",
    "y_dancer = encoder_dancer.fit_transform(y_dancer)\n",
    "y_dancer_test = encoder_dancer.fit_transform(y_dancer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Abi', 'Alex', 'CJ', 'Ryan', 'XY'], dtype=object)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_dancer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       sidepump\n",
       "1       sidepump\n",
       "2       sidepump\n",
       "3       sidepump\n",
       "4       sidepump\n",
       "          ...   \n",
       "3851    sidepump\n",
       "3852    sidepump\n",
       "3853    sidepump\n",
       "3854    sidepump\n",
       "3855    sidepump\n",
       "Name: movename, Length: 3856, dtype: object"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = extracted_features_training.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            gun\n",
       "1            gun\n",
       "2            gun\n",
       "3            gun\n",
       "4            gun\n",
       "          ...   \n",
       "1595    sidepump\n",
       "1596    sidepump\n",
       "1597    sidepump\n",
       "1598    sidepump\n",
       "1599    sidepump\n",
       "Name: movename, Length: 1600, dtype: object"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_val = extracted_features_testing.iloc[:, -1]\n",
    "# y_val\n",
    "y_test = extracted_features_testing.iloc[:, -1]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = encoder.fit_transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gun', 'hair', 'sidepump'], dtype=object)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histories = []\n",
    "cm_hist = []\n",
    "classification_report_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on dancer classification 4 April 2am\n",
    "def perform_mlp(X_test, y_dancer_test, fold, pca):\n",
    "    start_time = timer()\n",
    "    k = fold\n",
    "    perform_pca = pca\n",
    "    number_of_classes = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    #kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    \n",
    "# # Trial 3: ave accuracy 81%\n",
    "#     mlp_adam = nn.MLPClassifier(hidden_layer_sizes=(200, 150), max_iter=500, activation='tanh', solver='adam',\n",
    "#                                       batch_size = minSamples, validation_fraction=0.2, n_iter_no_change=20,\n",
    "#                                       alpha=1e-4, early_stopping=True, verbose=0, tol=1e-6, random_state=None, \n",
    "#                                       learning_rate_init=0.001, shuffle=False) \n",
    "\n",
    "    acc_scores = []\n",
    "    cv_iteration = 1\n",
    "    cv_pca_iteration = 1\n",
    "    train_histories.clear()\n",
    "    cm_hist.clear()\n",
    "    classification_report_hist.clear()\n",
    "#     train_histories_mlp_sgd.clear()\n",
    "#     train_histories_mlp_adam.clear()\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        \n",
    "        if perform_pca == False:\n",
    "            print('\\nTraining model and cross validate using fold #{}...\\n ' .format(cv_iteration))\n",
    "            cv_iteration += 1\n",
    "        \n",
    "        X_train , X_val = X.iloc[train_index,:], X.iloc[val_index,:]\n",
    "        y_train , y_val = y_dancer[train_index], y_dancer[val_index]\n",
    "        \n",
    "        y_val_without_transform = y_val\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        y_train = to_categorical(y_train, number_of_classes)\n",
    "        y_val = to_categorical(y_val, number_of_classes)\n",
    "        y_dancer_test_categorical = to_categorical(y_dancer_test, number_of_classes)\n",
    "        \n",
    "        if perform_pca == True:\n",
    "            print('\\nTraining model with PCA and cross validate using fold #{}...\\n ' .format(cv_pca_iteration))\n",
    "            cv_pca_iteration += 1\n",
    "            pca = PCA(n_components = 12)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_val = pca.transform(X_val)\n",
    "#             pca.explained_variance_ratio_\n",
    "\n",
    "        def mlp_model():\n",
    "            model = Sequential()\n",
    "#             model.add(Flatten(input_shape=X_train[0].shape))\n",
    "            model.add(Dense(units=32, kernel_initializer='uniform', activation='relu', input_shape=X_train[0].shape))\n",
    "#             model.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(units=number_of_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            return model\n",
    "        \n",
    "        mlp = mlp_model()\n",
    "        print(mlp.summary())        \n",
    "        \n",
    "#         checkpoint_filepath=\"MLP_weights_checkpoint.hdf5\"\n",
    "                \n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.00001, patience=20, verbose=1),\n",
    "#             ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only=True, monitor='val_accuracy',\n",
    "#                             verbose=1, save_best_only=True, mode='max')  \n",
    "        ] \n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_val, y_val),\n",
    "                                  callbacks=[my_callbacks], shuffle=True)\n",
    "        \n",
    "        mlp_pred = np.argmax(mlp.predict(X_test_scaled), axis=-1)\n",
    "        scores = mlp.evaluate(X_test_scaled, y_dancer_test_categorical, batch_size=64, verbose=0)\n",
    "        acc_scores.append(scores[1])\n",
    "        train_histories.append(history.history)\n",
    "        \n",
    "#         mlp_weights = mlp.get_weights()\n",
    "#         print(\"MLP Weights:\", mlp_weights)\n",
    "        \n",
    "#         mlp.save('saved_models/MLP_99.6_accuracy')\n",
    "        \n",
    "        print('y_test\\n', y_test)\n",
    "        print('')\n",
    "        print('mlp_pred\\n', mlp_pred)\n",
    "        \n",
    "        cm_hist.append(confusion_matrix(y_dancer_test, mlp_pred))\n",
    "        classification_report_hist.append(classification_report(y_dancer_test, mlp_pred, target_names=encoder_dancer.classes_))\n",
    "        \n",
    "\n",
    "    end_time = timer()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return mlp, acc_scores, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model and cross validate using fold #1...\n",
      " \n",
      "Model: \"sequential_133\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_342 (Dense)            (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 1s 4ms/step - loss: 1.6022 - accuracy: 0.3649 - val_loss: 1.5240 - val_accuracy: 0.6464\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4421 - accuracy: 0.6865 - val_loss: 1.0442 - val_accuracy: 0.7500\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.7371 - val_loss: 0.6513 - val_accuracy: 0.7759\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.7778 - val_loss: 0.4410 - val_accuracy: 0.8705\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8752 - val_loss: 0.2799 - val_accuracy: 0.9365\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.9278 - val_loss: 0.1823 - val_accuracy: 0.9585\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9409 - val_loss: 0.1307 - val_accuracy: 0.9754\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9517 - val_loss: 0.1002 - val_accuracy: 0.9896\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9657 - val_loss: 0.0818 - val_accuracy: 0.9819\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.9675 - val_loss: 0.0667 - val_accuracy: 0.9909\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0933 - accuracy: 0.9758 - val_loss: 0.0558 - val_accuracy: 0.9896\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0826 - accuracy: 0.9760 - val_loss: 0.0478 - val_accuracy: 0.9896\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9777 - val_loss: 0.0413 - val_accuracy: 0.9935\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9803 - val_loss: 0.0369 - val_accuracy: 0.9948\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9798 - val_loss: 0.0316 - val_accuracy: 0.9948\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 0.9838 - val_loss: 0.0271 - val_accuracy: 0.9974\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9886 - val_loss: 0.0253 - val_accuracy: 0.9961\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9922 - val_loss: 0.0216 - val_accuracy: 0.9974\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9903 - val_loss: 0.0220 - val_accuracy: 0.9961\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9859 - val_loss: 0.0188 - val_accuracy: 0.9987\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9953 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9902 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.9955 - val_loss: 0.0137 - val_accuracy: 0.9987\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9950 - val_loss: 0.0138 - val_accuracy: 0.9987\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9973 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9938 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.9942 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9967 - val_loss: 0.0094 - val_accuracy: 0.9987\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9967 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9949 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.0069 - val_accuracy: 0.9987\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9946 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9968 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9958 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9963 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0056 - val_accuracy: 0.9987\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0051 - val_accuracy: 0.9987\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9958 - val_loss: 0.0058 - val_accuracy: 0.9987\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9947 - val_loss: 0.0054 - val_accuracy: 0.9987\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9962 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9968 - val_loss: 0.0050 - val_accuracy: 0.9987\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9976 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.0051 - val_accuracy: 0.9987\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9981 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9961 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9944 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9976 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9944 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9942 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9982 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9975 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9979 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9984 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9980 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9960 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9940 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9975 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9968 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 00094: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 4 4 4]\n",
      "\n",
      "Training model and cross validate using fold #2...\n",
      " \n",
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_345 (Dense)            (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.6027 - accuracy: 0.3617 - val_loss: 1.5305 - val_accuracy: 0.5422\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4462 - accuracy: 0.5640 - val_loss: 1.0875 - val_accuracy: 0.5901\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9808 - accuracy: 0.6122 - val_loss: 0.7231 - val_accuracy: 0.7665\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.7810 - val_loss: 0.4636 - val_accuracy: 0.8768\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8656 - val_loss: 0.2923 - val_accuracy: 0.9326\n",
      "Epoch 6/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.9226 - val_loss: 0.1834 - val_accuracy: 0.9624\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9555 - val_loss: 0.1331 - val_accuracy: 0.9741\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1705 - accuracy: 0.9508 - val_loss: 0.1037 - val_accuracy: 0.9754\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.9723 - val_loss: 0.0809 - val_accuracy: 0.9818\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9670 - val_loss: 0.0674 - val_accuracy: 0.9870\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9764 - val_loss: 0.0537 - val_accuracy: 0.9883\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9794 - val_loss: 0.0438 - val_accuracy: 0.9896\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9856 - val_loss: 0.0354 - val_accuracy: 0.9909\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9880 - val_loss: 0.0325 - val_accuracy: 0.9922\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0567 - accuracy: 0.9871 - val_loss: 0.0280 - val_accuracy: 0.9935\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9855 - val_loss: 0.0237 - val_accuracy: 0.9948\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9891 - val_loss: 0.0211 - val_accuracy: 0.9987\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9907 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9922 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9949 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9890 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9933 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9935 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9929 - val_loss: 0.0110 - val_accuracy: 0.9961\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9924 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9926 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.9957 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9986 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9986 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9959 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9986 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9968 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9973 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9987 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9977 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9981 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9996 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9935 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9990 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9987 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9945 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9989 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9973 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9947 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9986 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9991 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9991 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9938 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9984 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9978 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9948 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9984 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9967 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9987 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9975 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9980 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9974 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9993 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9974 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9979 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9992 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9996 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9990 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 139/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9967 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 0.9994 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9966 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9986 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9972 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9978 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9975 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 00156: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 4 4 4]\n",
      "\n",
      "Training model and cross validate using fold #3...\n",
      " \n",
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_348 (Dense)            (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_349 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 1.6030 - accuracy: 0.2770 - val_loss: 1.5332 - val_accuracy: 0.4643\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4209 - accuracy: 0.5282 - val_loss: 1.1038 - val_accuracy: 0.6083\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9739 - accuracy: 0.6806 - val_loss: 0.7592 - val_accuracy: 0.7432\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.8002 - val_loss: 0.4760 - val_accuracy: 0.8833\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.9117 - val_loss: 0.2616 - val_accuracy: 0.9546\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9473 - val_loss: 0.1578 - val_accuracy: 0.9741\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9666 - val_loss: 0.1140 - val_accuracy: 0.9715\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9605 - val_loss: 0.0871 - val_accuracy: 0.9767\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9675 - val_loss: 0.0712 - val_accuracy: 0.9818\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9801 - val_loss: 0.0607 - val_accuracy: 0.9870\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9883\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.9856 - val_loss: 0.0440 - val_accuracy: 0.9909\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9852 - val_loss: 0.0391 - val_accuracy: 0.9909\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9901 - val_loss: 0.0314 - val_accuracy: 0.9935\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9863 - val_loss: 0.0286 - val_accuracy: 0.9974\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9867 - val_loss: 0.0251 - val_accuracy: 0.9948\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9913 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9910 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9910 - val_loss: 0.0185 - val_accuracy: 0.9961\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9938 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9933 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9946 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9972 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9952 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9965 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9929 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9968 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9990 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9976 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.9974 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9974 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9984 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9991 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9981 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9992 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9984 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9979 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9979 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9976 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9949 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9987 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9997 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9978 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9990 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9987 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9979 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9978 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9973 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9944 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9992 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9984 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9991 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9989 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9974 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9972 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9995 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 118/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9934 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9968 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9955 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9968 - val_loss: 0.0033 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9980 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9995 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9975 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9976 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9988 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9957 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9952 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 00131: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 4 4 4]\n",
      "\n",
      "Training model and cross validate using fold #4...\n",
      " \n",
      "Model: \"sequential_136\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_351 (Dense)            (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 1s 5ms/step - loss: 1.6026 - accuracy: 0.3642 - val_loss: 1.5427 - val_accuracy: 0.6031\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4509 - accuracy: 0.6456 - val_loss: 1.0988 - val_accuracy: 0.6900\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9249 - accuracy: 0.7419 - val_loss: 0.6760 - val_accuracy: 0.7834\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.8115 - val_loss: 0.4272 - val_accuracy: 0.8833\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.8959 - val_loss: 0.2759 - val_accuracy: 0.9326\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.9314 - val_loss: 0.1891 - val_accuracy: 0.9481\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9486 - val_loss: 0.1353 - val_accuracy: 0.9559\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9558 - val_loss: 0.1036 - val_accuracy: 0.9637\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1102 - accuracy: 0.9712 - val_loss: 0.0815 - val_accuracy: 0.9741\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9675 - val_loss: 0.0657 - val_accuracy: 0.9780\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9753 - val_loss: 0.0542 - val_accuracy: 0.9857\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0735 - accuracy: 0.9757 - val_loss: 0.0453 - val_accuracy: 0.9948\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9814 - val_loss: 0.0426 - val_accuracy: 0.9883\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9822 - val_loss: 0.0346 - val_accuracy: 0.9961\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9870 - val_loss: 0.0298 - val_accuracy: 0.9987\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9877 - val_loss: 0.0265 - val_accuracy: 0.9987\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9878 - val_loss: 0.0232 - val_accuracy: 0.9987\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9908 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9920 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9899 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9926 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9921 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9931 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9967 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9947 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9931 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.9965 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9949 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9945 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9957 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9958 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9955 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9946 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9983 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9965 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0030 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9992 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9974 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9979 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9991 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9982 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9984 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9964 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9928 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9974 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9973 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9946 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9992 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9966 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9975 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9983 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9972 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9977 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9981 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9991 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9995 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9992 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9997 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9962 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 00114: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 4 4 4]\n",
      "\n",
      "Training model and cross validate using fold #5...\n",
      " \n",
      "Model: \"sequential_137\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_354 (Dense)            (None, 32)                864       \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_356 (Dense)            (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.6029 - accuracy: 0.3038 - val_loss: 1.5359 - val_accuracy: 0.4514\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 1.4528 - accuracy: 0.5471 - val_loss: 1.1035 - val_accuracy: 0.7406\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.9817 - accuracy: 0.7551 - val_loss: 0.6085 - val_accuracy: 0.8132\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.8219 - val_loss: 0.3696 - val_accuracy: 0.8755\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3644 - accuracy: 0.8794 - val_loss: 0.2598 - val_accuracy: 0.9351\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.9321 - val_loss: 0.1783 - val_accuracy: 0.9598\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9566 - val_loss: 0.1259 - val_accuracy: 0.9715\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9645 - val_loss: 0.0928 - val_accuracy: 0.9818\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9760 - val_loss: 0.0721 - val_accuracy: 0.9831\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9803 - val_loss: 0.0607 - val_accuracy: 0.9857\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9828 - val_loss: 0.0478 - val_accuracy: 0.9870\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9812 - val_loss: 0.0419 - val_accuracy: 0.9935\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9868 - val_loss: 0.0380 - val_accuracy: 0.9909\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9840 - val_loss: 0.0325 - val_accuracy: 0.9922\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9900 - val_loss: 0.0304 - val_accuracy: 0.9922\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9928 - val_loss: 0.0294 - val_accuracy: 0.9922\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 0.0249 - val_accuracy: 0.9935\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9930 - val_loss: 0.0233 - val_accuracy: 0.9935\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.9944 - val_loss: 0.0192 - val_accuracy: 0.9948\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9934 - val_loss: 0.0217 - val_accuracy: 0.9935\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.0175 - val_accuracy: 0.9948\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9910 - val_loss: 0.0157 - val_accuracy: 0.9948\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9939 - val_loss: 0.0163 - val_accuracy: 0.9948\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9944 - val_loss: 0.0136 - val_accuracy: 0.9948\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9919 - val_loss: 0.0141 - val_accuracy: 0.9948\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0121 - val_accuracy: 0.9961\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9985 - val_loss: 0.0126 - val_accuracy: 0.9961\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9972 - val_loss: 0.0122 - val_accuracy: 0.9961\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9955 - val_loss: 0.0100 - val_accuracy: 0.9974\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9947 - val_loss: 0.0122 - val_accuracy: 0.9948\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9970 - val_loss: 0.0099 - val_accuracy: 0.9948\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9971 - val_loss: 0.0090 - val_accuracy: 0.9987\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9981 - val_loss: 0.0074 - val_accuracy: 0.9987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.0095 - val_accuracy: 0.9961\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9962 - val_loss: 0.0075 - val_accuracy: 0.9987\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9980 - val_loss: 0.0092 - val_accuracy: 0.9974\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0074 - val_accuracy: 0.9987\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.0085 - val_accuracy: 0.9974\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.0081 - val_accuracy: 0.9974\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0064 - val_accuracy: 0.9987\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9967 - val_loss: 0.0083 - val_accuracy: 0.9948\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9971 - val_loss: 0.0064 - val_accuracy: 0.9974\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0059 - val_accuracy: 0.9987\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0078 - val_accuracy: 0.9948\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9978 - val_loss: 0.0082 - val_accuracy: 0.9974\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9987\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9968 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9994 - val_loss: 0.0067 - val_accuracy: 0.9987\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 0.0063 - val_accuracy: 0.9974\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0054 - val_accuracy: 0.9974\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9987 - val_loss: 0.0048 - val_accuracy: 0.9974\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.0040 - val_accuracy: 0.9987\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9991 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9987\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0036 - val_accuracy: 0.9987\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0036 - val_accuracy: 0.9987\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9973 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9997 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9984 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 00110: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "mlp_model, acc_scores, time_taken = perform_mlp(X_test, y_dancer_test, fold=5, pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - prediction accuracy of each fold:\n",
      " [0.965624988079071, 0.9643750190734863, 0.9662500023841858, 0.9568750262260437, 0.9674999713897705]\n",
      "\n",
      "MLP - average accuracy in 5-fold = 0.9641250014305115 with std. deviation 0.0037624635682507513\n",
      "\n",
      "Time taken: 55.86 seconds\n"
     ]
    }
   ],
   "source": [
    "print('MLP - prediction accuracy of each fold:\\n {}'.format(acc_scores))\n",
    "print('')\n",
    "print('MLP - average accuracy in 5-fold = {} with std. deviation {}' .format(np.mean(acc_scores), np.std(acc_scores)))\n",
    "print('')\n",
    "print('Time taken: {:.2f} seconds' .format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAG4CAYAAAA3yvKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABTtUlEQVR4nO3dd3gU1f7H8fcJoUQhJBFQkiBIb9IR6S0BlW4FqYL12rh2RbGBetWfYLnqRUVpgl3EQglIryJNRZqAkiBSQhFSTDi/P3ZZggmQxWxmd/i8noeH7M6cne98n5n97sycOWOstYiIiLhNmNMBiIiIBIIKnIiIuJIKnIiIuJIKnIiIuJIKnIiIuJIKnIiIuFK40wEEggmPsKZYKafDCBkNa13odAgiImdk+/Zt7Nmzx+Q1zZ0Frlgpite41ukwQsaiZa85HYKIyBlp2azJSafpFKWIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSCpyIiLiSClwBKV4snAUT7mPZBw+x8uNhPHrrFQBER57Dl2/cwbqpw/nyjTuIKhVxQrsKF0Sze9H/MbR/RyfCDlozZ0ynXp0a1KlZlReef87pcIKe8uUf5cs/oZovFbgCkpGZxWU3v0Kz656jWe9n6dSiNpdcXIn7bkhk7vINXNzjKeYu38B9N3Q6od3z913FzEU/OhR1cMrOzmboXbczddo3rFr7Ex9Nmcz6n35yOqygpXz5R/nyTyjnSwWuAB1OywSgaHgRwsOLYK2la7t6TJy2DICJ05bRrX093/zd2tVj6449/LTld0fiDVYrli+nSpWqXFS5MsWKFeOa63rz5bSpTocVtJQv/yhf/gnlfKnAFaCwMMPSKQ/x6+znmLP0Z1b8sJ1y55Xi9z0HAfh9z0HKxpQC4JwSxbj3hkRG/u9rJ0MOSikpycTHV/C9jouLJzk52cGIgpvy5R/lyz+hnK9wpwNwk6NHLZf2fo7SJSP44KWbqF2l/Ennfey2Lrw6cY7vqE+Os9bmes8Y40AkoUH58o/y5Z9QzpcKXAAc+DON+d9tolOL2vyx9xAXlInk9z0HuaBMJLv3HQKgad2K9EpowMihPSldKoKjRy3pmX/x5gfzHY7eeXFx8ezY8ZvvdXLyDmJjYx2MKLgpX/5RvvwTyvnSKcoCUia6JKVLenpIlihelA7NarBh2y6+mreOft2aAdCvWzO+nLsWgIQho6nZ5XFqdnmc1ybN5YV3Zqq4eTVp2pTNmzexbetWMjMz+eiDKXTp2t3psIKW8uUf5cs/oZwvx4/gjDG9gE+BWtban40x7YD7rLVd85j3a+B6a+3+Qg0yHy4oE8lbT/WnSFgYYWGGT2Z9zzcLfmDZ2q1M/M9gBvZszm87U+n7wDtOhxr0wsPDGfXya3Tr0pns7GwGDhpM7Tp1nA4raClf/lG+/BPK+TJ5nV8t1ACM+RAoD8y21j5xqgKXX2HnlLPFa1xbQBG6X+qK15wOQUTkjLRs1oSVK7/L86Kgo6cojTElgZbAEKB3jkmRxpjPjDE/GWPeNMaEeeffZowp40SsIiISWpy+BtcTmG6t3QjsM8Y08r5/CXAvcDFQBbjSmfBERCRUOV3g+gBTvH9P8b4GWG6t/cVamw1MBlqd7oOMMTcbY74zxnxns9ICE62IiIQMxzqZGGPOAzoAdY0xFigCWOBr7/85nfZCobV2DDAGPNfgCjZaEREJNU4ewV0NjLfWVrTWVrLWVgC24jlau8QYc5H32tt1wEIH48xTieJFmfn23YSFGaa+9i92zn+eT16+9aTzFysazoTnbuCHqY8zf/x9XFg+xjetb7dmrJs6nHVTh9PXe0sBwPjnbqDKhWUDuh6FJS0tjcQObcnOzmbi+HHUrVWNurWqMXH8uDznz8jIoN/111GnZlVat2jG9m3bfNNO1r5/395s3rQp0KtSKJQv/yhf/jlb8uVkgesDfPa39z4BrgeWAM8BP+Apen+fz3EDezRn6uw1HD1qGTU+iSGPjj/l/IN6Nif1UBp1ezzJq5O+ZeTdPQDP0waG3Xw5bfq/SOt+LzDs5st9TxwY89EC7hmYEPB1KQzj3h1Lj55XcuDAAUaOeJL5i5axYPFyRo54ktTU1Fzzvzf2HaKjovnx583cefe/GfbIgwDs27fvpO1vvuU2Xnrx+UJdr0BRvvyjfPnnbMmXYwXOWtvOWjv9b++9Yq2tZa3tYK29zlpb21p7q7X2qHd6JWvtHmciPlHvK5owzXvT9tzlGzl0OOOU83dtV49J3kGXP01aRbtLagCQ2KIWs5f+TOrBI+w/lMbspT/TqWVtABZ9v4UOzWpQpIjTl0r/uSmTJ9Gtew9mzZxBx46JxMTEEB0dTceOicycMT3X/F9Om0rf/gMBuPKqq5k7ZzbW2lO2b9mqNXPmJJGVlVWo6xYIypd/lC//nC35Cv1vTgcUDS9Cpbgy/LpzX77bxJYrzY7fPb9ssrOPcvDPNM6LOpfYslHs2HX8F1PyH/uJLRsFeMaA2/LbHupVjyvQ+AtbZmYm27b+QsVKlTwDt1bIMXBrfDwpKbkHbs05X3h4OJGlS7N3795Ttg8LC6NKlaqsXbMmwGsUWMqXf5Qv/5xN+VKBOwNlokty4NARv9rkNTiptZDXmKU2R5+a3fsOUb5sab9jDCZ79uyhdFQUkP+BW0823+naly1bjp07U/5BtM5TvvyjfPnnbMqXCtwZSEvPpETxon61Sd61n/gLogEoUiSMyJIR7DtwmOQ/9hN/frRvvrhyUezcfcD3ukTxoqRl/FUwgTskIiKC9PR0wDtw6285Bm7dsYPy5XMP3JpzvqysLA4eOEBMTMxp26dnpBMREZHr80KJ8uUf5cs/Z1O+VODOwP5DaRQJC6N4sfzfZfHVvHW+HpJXJjRk3oqNAMxavJ6E5jWJKhVBVKkIEprXZNbi9b52VS8sx/otOwt2BQpZdHQ02dnZpKenk9ipM0lJM0lNTSU1NZWkpJkkduqcq02Xrt2ZNMHTI+vTTz6mbfsOGGNO237zxo3Uqh0a4+SdjPLlH+XLP2dTvhwfbDlUJS1dT4uGVfh22QaS3hlK9YvOp2REcTZPf5pbn3yfpCXreey2Lnz/0698NW8d732+mLEjBvDD1MdJPXiY/g+9C0DqwSM8+9Z0Fk58AIBnxkwn9aDn9Ge5mFKkZ2T6HpgayhISOrF40UI6dEzg4Uceo1XzpgA8Mmw4MTGeWyaeemI4jRo3oWu37gwaPITBg/pTp2ZVoqNjmDDJMx5ATEzMSdvv2rWLEhERlC9/8ufwhQrlyz/Kl3/Olnw5PthyIBTGYMv1a8RzV78ODHns1LcH/BN39m3PwcPpjPt8ScCWAYUz2PLqVat4ZfRLjB03IWDLeGX0KCIjIxk0eEjAllFYlC//KF/+cVO+gnaw5VC2ZsMO5n23kbCwwD3Zdv+hNCZ6by0IdQ0aNqRtu/ZkZ2cHbBlRUVH0GzAwYJ9fmJQv/yhf/jlb8qUjONHjckQkZOkITkREzjoqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4koqcCIi4krhTgcQCA1qXci8Ra84HUbIiG55v9MhhJTURS84HYKI5IOO4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ERExJVU4ArJ/v376d/nGhrXr02TBnVYtnSJ0yE5Kr5caaa/fgurptzHysn3cvt1rQAYfktnlk+8h6UT/s20V26ifJnIE9pVOD+K3d+OYGjftk6EHbRmzphOvTo1qFOzKi88/5zT4QQ95cs/oZqvcKcDOFs8eN9QEjp1ZsLkj8jMzOTIkSNOh+SorOyjPPTyl6zekEzJc4qzeNzdzF6+kVET5/LU/2YA8K9rW/LwkATu+s+nvnbP/7s7M5f87FTYQSk7O5uhd93OV9/MIi4+nlaXNqVr1+7Uql3b6dCCkvLln1DOl47gCsHBgwdZvHABAwYNAaBYsWJERUU5G5TDft97iNUbkgH480gGP2/7g9iypTl0OMM3zzkRxbD2eJtubeqwNXkvP/2yq7DDDWorli+nSpWqXFS5MsWKFeOa63rz5bSpTocVtJQv/4RyvlTgCsG2rb9wXpmy3HbzYFpd2pg7bruJw4cPOx1W0LiwfDQNqsey4sdfAXji1svY9MUwenduxNNjPEdz55Qoyr0D2jPy7VlOhhqUUlKSiY+v4HsdFxdPcnKygxEFN+XLP6GcLxW4QpCVlcWa1d8z5KZbWbh0Jeeccy4vvfgfp8MKCudGFGPycwO4f9QXvqO3J96cTrXuI5ky43tuvaYlAI/d3JlXJ8/ncFqmk+EGJZvzMNfLGONAJKFB+fJPKOdLBa4QxMXFExcXT9NLmgHQs9dVrFn9vcNROS+8SBiTnxvAB9NXMXXuD7mmfzhjFT3bXwxA0zoVGHlHF37+7GHu6N2a+wd24NarWxR2yEEpLi6eHTt+871OTt5BbGysgxEFN+XLP6Gcr0IrcMaYXsYYa4yp6X1dyRiT+1vNhc6/4ALi4iuwaeMGAObOnUPNmsF/gTbQ3nz0WjZs+4NXJs/3vVelQhnf311a12Hj9j8ASLjlDWr2epaavZ7ltSkLeGHcHN78eHGhxxyMmjRtyubNm9i2dSuZmZl89MEUunTt7nRYQUv58k8o56swe1H2ARYCvYEnCnG5QeGFl17mxhv6k5mZSaVKF/H6mLFOh+SoFvUr0feKxqzbtJOlE/4NwONvfMOg7pdQ7cKyHD1q+fX3VO76zycORxr8wsPDGfXya3Tr0pns7GwGDhpM7Tp1nA4raClf/gnlfJm8zq8W+EKMKQlsANoDX1hraxpjKgFfWmvrGmOKAM8B7YDiwH+ttf8zxtwD1LXWDjbGXAxMBi6x1p6yj32jxk3svEXLA7hG7lKu7YNOhxBSUhe94HQIIuLVslkTVq78Ls+LgoV1irInMN1auxHYZ4xp9LfpQ4AD1tqmQFPgJmPMRcBooKoxphfwLnDL6YqbiIgIFF6B6wNM8f49xfs6p07AAGPMamAZcB5QzVp7FBgETADmWWsXnWwBxpibjTHfGWO+27N7dwGHLyIioSbg1+CMMecBHYC6xhgLFAEs8HrO2YA7rbUz8viIasCfwCm77VhrxwBjwHOKsgBCFxGREFYYR3BXA+OttRWttZWstRWArUB8jnlmALcZY4oCGGOqG2PONcaUBl4G2gDnGWOuLoR48yUtLY3LE9uTnZ3NpInjaFC3Bg3q1mDSxHF5zp+RkcGgfr2pX6c67Vs3Z/v2bQD8un07bVo0pWWzRlzS6GLeeetNX5tB/fuwefOmwlidgCtRPJyZb9xKWJjxdC75+AHWffwAfa9onOf8F14Qxdev3czyifcw4/VbiStXGoA2jauwdMK/ff9S5z9DtzaeC97jR/Q9oRdmKEtLSyOxQ1uys7OZOH4cdWtVo26takwcf/Ltq9/111GnZlVat2jG9m3bfNNO1r5/395s3uSO7Uv58s/Zkq+AdzIxxswFnrPWTs/x3l3A5UAFbyeTMGAE0A3P0dxuPNftRgGrrbWvGGMqAN8CLay1f5xqmYXRyWTMm6+TlZVF7+v70a7lJcxdtBxjDG1bNGXe4hVER0efMP9b/3uDH39Yy+hX3+DjD6fw5Ref897EKWRmZmKtpXjx4vz5559c2rges75dSPnYWBYumMcHkyfx6utjArouhdHJ5JarWxBeJIz3v1nJovfupuWgl7EWFo+7mxYDX2b/obQT5p/0TD++XrieSV+vpG3jKgzo1pQhT0w5YZ7oyAh++PghqnYbQVrGX7RqWJk+lzXi9mc/Dui6FEYnkzdf/y9ZWVlc368/LS9twqKl32GMoUWzxixetjLX9vW/N17nh3VrefX1N/nwgyl8MfUzJr7/Afv27Ttp+wXz5zF50kRe/99bAV+fQFO+/OOmfDnaycRa2y5ncfO+94q19nJrbV3v66PW2kestRdba+taa9tbaw9Yawdba1/xzvObtbbq6YpbYflwyvt06dad2bNm0L5jAjExMURHR9O+YwJJM6fnmv+rL6fSp+8AAHpeeTVz587BWkuxYsUoXrw44PmVdPToUV+bFi1bM3fObLKysgpnpQKod+eGTJv/I4mX1mD28k2kHkxj/6E0Zi/fRKfmNXLNX/Oi85n73WYA5q3cQtc2ubsl9+pQj5lLfiYt4y8AFq3eSodLqlGkSOiPXzBl8iS6de/BrJkz6Ngx0bd9deyYyMwZubevL6dNpW//gQBcedXVzJ0zG2vtKdu3bNWaOXOSXLF9KV/+OVvyFfrfBA7IzMxk27ZfqFixEjtTUojLMU5bbFw8O1NScrXZmZLiG88tPDycyMjS7Nu7F4Adv/1G86YNqF2tIkPvfYDy3lECwsLCqFylCuvWrimEtQqcouFFqBR3Hr/uTCW2bGl27Nrvm5b8xwFiy5bO1Wbdpp2+UUx6tKtL5LkliIk854R5rklswIczV/teW2vZ8tse6lUrH5D1KCyZmZls2/oLFStV8owDWCHHOIDx8aSk5B4HMOd84eHhRJYuzd69e0/ZPiwsjCpVqrJ2TWhvX8qXf86mfKnAnYG9e/ZQunQUkP9x2vI8FeydL75CBZasWM3qHzby/sTx/LHr+Gj5ZcqW4/eduQtmKCkTdS4HvKcg8xrCLq/cPPzKl7RuWJkl44fSulFlkv/YT1b28aPbC84rRZ0qFzBr6YYT2u1O/TPXM+RCzZ49eyjtfdrEP9m+jDGnbV+2bDl2hvj2pXz552zKlwrcGSgREUFGejoAsXFxJOcYpy0leQcXlM99BBEbF+cbzy0rK4uDBw8QExNzwjzlY2OpVbs2ixct8L2XkZ5OiYiIQKxGoUnL+IsSxTwddpP/OED8+VG+aXHlSrNzz8FcbXbuOUjvh8bTfMBoHn/Dc8rj4OF03/SrEurzxbwfTih6ACWKFyUtI7RPIUVERJDu3b7i4uLZ8VuOcQB37KB8+dwdinPOl5WVxcEDnu3rdO3TM9KJCPHtS/nyz9mULxW4MxAdHU12djbp6el0TOzMnKRZpKamkpqaypykWXRM7JyrzRVdujN50ngAPv/0Y9q2bY8xhuQdO0hL8xzdpKamsnTJYqpVP35NavPmTdSqFRrD4pzM/kNpFCkSRvFi4cxauoGEZtWJKhVBVKkIEppVz3UUBnBe6XN8vwTvH9iBcdNWnDD92k4nnp48pmqFMqz/5feArEdhybl9JXbqTFLSTN/2lZQ0k8ROubevLl27M2mCpwfbp598TNv2HTDGnLb95o0bqVU7tLcv5cs/Z1O+9ETvM9QhIZElixfSvkMCDzw8jHatPE8KePCRR31HZiOeepxGjRpzRdfuDBg0mJsHD6B+nepER8fw7oT3AdiwYT3DHrrfd7h/19B7qFPXc+3pj127KFEiIs8jwlCTtGwjLepfxLcrNvHs2CQWvnsXAM+8M4vUg54C/9jNnfh+/Q6+WvATbRpX4al/XY61sHDVLwx94TPfZ11YPpr4clEs+P6XE5ZRLqYk6Rl/8fveQ4W3YgGSkNCJxYsW0qFjAg8/8hitmjcF4JFhw33b11NPDKdR4yZ07dadQYOHMHhQf+rUrEp0dAwTJnl6nMbExJy0/a5duygREUF5F2xfypd/zpZ8FcpYlIWtMG4TWLN6Fa+9Moq3xo4P2DJee2U0kZGlfE8CD5TCuE2gfvVY7rq+Ta6u/gXpzt6tOXg4PdfRXkErjNsEVq9axSujX2LsuAkBW8Yro0cRGRnJoMGB3b4Kg/LlHzflKxjGonSd+g0a0qZtO7KzswO2jKio0lzfb2DAPr8wrdmYwryVWwgLC9yDEvf/mcbEr1cG7PMLU4OGDWnbrn2At68o+g1wx/alfPnnbMmXjuBETxPwk54mIBI8dAQnIiJnHRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxJRU4ERFxpXCnAwgEa+GotU6HETJSF73gdAghJTpxhNMhhJTd0x9xOoSQY4xxOoSQcapveh3BiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK6nAiYiIK530gafGmEMcf5bcsafvWe/f1lobGeDYREREzthJC5y1tlRhBiIiIlKQ8nWK0hjTyhhzg/fvMsaYiwIbloiIyD9z2gJnjHkceBB42PtWMWBiIIMSERH5p/JzBNcL6A4cBrDWpgA6fSkiIkEtPwUu01pr8XY4McacG9iQRERE/rn8FLgPjTH/A6KMMTcBScBbgQ1LRETknzlpL8pjrLUvGmMSgYNAdWC4tXZWwCMTERH5B05b4LzWARF4TlOuC1w4IiIiBSM/vShvBJYDVwJXA0uNMYMDHZiIiMg/kZ8juPuBhtbavQDGmPOAxcDYQAYmIiLyT+Snk8kO4FCO14eA3wITjoiISME41ViU93j/TAaWGWOm4rkG1wPPKUs5hdtvuZEZ07+ibNlyLPluDQAjnhzO119NI8yEUbZcWV7/31jKx8Y6HGlwmjljOvfdczfZ2dkMGnwj9z/wkNMhOap40SIkvTyAYsXCCS8Sxmfz1jPivflMGN6LahXOAyCqZAn2/5nOpTe97WtXoVwk3793KyPfm8/oD5c6FX7Qee2V0Yx79x2MMdSpU5c33hpLiRIlnA4raNWufhElS5aiSJEihIeHs2DJCqdDypdTHcGV8v7bAnzO8YGXpwI7AxtW6Lu+/wA+/vyrE96769/3sXj5KhYuW0nny7vw/LMjHIouuGVnZzP0rtuZOu0bVq39iY+mTGb9Tz85HZajMv7K5rJ7JtLsxrdoduNbdLqkCpfUiqP/U59x6U1vc+lNb/P5/J+ZumDDCe2evz2Rmcs2OxR1cEpJTubN/77K/MXLWf79WrKPZvPxh1OcDivofT1zDktWrAqZ4ganHmz5ycIMxG1atmrD9u3bTngvMvL4AxiOHD6MMQbJbcXy5VSpUpWLKlcG4JrrevPltKnUql3b4cicdTj9LwCKhocRXiQM6/vN6XFVu9pcds8E3+tuLauzNWW/r50cl5WVRVpaGkWLFuXIkSOUL68zKW6Un16UZY0xLxhjvjbGzDn2rzCCc6OnH3+UOtUq8dEHk3nksSecDicopaQkEx9fwfc6Li6e5ORkByMKDmFhhqVv3civn93DnJVbWbE+xTetZb0L2ZX6J1uSUwE4p0RR7u3TgpHj5jsVbtCKjYvjrn/fS+1qlahaKY7SkaXpmNjJ6bCCmsHQo0tnWl3ahLFvj3E6nHzLTyeTScDPwEXAk8A2IHSOUYPMY0+O4MdN27jmuj6MefO/TocTlDwjw51IR7tw9Kjl0pvepuo1L9OkZiy1K5X1Tbu2Qx0+mv2j7/Vjg9rw6sfLdPSWh9TUVL6a9gXrft7Cpq07OHzkMFPe1/jxp5I0dyGLlq3k0y++Zsybr7NwQWj8cMpPgTvPWvsO8Je1dp61djBwaYDjcr2rr+vDtKmfOR1GUIqLi2fHjuMddZOTdxCrzjg+Bw5nMH/1djpdUgWAImGGHq1r8PG3x69TNq0Vx8hbOvLz5Du44+pLuL9vS27t2cSpkIPK3DlJVKxUibJly1K0aFG69+jFsqVLnA4rqB3rDFeuXDm69ejJyhWh0c8wPwXu2E/AncaYLsaYhkB8AGMCwBhzgTFmijFmizHmJ+8p0urGmB8CvexA2bJ5k+/vb76aRrXqNRyMJng1adqUzZs3sW3rVjIzM/nogyl06drd6bAcVab0OZQ+tzgAJYqF06HxRWz4dQ8AHRpfxMbf9pK85/jdPAl3j6dmn9eo2ec1Xvt4OS9MWsSbn3/nSOzBJr7ChaxYvowjR45grWXut3OoUbOW02EFrcOHD3Po0CHf33OSZlG7Tl2Ho8qf/NzoPcIYUxq4F3gViAT+HcigjOd81GfAOGttb+97DYDzA7ncgjRkYF8Wzp/H3r17qF21Ig89+jizZnzD5k0bMWFhVKhwIaNeed3pMINSeHg4o15+jW5dOpOdnc3AQYOpXaeO02E56oLzSvLWQ90pEmYICzN8Mnc93yz19I68pkMdPsxxelJOreklzejZ6ypaXdqE8PBw6tdvwA1DbnI6rKD1x65d9Ln2SsDTOefa3n1I7HyZw1Hlj8nreofTjDEdgCestW3+9n4l4Etr7Sl/PjRs1MTOXbQsgBG6S/GiRZwOIaREJ+r2Dn/snv6I0yGEHF1zzr/WzZvy/crv8kzYqW70fhU4afWz1t5VALGdTF1gZQA/X0REXO5UpyhD6oS9MeZm4GaAChUudDgaERFx2qlu9B5XmIH8zY94nlyQb9baMcAY8JyiDERQIiISOvLTi9IJc4Di3ieIA2CMaQpUdC6kE6WlpXFFp/ZkZ2fz/sTxNLq4Jo0ursn7E8fnOX9GRgY39O9Dw7o16NimuW+Uk19/3U7bFpfQqlljLm1cj7Fv/c/XZvCA60/oeRnK0tLSSOzQluzsbCaOH0fdWtWoW6saE8fn/TsqIyODftdfR52aVWndohnbt23zTTtZ+/59e7N5kzvyVaJYODNH9ycszNC3cz3WTfgX6yb8i76d6+U5/4Xnl+br/+vL8rdvYsao/sSVKXXC9FLnFGPLh3cx6q7OvvfGP9aLKnHRAV2PwpKWlsZlCZ79cdKEcTSoU4MGdWowacLJt6+B/XpTv3Z12rdu7tu+1q5ZTYe2LWna8GIubdKATz76wNdmUP8+bHbR/tg5oZ0vX/VrV6d+7eqnzNeAvr2pV6sa7VpdesL+2LPr5cSVi+bqnt1OaDOwXx/H98egLHDW0/OlF5DovU3gR+AJIAXIcDK2YyaOe5duPXpx8MAB/vPM08yet5g585fwn2eeZn9qaq75J7w3lqioaFb9sIF/3TmUJx59GIALLijPzG8XsHDZSpLmLWbU/z3PzhTPCBWDb7qFl196sVDXK1DGvTuWHj2v5MCBA4wc8STzFy1jweLljBzxJKl55Ou9se8QHRXNjz9v5s67/82wRx4EYN++fSdtf/Mtt/HSi88X6noFysAr6jN1wc+UPrc4wwa0ps2/xtL6trEMG9CaqJK5BwV+9taOTJq5jktufItnxi/gqZs6nDD98cHtWLD21xPeG/PFSu7p3SKg61FYJox7l+49e3HgwAGeG/k0cxYs4duFS3lu5NN5bl/jvfvjmp82cvuddzP8Uc9g3hHnnMOYd95jxap1fPbF1zx4/z3s378fgBtvupXR//dCYa5WwIx/byzde3jy9eyIp/h24VLmLlrGsyOeyjNf4959h6ioKNau38Ttdw3lsWHHBz+/+577eGts7h/2N958K6NfcnZ/DMoCB2CtTbHWXmutrWKtrWOt7QLUxjP4s+M++uB9rujandlJM2nfIYHomBiioqNp3yGBpFkzcs3/9Vdf0KdffwB69LqKeXPnYK2lWLFiFC/uub8pMyMDe/Sor02Llq2Z++1ssrKyCmelAmjK5El0696DWTNn0LFjIjExMURHR9OxYyIzZ0zPNf+X06bSt/9AAK686mrmzpmNtfaU7Vu2as2cOUmuyFfvjnWZtnAjiU2rMHvlVlIPpbP/z3Rmr9zqu8E7p5qVyjJ35VYA5q3aRteW1X3TGla/gHLR55K04pcT2ixa+ysdGl9EkbDQ77H3wZT36dK1O7NnzaB9xwTf9tG+YwJJM3NvX19Nm8r1/QYA0PPKq5n7rWd/rFatOlWrVgM8NzeXLVuOPXt2A9CiVWvmznHH/vjhlPfp2q0HSXnka1ae+frCtz/2uvJq5n472zfiUPsOHSlZqlSuNi1btebb2c7mKz9jUVY3xsw+doO1MaaeMebRwIeWK46ngKeAZwt72X+XmZnJtq1bqVixEjtTkomLP37fe2xcHDtTco+buDMlhbg4z/iK4eHhREaWZt/evQDs2PEbLS5pSJ3qlbj7nvt9owaEhYVRuUoVfli7phDWKnA8+fqFipUqecaZrJBjnMn4eFLyyFfO+cLDw4ksXZq9e/eesn1YWBhVqlRl7ZrQzlfR8DAqxUbz664DxJYpxY4/DvqmJe8+SGyZ3F8m67bsomfbmgD0aF2DyHOLExMZgTHw3G2JPPLm7FxtrIUtyfuoVzVkbi/N04nbV0qucUxTUlJytck5X3h4OKUjPdtXTt+tWE5mZiaVK3t+UBzbH9e5YH/c6s3XzuTc+9POPMZ9zTk+7Mny9XeefFV1NF/5OYJ7C3gY74gm1tq1QO9ABpUXa+1wa219a+2qwl723+3ds4fSUVFA3uMmksc9LKcaXzE+vgKLl6/i+3UbmDxpPH/s2uWbp2zZcuzcmXsHDSV7TpOvvO75Odl8p2vvhnyVKX0OB/5MB/LclPLMwcNvJNG6XkWWjLmR1vUrkrz7IFnZR7mlRxNmLNvMjt0Hc38QsHv/Ecqfl7tghpK9e/ZQunQU8M+3r2N+37mTmwYP5I0x7xAWdvxr0g3bVyDydTJly5XzXXJxQn4K3DnW2r8PPBb6x+j/QEREBOnpni+g2Lh4knfs8E1LSU7O89EbsXFxJCd7xlfMysri4MEDRMfEnDBP+dhYataqw5LFC33vpaenExEREYjVKDQ58xUXF8+O33KMM7ljR575yjlfVlYWBw8cICYm5rTt0zNCP19pGVmUKObp4Jy8+xDx5Y4/ZimubCQ79/6Zq83OvX/S+/GPaX7z2zz+9rcAHDycQbM68dzaswk/T76DZ29L4PpO9Xj6pva+diWKFSEtI7R35xIREWT4tq+4XOOYli9fPlebnPNlZWVx4KBn+wI4ePAgV/fqxvAnnuKSZicOu5uekU5EidDevkpERJCR4f3+is+9P12Qx7ivOceH/Xu+TsXp76/8FLg9xpgqeG/6NsZczVn+wNOo6GiOZmeTnp5Ox4ROzJk9i/2pqexPTWXO7Fl0TMj96I3Lr+jG5ImeZ3VN/ewT2rRtjzGG5B07SEtLA2B/airLli6marXj10+2bN5EzVqhPUxVdHQ02d58JXbqTFLSTFJTU0lNTSUpaSaJnTrnatOla3dfj65PP/mYtu07YIw5bfvNGzdSq3Zo52v/n+kUCQujeNEizFqxhYQmlYkqWYKokiVIaFKZWStyX4Y+z3s6EuD+vi0Z943ntNANIz+neu9XqdnnNR5+I4n3Z67lsbe+9bWrGn8e67ftLpT1CpSc21fHxM7MSZrl2z7mJM2iY2Lu7euKrt19PZ4///Rj2rbz7I+ZmZlcf+1V9Onbn15XXZOr3eZNm0J++8qZr4Q88pWQZ766+fbHzz79mLbtOuTrCG7zJmf3x/yMRXk7nvvLahpjkoGtQL+ARhUC2ndMZOnihbTrkMD9Dw2jfWvPL70HHn7Ud2Q28qnHadioCVd07Ub/QYO5ZchAGtatQXR0NGPHvw/Axg3rGfbwA77Tb3fefQ916l4MeMaAK1GiBBfk8Qs01CQkdGLxooV06JjAw488RqvmTQF4ZNhw3y/Bp54YTqPGTejarTuDBg9h8KD+1KlZlejoGCZM8jxxOSYm5qTtd+3aRYmIiDx/sYeapO9+ocXFF/Lt91t5dsICFr45GIBnxi8g9ZDn1/djN7Tl+w0pfLV4E20aVOSpmzpgrWXh2l8Z+nLujgJ/Vy76XNIz/uL3fbmPCENNh4RElixaSPuOCTzw8DDatWwGwIOPPOrbPkY8+TgNGzemS9fuDBg0mJsGD6B+7epEx8Twrnd//PTjD1m0cD779u31faG/+dZY6tVvwB+7dhEREeGK/bFjjnw9+MijtG1xCQAPDXvMl6+nnxxOo0ZN6NKtOwNvGMKNNwygXq1qRMfE8N6Eyb7PSuzQho0bfubwn39SvXIFXn/zbRI6dWZXEOQr32NRGmPOBcKstYdOO7PDCmMsyjWrV/HfV0cz5p3A3Q//31dHU6pUJAMGDQ7YMqBwxqJcvWoVr4x+ibHjJpx+5jP0yuhRREZGMmjwkIAtAwpnLMr6Vc/nrmsuZcizUwO2jDuvvoSDRzIZ9/XqgC0DCmcsyjWrV/Hay6N4692870MtCK+9MppSpUox8IbAbl8Q+LEo16xexasvj+LtQObr5VGUiowMeL7OaCzKY4wxw//2GgBr7VMFEl2Iqt+gIa3beG6ULFIkMAWidOkoel/vjoPlBg0b0rZd+4DmKyoqiuu9t2KEujWbdzFv9TbCwgxHjwZmYJ79f2bw/sy1Afnswla/QUNatw30/liaPn3dsX3Vb9CQNoHOV1SU4/k67RGcMebeHC9LAF2B9d4HnwYlPU3AP3qagH/0NAH/6GkC/tPTBPLvHx3BWWv/L+drY8yLwBcFFJuIiEhAnMlIJucAlQs6EBERkYKUn2tw6zj+XLgiQFk8I4qIiIgErfzcJtA1x99ZwC5rbWjfGSoiIq53ygJnjAkDvrLW1i2keERERArEKa/BWWuPAmuMMXpEtoiIhJT8nKIsD/xojFkOHD72prW2e8CiEhER+YfyU+CeDHgUIiIiBSw/Be4Ka+2DOd8wxvwHmBeYkERERP65/NwHl5jHe5cXdCAiIiIF6aRHcMaY24B/AZWNMTkHrCsFLAp0YCIiIv/EqU5Rvg98AzwLPJTj/UPW2n0BjUpEROQfOmmBs9YeAA4AfQovHBERkYJxJmNRioiIBD0VOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcSUVOBERcaX8PA8u5Fgs2Uet02GIS+365hGnQwgpZbu95HQIIWffV/c6HYIr6AhORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQVORERcSQUuQO687UZqVIqlZdMGvveeeepxWjdrSNvmjbmq++Xs3JniXIBBbuaM6dSrU4M6NavywvPPOR1O0PvvK6Np1uhiLm1cj8EDric9Pd3pkBwXX7YU05+/llVv38DKMYO4vWcjAIYPbMnyNwey9I0BTHv2asrHnHtCuwplS7F76l0MvbqJE2EHnVtuGkzFuPNp0uBip0PxmwpcgPTpO5APP//yhPfuGHovC5atYt6SlXS67ApefHaEQ9EFt+zsbIbedTtTp33DqrU/8dGUyaz/6SenwwpaKcnJvPn6q8xdtJylK9eSnZ3NJx9NcTosx2VlH+WhMXNpeOO7tL17Erd0b0DNC89j1EcruOTWcVx623i+WbaFh/s1P6Hd87e2Z+aKrQ5FHXz6DxjE519+43QYZ0QFLkBatGpNdHTMCe9FRkb6/j5y5AgYU9hhhYQVy5dTpUpVLqpcmWLFinHNdb35ctpUp8MKatlZWaSlpZGVlUVa2hEuKB/rdEiO+33fYVZv/gOAP9P+4udf9xFbpiSHjmT65jmnRFGsPd6mW4uqbP39AD9t31vY4QatVq3bEPO377JQoQJXyEY88RgX17iIjz+YzMOPPuF0OEEpJSWZ+PgKvtdxcfEkJyc7GFFwi42L486h91K3eiWqXxRHZGRpOiZ0cjqsoHLh+ZE0qFqOFT/vBOCJQa3YNOlmeneozdPjFwGeYnfvtZcwcsJiJ0OVAqQCV8gefeJp1m3YytXX9eHt/73udDhByeb8Se1ldLR7UqmpqXz15ResXb+FDb/s4Mjhw3wweaLTYQWNc0sUZfLw7tz/xre+o7cn3ltItb5jmDLnJ27t3hCAx/q34NVPV3I4/S8nw5UCpALnkKuv7c20qZ85HUZQiouLZ8eO33yvk5N3EBurU24nM3dOEhUrVaJM2bIULVqUbj17sWzpEqfDCgrhRcKYPLw7H8xZz9RFm3JN/3DOz/RsXR2ApjXLM/LGNvw8/ibu6NWI+3s38xU/CU3hTgdwNtmyeRNVqlYD4JuvplGteg2HIwpOTZo2ZfPmTWzbupXYuDg++mAK70143+mwglaFChfy3fJlHDlyhIiICOZ9O4eGjdQDEODNezqz4dd9vPLJSt97VWKj2JKyH4Auzauw8bd9ACTce7xjzrD+LTiclsmbX6wq1HilYAWswBljsoF13mVsBfpba/cHannB5qZB/Vi0YB579+6hbvVKPDRsOLNmTGfzpo2EhRkqXFiRF1/+r9NhBqXw8HBGvfwa3bp0Jjs7m4GDBlO7Th2nwwpaTS5pRo9eV9GmeRPCw8OpV78Bg4bc5HRYjmtRJ46+iXVY98tulr4xAIDHxy5g0GUXU61CDEePWn794yB3vTzL4UiD28B+1zN//lz27tlD1Ysq8OjwJxh0wxCnw8oXk9f1jgL5YGP+tNaW9P49DthorR0ZkIX9TYNGje2cBcsKY1GucE5xHcj7IzPrqNMhhJTzu7/kdAghZ99X9zodQshoeWlTvl/5XZ4X6QvrGtwSIM4YU8UY8/2xN40x1YwxK71/DzfGrDDG/GCMGWO8vQqMMXONMf8xxiw3xmw0xrQupJhFRCSEBbzAGWOKAB2BL6y1W4ADxpgG3sk3AO95/37NWtvUWlsXiAC65viYcGvtJcBQ4PGTLOdmY8x3xpjv9u7ZU/ArIiIiISWQBS7CGLMa2AvEAMdOdL8N3OAtfNcBx3oPtDfGLDPGrAM6ADkvunzq/X8lUCmvhVlrx1hrm1hrm5xXpkyBroiIiISeQBa4NGttA6AiUAy43fv+J8DleI7QVlpr9xpjSgCvA1dbay8G3gJK5PisDO//2QRJz8+0tDS6de5AdnY2kyeNp2n9WjStX4vJk8bnOX9GRgZDBlxPk3o1SWzXgl+3b/NNKxtZnLbNG9O2eWP6XtvL9/6NA/uyZXPurs2hKC0tjcQObcnOzmbi+HHUrVWNurWqMXH8uDznz8jIoN/111GnZlVat2jG9m3bfNNO1r5/395s3uSefF2R2J7s7GzenziOhnVr0LBuDd6fePJ8DerXmwZ1qtOhdXO2e7evX7dvp02LprRq1ohmjS7mnbfe9LW5oX8f12xfJYqFM/PF6wgLM56OJe8OYd27Q+ibmHfnpAvLRfL1f65h+ZsDmfHCdcSVKembNnXkVez89A4+earXCW3GP9KVKrFRgVyNQpOWlkanju18++PFtatzce3qp9wf+1/fm7q1qtGm5aW59se82g/o28fx/THgpyittQeAu4D7jDFFrbXpwAzgDeBd72zHitkeY0xJ4OpAx/VPTRr/Ll279+TggQO88OwIZn67iFlzF/PCsyPYn5qaa/6J48YSFRXFd2t/5rbb7+bJxx7xTYuIiGDekpXMW7KSSR8evzfuhhtv4dXRLxbK+gTauHfH0qPnlRw4cICRI55k/qJlLFi8nJEjniQ1j3y9N/YdoqOi+fHnzdx5978Z9siDAOzbt++k7W++5TZeevH5Ql2vQJk47l269ejFgQMHeG7k08yev4Q5C5by3Min88zX+PfGEhUdzeofN/KvO+/m8WEPAXBB+fLM+nYhC5d9z+z5Sxj94vPsTPEM8j3k5lsZ/dILhbpegTKwc12mLtxE6XOLM6xfc9rcNYnWd05kWL/mRJUsnmv+Z29uy6Skn7jk1nE8M2kxTw0+fml/1EcrGPL817najJm2mnuuvSSg61FYxr03lh49PdvXMyOfYt7CpcxftIxnRj6V9/747jtERUfxw/pN3HnXUB59xLN97du376Ttb7rlVl76P2f3x0LpZGKtXQWsAXp735oEWGCmd/p+PEdt64DPgRWFEdc/8fGHk7m8a3fmJM2kXfuORMfEEBUdTbv2HZk9a0au+b/5ahq9+/YHoHuvq5g/d06eI3bk1LxlK+Z9O4esrKyArENhmjJ5Et2692DWzBl07JhITEwM0dHRdOyYyMwZ03PN/+W0qfTtPxCAK6+6mrlzZmOtPWX7lq1aM2dOkivy9eGU97miW3fmzJpB+44JvvVt3zGB2TNz5+vrL6dyfV9PV/ieV17NPO/2VaxYMYoX93zBZ2ZkcPTo8R6gLVq2Zu6c2a7IV+8OtZi2ZDOJjSsx+/vtpB5KZ/+fGcz+fjudmlyUa/6aF57H3FXbAZi3+je6Nq/qmzZ39a8cOpJ7NJNFP+ygQ6OKFAkL/VF1Ppj8Pl279SBp5gw65Ni+OnRMYFYe++NX076gn3d/7HXV1cz91rM/nqp9y1at+dbh7StgBe7YLQI5Xnez1k7wvmwFjLXWZueY/qi1tqq1NsFae4O19gnv++2std95/95jra0UqJjzKzMzk+1bt3JhxUrs3JlCbI5xE2Pj4vN8DM7OlOPzhYeHE1m6NPv2egZ0TU9Pp0PrZnRq35KvcgwqHBYWxkWVq/DDujUBXqPAyszMZNvWX6hYqZJnnMkKOcaZjI8nJSX3OJM55zuWr717956yfVhYGFWqVGXtGhfka9svVKxYiZSUlFzjcqak5L19xeXcviKPb187fvuNFk0bULtaRYbe+wDlvaPChIWFUblKFdatDe18FQ0Po1L5KH7ddZDYMiXZsfuQb1rynkPElimZq826X3bTs5VnBJMeLasReW5xYkqVyDVfTtbClpRU6lUpV7ArUMgyMzPZmnN/zLV95bE/JiefuH3l3B9P0t63Pzq4fRX6UF3GmM+AAcDLhb3sgrJ37x4iS0cB+R838VTzrfn5F+YsWMaYsRMY9uC9bP1li2+esmXL8vvOnQUUuTP27NlD6ago4J/n63Tty5YtF/LP2du7Zw+lC3D7iq9QgcUrVrPqh428P3E8f+za5ZunbNly/B7i+SoTGcGBw57n3+Wdm9xtHh4zl9b14lnyen9a14snefchsrJPf3/j7v1HKH/euaedL5jt2bOHqALavvK1P+bxg6ywFHqBs9b2stbWs9aGbF/+iBIRZGR4dqjY2DhScoybmJK8gwsuKJ+rTWzc8fmysrI4eOAA0TGeR1CU9z7apNJFlWnZug3r1qz2tUtPz6BERESgVqVQRERE+B7AGRcXz47fcowzuWOHb/1zyjnfsXzFxMSctn16RjoRIZ6vEhERZPjyFZdrXM7y5fPevpJzbl8Hj29fx5SPjaVW7dosXrTA9156enrIb19pmVmUKOrpe5a8+xDxZUv5psWVKcXOvX/marNz32F6P/UFzf81gcffXQjAwRyP0TmZEkXDScsI7VO6ERERpGfk2B9zbV957I/x8SduXzn3x1O0T093dn/UYMtnICo6muzsbM+pxYROfDsnif2pqexPTeXbOUl0yONRJZdd0ZUpkzxnaL/47BNat22PMYb9qalkZHg6ie7ds4flS5dQvWYtX7stmzdSs1btwlmxAInOka/ETp1JSppJamoqqampJCXNJLFT51xtunTtzqQJnh5Zn37yMW3bd8AYc9r2mzdupFbt0B7WK2e+OiR2Zk7SLN/6zkmaRYfE3Pm6okt33vf24P38049p492+knfsIC0tDfA8dWDpksUnjIG6ZfMmatUK7Xzt/zODIkUMxYsWYdbKbSQ0rkRUyeJElSxOQuNKzFq5LVeb8yIjfI9jvL93M8bN+CFfy6oaH836EH9WXM7tK6FTZ2bn2L5mJ80iIY/98Yqu3Zjo3R8/++Rj2rbz7I+na795k7P7Y1B0uQ9F7TsmsHTJItq178h9Dz5CQlvPU4Hve2iY75fzs08/QYNGjbm8Szf6DRzMbTcOokm9mkRFR/P2e5MA2LhhPffc9S/CwsI4evQod99zv6+g/bFrFyUiIvI8Igw1CQmdWLxoIR06JvDwI4/RqnlTAB4ZNpwYb76eemI4jRo3oWu37gwaPITBg/pTp2ZVoqNjmDDJMxBuTEzMSdvv8uYrryOcUNM+IZElixfSvkMCDzw8jPatmgHw4COP+tZ35FOP07BRY67o2p3+gwZz8+ABNKhTnejoGMZ6B6fesGE9jz50v+900p1D76FO3YsB7/ZVIoILXJCvpJXbaVE3jm9X/cqzk5aw8NV+ADwzcQmphzxHK48NaMn3G3/nq6VbaFO/Ak8Nbo21loXrdjD0tdnHP+v/elO9QgwlI4qyedIt3PrSDJJWbqNc1DmkZ2Tx+77DjqxjQeqYkOjbHx965FFat/D0Dn142GN57483DGHIoAHUrVWN6OgYxk+cDHj2x5O1D4b9MWBjUTqpMMaiXLtmFa+/Opo33877vpGC8MZroylVKpJ+AwcHbBlQOGNRrl61ildGv8TYcRNOP/MZemX0KCIjIxk0OLADwRbGWJRrVq/iv6+MYszYvO+rLAj/fWU0pSJLMWBQYPNVGGNR1q9SjruuasyQ578J2DLuvLIxB49kMG56/o72/olAj0W5etUqXn15FO+8F7jt69WXR1EqMjLgAzMHw1iUrlOvfkNat/HcKBkokaWj6O3t+h3qGjRsSNt27QOar6ioKPoNGBiwzy9M9Rs0pHXbwG5fpaNKc30/d+RrzZY/mLfmN8IC2IV//58ZTJz5Y8A+vzA1aNiQNu0CvX1F+W4tcIqO4ERPE/CTnibgHz1NwH96mkD+6QhORETOOipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSipwIiLiSuFOBxAIRYzhnOKuXDUJAsXC9bvQH6lf3+d0CCEnuukdTocQMjI2/HrSadpTRUTElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgRETElVTgCsHMGdOpV6cGdWpW5YXnn3M6nJCgnPlH+fKP8pW3sDDDkskP8snLtwIQHXkOX75xB+umDufLN+4gqlSEb977Bnfih6mPs+azx0hoXsupkE9JBS7AsrOzGXrX7Uyd9g2r1v7ER1Mms/6nn5wOK6gpZ/5RvvyjfJ3cHde3Z8PWXb7X992QyNzlG7i4x1PMXb6B+27oBEDNyhdwTedGNLp6JN1vf52XH76WsDDjVNgnpQIXYCuWL6dKlapcVLkyxYoV45rrevPltKlOhxXUlDP/KF/+Ub7yFlcuista1eHdzxb73uvarh4Tpy0DYOK0ZXRrX8/3/kczvifzryy2p+xly297aFq3khNhn5IKXIClpCQTH1/B9zouLp7k5GQHIwp+ypl/lC//KF95e+H+qxj28uccPWp975U7rxS/7zkIwO97DlI2phQAcWVLs+P3VN98yX+kEluudOEGnA8qcAFmrc31njHBdygfTJQz/yhf/lG+cru8dV3+2HeIVet/y1+DPPKVR1odF+50AG4XFxfPjh3HN5rk5B3ExsY6GFHwU878o3z5R/nKrXmDynRtezGXtapD8WJFiTy3BGNHDOCPvYe4oEwkv+85yAVlItm97xAAyX/sJ/6CaF/7uHLR7Nx9wKnwT0pHcAHWpGlTNm/exLatW8nMzOSjD6bQpWt3p8MKasqZf5Qv/yhfuQ1/9QuqXvYYNbs8zoCH3mXuio0MfnQ8X81bR79uzQDo160ZX85dC8BXc9dyTedGFCsaTsXY86h6YVlW/LDNwTXIW1AcwRljKgDzgcbW2n3GmGjge+/k7tbadd75HgAqW2tvdShUv4WHhzPq5dfo1qUz2dnZDBw0mNp16jgdVlBTzvyjfPlH+cq/F9+dxcT/DGZgz+b8tjOVvg+8A8D6X37nk5mrWPXJMLKyjzL0uQ9PuHYXLExe56Od4C1eVa21Nxtj/gdsA1YBw4A2QCyeItjEWpt60g8CGjduYhct+y7AEYuIBEZ00zucDiFkZGz4kKNH/sjzImpQHMF5jQJWGmOGAq2AO621mcaYwcAAoAvwxOmKm4iICARRgbPW/mWMuR+YDnSy1mZ6Jw0FlgObrLUTTtbeGHMzcDNAhQsvDHC0IiIS7IKtk8nlwE6g7rE3rLUpwBzgjVM1tNaOsdY2sdY2KVumbGCjFBGRoBc0Bc4Y0wBIBC4F/m2MKZ9j8lHvv6CRlpZGYoe2ZGdnM3H8OOrWqkbdWtWYOH5cnvNnZGTQ7/rrqFOzKq1bNGP7tm2+aSdr379vbzZv2hToVSkUypd/lC//KF/+KVG8KDPfvpuwMMPU1/7FzvnP+8afzEuxouFMeO4Gfpj6OPPH38eF5WN80/p2a8a6qcNZN3U4fb09LgHGP3cDVS509mAjKAqc8dxl+QYw1Fr7K/AC8KKzUZ3auHfH0qPnlRw4cICRI55k/qJlLFi8nJEjniQ1NfdlwvfGvkN0VDQ//ryZO+/+N8MeeRCAffv2nbT9zbfcxksvPl+o6xUoypd/lC//KF/+GdijOVNnr+HoUcuo8UkMeXT8Kecf1LM5qYfSqNvjSV6d9C0j7+4BeAZjHnbz5bTp/yKt+73AsJsv9w3IPOajBdwzMCHg63IqQVHggJuAX621s7yvXwdqGmPaOhjTKU2ZPIlu3Xswa+YMOnZMJCYmhujoaDp2TGTmjOm55v9y2lT69h8IwJVXXc3cObOx1p6yfctWrZkzJ4msrKxCXbdAUL78o3z5R/nyT+8rmjDNe0/b3OUbOXQ445Tzd21Xj0neMSk/TVpFu0tqAJDYohazl/5M6sEj7D+UxuylP9OpZW0AFn2/hQ7NalCkiHNlJigKnPf62XU5Xmdbaxtba+d5Xw+y1n7sXIQnyszMZNvWX6hYqZJnXLsKOca1i48nJSX3uHY55wsPDyeydGn27t17yvZhYWFUqVKVtWvWBHiNAkv58o/y5R/lyz9Fw4tQKa4Mv+7cl+82seWOjz2ZnX2Ug3+mcV7UucSWjWLHrpxjUu4ntmwU4BkSbctve6hXPa5A4/dHUBS4ULNnzx5KR0UB+R/X7mTzna592bLl2Lkz5R9E6zzlyz/Kl3+UL/+UiS7JgUNH/GqTdw7zHJISy/Ec7t53iPJlnRuEWQXuDERERJCeng54x7X7Lce4djt2UL587nHtcs6XlZXFwQMHiImJOW379Ix0IiIicn1eKFG+/KN8+Uf58k9aeiYlihf1q03yruNjTxYpEkZkyQj2HTjsGZPy/JxjUkadMCZlieJFScv4q2ACPwMqcGcgOjqa7Oxs0tPTSezUmaSkmaSmppKamkpS0kwSO3XO1aZL1+5MmuDpkfXpJx/Ttn0HjDGnbb9540Zq1Q7tYYSUL/8oX/5Rvvyz/1AaRcLCKF4s/7dBfzVvna+H5JUJDZm3YiMAsxavJ6F5TaJKRRBVKoKE5jWZtXi9r13VC8uxfsvOgl0BPwTNjd6hJiGhE4sXLaRDxwQefuQxWjVvCsAjw4YTE+PpQvvUE8Np1LgJXbt1Z9DgIQwe1J86NasSHR3DhElTAIiJiTlp+127dlEiIoLy5cvnEUFoUb78o3z5R/nyT9LS9bRoWIVvl20g6Z2hVL/ofEpGFGfz9Ke59cn3SVqynsdu68L3P/3KV/PW8d7nixk7YgA/TH2c1IOH6f/QuwCkHjzCs29NZ+HEBwB4Zsx0Ug96Tn+WiylFekam73lyTgiasSgLUmGMRbl61SpeGf0SY8eddHCVf+yV0aOIjIxk0OAhAVtGYVG+/KN8+cdt+Qr0WJT1a8RzV78ODHns1LcH/BN39m3PwcPpjPt8ScCWAacei1KnKM9Qg4YNaduuPdnZ2QFbRlRUFP0GDAzY5xcm5cs/ypd/lC//rNmwg3nfbSQsLHAPet1/KI2J3lsLnKIjOBGRIKOnCeSfjuBEROSsowInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKupAInIiKuZKy1TsdQ4Iwxu4HtTseRhzLAHqeDCCHKl3+UL/8oX/4J1nxVtNaWzWuCKwtcsDLGfGetbeJ0HKFC+fKP8uUf5cs/oZgvnaIUERFXUoETERFXUoErXGOcDiDEKF/+Ub78o3z5J+TypWtwIiLiSjqCExERV1KBExERV1KBE3EJY4z2ZwkIY0xjY0xnp+Pwl3aIQmSMudDpGEKRMcY4HUMwM8bEAVhrj6rISUEzxhQFegHnOx2Lv7QzFALjEQm86v1f/GC9PaGMMUWcjiXYGGPCgTuNMbNBRe5MHfsRZYwpoh9UJ7LW/gX8CnQzxhQNpe0rZAINZdbjIHAN0MwYc4/TMYUaY8wVwPtOxxFMjDE1gJeAD4E/jDGfgYrcmbDWWmNMN+Bt4EVjTEWnY3KaMaaCMSbG+3I+kGqt/ctae9TJuPyhnSDAcn7RWGszgcPAv40xtzkXVUjaAew99uJs/5VtjKkNfIxnzNWtwE3AARW5M2OMqQU8iueLPAOYZoy5yNmonGOM6QKsBf7PGPM10BXoZYxp72xk/gl3OgA3M8aYY792jDGNgAPW2sXGmETgC2NMmLX2v85GGdyMMQlAPJ5TJDWMMRWttdvtWXwD57HT3cAoa+3YHO/fhec0+GfW2l7Hilwo/eJ2gnfffBD4yFr7rve9I8BnxpirrLVbHA3QGRcBX+L54dQXyAQ2A/WNMcnW2o1OBpdf+oUXIMaYOsDL3r8HA+8A7xpjXgZK4PlFdLtOV54o55GZMaYk0BDoA/QE2gNvGmPuM8Y8aIw5z5koHZcGJAOfgO86HN7T4LcBfxljPvG+d/RsP9rNh31AFNDEGFMWwFo7As8X/NfGmAgHY3PKZiDcWptlrR1nrZ0M3ABcDFzlPYMQ9FTgCliOL5NwoJwx5j3gcuBS4GZgFfAvYBeeDaafMSaq8CMNPt4j3mMdSqoDJa21L1hrO1tr78LzI+ELIAuogedL6Wx0Lp7C3wrAWpuV43RkEeA54DdjzDve6Wft0W5ecnQoqWGMqWSt3Qb0w5PX240xZQCstY8CXay1aY4FW4iMMR2NMXcYY1oCG4ALj/XQBbDW/gyMBhoBl3t7VwY1FbiCdz6AtXYN8CSQDTSw1mZ4N5D5QGmgnbV2GdDCWrvfqWCDSY7idj+eU3ATjTGjjDHx3lkMnuHlRltrB5+lp47wbi+v4vkl3cD79rEfVp2BW/F8EVljzLmFHV+w83Yo6Q5MAJ4wxrwAlAQGA/WB+3MUuc3ORVroiuEpXg8AbwG1gBuMMd29PcGLWGvXAQ8B73t7VwY1FbgCZIypCaR4v5RvsNauB14A1htj3gCw1v4C/AHU9TbLdCba4GSM6Qi0t9Z2BrYAFYEU7+Q5QIwxJlyn3fgM2AncaozpABz1/vJ+Ck9v07J4vqyKOxdicDLGNMXzJX0FnlNx3YB78fzwvA3PabhoxwJ0iLX2G+8Pxx54rr1NB5oBQ4GPgAXGmGustVustTsdDDXfNNhyATLGVACm4DmN1hFPz7/PgVTgSqAp8C6ejWdAqFyoDSRjzMXACKCn95d1Wzyn36Lx7FzdrbWZxph6wFHgkLU2GJ/WXuiMMecD1+I55f09UAV4zlr7uffIbr/39Jt4GWOKA3HAeUAM8AxwH3AHnqPg4cCGUDg6CYScnZKMMf8G6llrbzDGlAcSgYXeH+khQQWugBljRgGxeHoeXYunsJXHc8j/HDATGK4vHg9jTCk8RR889wlWA97Ec2R7pbX2iDHmdu+0y8+W6yH+8Ba6o0Bxa+0Op+MJVt5u/+OBHtbafcaYp/F8Yc8wxgzDc133We+Zl7Oe917Akdbafk7HcqZ0irKA5Dhl9iBggTJ4Tq01BlbjKXRbgMdV3MAYcwGAtfYQnh8DGcCH3qPaL/Ccxr3DGPMAnmtKt6u45c1au8tau/tYcdPp2xPl6ICzE08nr/Le1+HA68aYHnh66f5Xxe0EB4BGxphmTgdypnQEV4C8XyzFgMeAyniugTzkPWVUGc8po31OxhgMvNcqf8JzG8VP1tq3vJ0hXgHOtdb29t5o2hA4Bxjv7aAjkm/HeuUaY+JzFP//AJWstdd5Xz+D5zrvFGvtNAfDDTre77NhwFhrbcrp5g9GKnABYDxDKC0AXrXWPu10PMEmj2uVO4EPgB/wXA8pa63t4523iLU226lYJfQYY2LxdBj5E9gNzMXTYWIKnrMorwMfWGuTvPOf4z0V7rtNRTyMMeHW2iyn4zhTOkUZANbaDXhOVRYxxpzjdDzBxlr7G7AczxHuFXi+fG7Gc33kf0AVY8zr3tk1Cofkm/fswDQ8t1E8C1yG5xR4CeBuPDdvlwLqHWtjrT3i/V/F7W9CubiBClwgLcFz/U1yOMm1yp147j/ahKcX22a8o8DoS0fyyzu6xmQ8ZwEG4N0HvfdLPm6tvQ1IwvO9d58xpr5jwUqh0CnKADp26sPpOILNaa5VVgP2WGtTnYxRQo8xphUw31ob5n1dFc8N78Pw3F7yi/f9InhuZt5grf3UoXClEOgILoBU3PJmPTLwjCSRAEyy1n7unbZJxU3OhLV2IXCFMebYfVrN8AyR9w4wwRgzxRjTyXtNNx7PqC/iYnqagDjGWrvBGPMgUFFHu1IQrLXTveMp/gmsB8rhuaE7As/oJTl7Mb/qQIhSiHSKUhzl7RTwAnCdCpwUFO/wZeOttfEnma7euWcBFThxnI7eJBCMMZcD44AaOu19dlKBExHXMsZcARyx1s51OhYpfCpwIuJ6uon77KQCJyIirqTbBERExJVU4ERExJVU4ERExJVU4ESCgDGmnTHmS+/f3Y0xD51i3ihjzL/OYBlPGGPuy+/7f5vnPWPM1X4sq5Ix5gd/YxQpSCpwIgHkHffQL9baL6y1z51ilijA7wIncrZRgRM5A94jlJ+NMeOMMWuNMR8fezSSMWabMWa4MWYhcI0xppMxZokx5ntjzEfGmJLe+S7zfsZCPE98P/bZg4wxr3n/Pt8Y85kxZo33XwvgOTyPFFptjHnBO9/9xpgV3liezPFZw4wxG4wxSUCNfKzXTd7PWWOM+eRvj3tKMMYsMMZsNMZ09c5fxBjzQo5l3/JPcytSUFTgRM5cDWCMtbYecJATj6rSrbWt8Dye5VEgwVrbCPgOuMcYUwJ4C+gGtAYuOMkyXgHmWWvr43nqwo94xlTcYq1tYK293xjTCagGXAI0ABobY9oYYxoDvfE8Gf1KoGk+1ulTa21T7/LWA0NyTKsEtAW6AG9612EIcMBa29T7+TcZYy7Kx3JEAk6DLYucud+stYu8f08E7gJe9L7+wPv/pUBtYJH3UXjF8DynrCaw1Vq7CcAYMxHPQ1//rgOeZ5vhHTvxgDEm+m/zdPL+W+V9XRJPwSsFfHZsGDRjzBf5WKe6xpgReE6DlgRm5Jj2obX2KLDJO2J/Te9y6+W4Plfau+yN+ViWSECpwImcub+PkpDz9WHv/waYZa3tk3NGY0yDPNqfKQM8a63939+WMfQMlvEe0NNau8YYMwhol2NaXutrgDuttTkLIcaYSn4uV6TA6RSlyJm70BjT3Pt3H2BhHvMsBVp6H76JMeYcY0x14GfgImNMlRzt8zIbuM3btogxJhI4hOfo7JgZwOAc1/bijDHlgPlAL2NMhDGmFJ7ToadTCthpjCkK9P3btGuMMWHemCsDG7zLvs07P8aY6saYc/OxHJGAU4ETOXPrgYHGmLV4njn2xt9nsNbuBgYBk73zLQVqWmvT8ZyS/MrbyWT7SZZxN9DeGLMOWAnUsdbuxXPK8wdjzAvW2pnA+8AS73wfA6Wstd/jOVW6GvgEWJCPdXoMWAbMwlOEc9oAzAO+AW71rsPbwE/A997bAv6HzgxJkNBYlCJnwHsK7ktrbV2nYxGRvOkITkREXElHcCIi4ko6ghMREVdSgRMREVdSgRMREVdSgRMREVdSgRMREVdSgRMREVf6fzKUfSOf21iIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    plot_confusion_matrix(conf_mat=cm_hist[i], class_names=encoder_dancer.classes_, show_normed=True, \n",
    "                          figsize=(7,7), hide_spines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Abi       0.88      1.00      0.94       304\n",
      "        Alex       1.00      0.96      0.98       240\n",
      "          CJ       0.97      0.95      0.96       400\n",
      "        Ryan       0.99      0.89      0.94       256\n",
      "          XY       1.00      1.00      1.00       400\n",
      "\n",
      "    accuracy                           0.97      1600\n",
      "   macro avg       0.97      0.96      0.96      1600\n",
      "weighted avg       0.97      0.97      0.97      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Abi       0.90      1.00      0.95       304\n",
      "        Alex       1.00      0.97      0.99       240\n",
      "          CJ       0.98      0.93      0.96       400\n",
      "        Ryan       0.96      0.91      0.93       256\n",
      "          XY       0.98      1.00      0.99       400\n",
      "\n",
      "    accuracy                           0.96      1600\n",
      "   macro avg       0.97      0.96      0.96      1600\n",
      "weighted avg       0.97      0.96      0.96      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Abi       0.90      1.00      0.95       304\n",
      "        Alex       1.00      0.97      0.99       240\n",
      "          CJ       0.98      0.94      0.96       400\n",
      "        Ryan       0.97      0.91      0.94       256\n",
      "          XY       0.99      1.00      0.99       400\n",
      "\n",
      "    accuracy                           0.97      1600\n",
      "   macro avg       0.97      0.96      0.97      1600\n",
      "weighted avg       0.97      0.97      0.97      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Abi       0.87      0.99      0.93       304\n",
      "        Alex       1.00      0.96      0.98       240\n",
      "          CJ       0.97      0.93      0.95       400\n",
      "        Ryan       0.96      0.89      0.93       256\n",
      "          XY       0.99      1.00      1.00       400\n",
      "\n",
      "    accuracy                           0.96      1600\n",
      "   macro avg       0.96      0.95      0.96      1600\n",
      "weighted avg       0.96      0.96      0.96      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Abi       0.91      1.00      0.95       304\n",
      "        Alex       1.00      0.97      0.99       240\n",
      "          CJ       0.98      0.91      0.94       400\n",
      "        Ryan       1.00      0.97      0.98       256\n",
      "          XY       0.96      1.00      0.98       400\n",
      "\n",
      "    accuracy                           0.97      1600\n",
      "   macro avg       0.97      0.97      0.97      1600\n",
      "weighted avg       0.97      0.97      0.97      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"\\nClassification Report for fold {}:\" .format(i+1))\n",
    "    print(classification_report_hist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good 3 april 1:54am\n",
    "def perform_mlp(X_test, y_test, fold, pca):\n",
    "    start_time = timer()\n",
    "    k = fold\n",
    "    perform_pca = pca\n",
    "    number_of_classes = 3\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    #kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    \n",
    "# # Trial 3: ave accuracy 81%\n",
    "#     mlp_adam = nn.MLPClassifier(hidden_layer_sizes=(200, 150), max_iter=500, activation='tanh', solver='adam',\n",
    "#                                       batch_size = minSamples, validation_fraction=0.2, n_iter_no_change=20,\n",
    "#                                       alpha=1e-4, early_stopping=True, verbose=0, tol=1e-6, random_state=None, \n",
    "#                                       learning_rate_init=0.001, shuffle=False) \n",
    "\n",
    "    acc_scores = []\n",
    "    cv_iteration = 1\n",
    "    cv_pca_iteration = 1\n",
    "    train_histories.clear()\n",
    "    cm_hist.clear()\n",
    "    classification_report_hist.clear()\n",
    "#     train_histories_mlp_sgd.clear()\n",
    "#     train_histories_mlp_adam.clear()\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        \n",
    "        if perform_pca == False:\n",
    "            print('\\nTraining model and cross validate using fold #{}...\\n ' .format(cv_iteration))\n",
    "            cv_iteration += 1\n",
    "        \n",
    "        X_train , X_val = X.iloc[train_index,:], X.iloc[val_index,:]\n",
    "        y_train , y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        y_val_without_transform = y_val\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        y_train = to_categorical(y_train, number_of_classes)\n",
    "        y_val = to_categorical(y_val, number_of_classes)\n",
    "        y_test_categorical = to_categorical(y_test, number_of_classes)\n",
    "        \n",
    "        if perform_pca == True:\n",
    "            print('\\nTraining model with PCA and cross validate using fold #{}...\\n ' .format(cv_pca_iteration))\n",
    "            cv_pca_iteration += 1\n",
    "            pca = PCA(n_components = 12)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_val = pca.transform(X_val)\n",
    "#             pca.explained_variance_ratio_\n",
    "\n",
    "        def mlp_model():\n",
    "            model = Sequential()\n",
    "#             model.add(Flatten(input_shape=X_train[0].shape))\n",
    "            model.add(Dense(units=32, kernel_initializer='uniform', activation='relu', input_shape=X_train[0].shape))\n",
    "#             model.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(units=number_of_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            return model\n",
    "        \n",
    "        mlp = mlp_model()\n",
    "        print(mlp.summary())        \n",
    "        \n",
    "#         checkpoint_filepath=\"MLP_weights_checkpoint.hdf5\"\n",
    "                \n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.00001, patience=20, verbose=1),\n",
    "#             ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only=True, monitor='val_accuracy',\n",
    "#                             verbose=1, save_best_only=True, mode='max')  \n",
    "        ] \n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_val, y_val),\n",
    "                                  callbacks=[my_callbacks], shuffle=True)\n",
    "        \n",
    "        mlp_pred = np.argmax(mlp.predict(X_test_scaled), axis=-1)\n",
    "        scores = mlp.evaluate(X_test_scaled, y_test_categorical, batch_size=64, verbose=0)\n",
    "        acc_scores.append(scores[1])\n",
    "        train_histories.append(history.history)\n",
    "        \n",
    "#         mlp_weights = mlp.get_weights()\n",
    "#         print(\"MLP Weights:\", mlp_weights)\n",
    "        \n",
    "#         mlp.save('saved_models/MLP_99.6_accuracy')\n",
    "        \n",
    "        print('y_test\\n', y_test)\n",
    "        print('')\n",
    "        print('mlp_pred\\n', mlp_pred)\n",
    "        \n",
    "        cm_hist.append(confusion_matrix(y_test, mlp_pred))\n",
    "        classification_report_hist.append(classification_report(y_test, mlp_pred, target_names=encoder.classes_))\n",
    "        \n",
    "\n",
    "    end_time = timer()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return mlp, acc_scores, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good 3 april 1:09am\n",
    "def perform_mlp(X_val, y_val, fold, pca):\n",
    "    start_time = timer()\n",
    "    k = fold\n",
    "    perform_pca = pca\n",
    "    number_of_classes = 3\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    #kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    \n",
    "# # Trial 3: ave accuracy 81%\n",
    "#     mlp_adam = nn.MLPClassifier(hidden_layer_sizes=(200, 150), max_iter=500, activation='tanh', solver='adam',\n",
    "#                                       batch_size = minSamples, validation_fraction=0.2, n_iter_no_change=20,\n",
    "#                                       alpha=1e-4, early_stopping=True, verbose=0, tol=1e-6, random_state=None, \n",
    "#                                       learning_rate_init=0.001, shuffle=False) \n",
    "\n",
    "    acc_scores = []\n",
    "    cv_iteration = 1\n",
    "    cv_pca_iteration = 1\n",
    "    train_histories.clear()\n",
    "    cm_hist.clear()\n",
    "    classification_report_hist.clear()\n",
    "#     train_histories_mlp_sgd.clear()\n",
    "#     train_histories_mlp_adam.clear()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        if perform_pca == False:\n",
    "            print('\\nTraining model and cross validate using fold #{}...\\n ' .format(cv_iteration))\n",
    "            cv_iteration += 1\n",
    "        \n",
    "        X_train , X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train , y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         print('X_train', X_train)\n",
    "#         print('X_test', X_test)\n",
    "\n",
    "#         print('X_train and X_test shape before standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "\n",
    "        y_test_without_transform = y_test\n",
    "\n",
    "#         print('X_train before standard scaler: ', X_train)\n",
    "#         print('X_test before standard scaler: ', X_test)        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_val_std_scaler = scaler.transform(X_val)\n",
    "        \n",
    "#         print('X_train and X_test shape after standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#         X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)        \n",
    "\n",
    "        \n",
    "        y_train = to_categorical(y_train, number_of_classes)\n",
    "        y_test = to_categorical(y_test, number_of_classes)\n",
    "        y_val_categorical = to_categorical(y_val, number_of_classes)\n",
    "        \n",
    "#         X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)     \n",
    "        \n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "        if perform_pca == True:\n",
    "            print('\\nTraining model with PCA and cross validate using fold #{}...\\n ' .format(cv_pca_iteration))\n",
    "            cv_pca_iteration += 1\n",
    "            pca = PCA(n_components = 4) # n=100 is the usual consensus in HAR\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "            pca.explained_variance_ratio_\n",
    "\n",
    "        def mlp_model():\n",
    "            model = Sequential()\n",
    "#             model.add(Flatten(input_shape=X_train[0].shape))\n",
    "            model.add(Dense(units=32, kernel_initializer='uniform', activation='relu', input_shape=X_train[0].shape))\n",
    "#             model.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(units=number_of_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            return model\n",
    "        \n",
    "        mlp = mlp_model()\n",
    "        print(mlp.summary())        \n",
    "        \n",
    "#         checkpoint_filepath=\"MLP_weights_checkpoint.hdf5\"\n",
    "                \n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.00001, patience=20, verbose=1),\n",
    "#             ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only=True, monitor='val_accuracy',\n",
    "#                             verbose=1, save_best_only=True, mode='max')  \n",
    "        ] \n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test, y_test),\n",
    "                                  callbacks=[my_callbacks], shuffle=True)\n",
    "        \n",
    "        mlp_pred = np.argmax(mlp.predict(X_val_std_scaler), axis=-1)\n",
    "        scores = mlp.evaluate(X_val_std_scaler, y_val_categorical, batch_size=64, verbose=0)\n",
    "        acc_scores.append(scores[1])\n",
    "        train_histories.append(history.history)\n",
    "        \n",
    "#         mlp_weights = mlp.get_weights()\n",
    "#         print(\"MLP Weights:\", mlp_weights)\n",
    "        \n",
    "#         mlp.save('saved_models/MLP_99.6_accuracy')\n",
    "        \n",
    "        print('y_test\\n', y_val)\n",
    "        print('')\n",
    "        print('mlp_pred\\n', mlp_pred)\n",
    "        \n",
    "        cm_hist.append(confusion_matrix(y_val, mlp_pred))\n",
    "        classification_report_hist.append(classification_report(y_val, mlp_pred, target_names=encoder.classes_))\n",
    "        \n",
    "\n",
    "    end_time = timer()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return mlp, acc_scores, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with PCA and cross validate using fold #1...\n",
      " \n",
      "Model: \"sequential_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_318 (Dense)            (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_319 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 995\n",
      "Trainable params: 995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "49/49 [==============================] - 1s 4ms/step - loss: 1.0894 - accuracy: 0.5723 - val_loss: 0.9847 - val_accuracy: 0.7008\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8597 - accuracy: 0.7653 - val_loss: 0.4201 - val_accuracy: 0.9184\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.9389 - val_loss: 0.1599 - val_accuracy: 0.9663\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9788 - val_loss: 0.0883 - val_accuracy: 0.9832\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.9891 - val_loss: 0.0639 - val_accuracy: 0.9870\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9860 - val_loss: 0.0520 - val_accuracy: 0.9909\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9908 - val_loss: 0.0452 - val_accuracy: 0.9883\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9902 - val_loss: 0.0402 - val_accuracy: 0.9896\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9893 - val_loss: 0.0367 - val_accuracy: 0.9909\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9928 - val_loss: 0.0332 - val_accuracy: 0.9909\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9854 - val_loss: 0.0298 - val_accuracy: 0.9896\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0272 - val_accuracy: 0.9935\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.0238 - val_accuracy: 0.9935\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9889 - val_loss: 0.0216 - val_accuracy: 0.9935\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9896 - val_loss: 0.0190 - val_accuracy: 0.9961\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.9921 - val_loss: 0.0173 - val_accuracy: 0.9961\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9938 - val_loss: 0.0160 - val_accuracy: 0.9987\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0142 - val_accuracy: 0.9974\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9974 - val_loss: 0.0126 - val_accuracy: 0.9974\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0113 - val_accuracy: 0.9974\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.0105 - val_accuracy: 0.9974\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.0096 - val_accuracy: 0.9974\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9987\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 0.9987\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9997 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9981 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9986 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9968 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9980 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9978 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 144/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 164/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 00164: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 1 1 1]\n",
      "\n",
      "Training model with PCA and cross validate using fold #2...\n",
      " \n",
      "Model: \"sequential_126\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_321 (Dense)            (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 995\n",
      "Trainable params: 995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeapcl/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 5ms/step - loss: 1.0914 - accuracy: 0.6667 - val_loss: 0.9890 - val_accuracy: 0.9144\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8846 - accuracy: 0.8973 - val_loss: 0.4512 - val_accuracy: 0.9702\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.9699 - val_loss: 0.1361 - val_accuracy: 0.9844\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9820 - val_loss: 0.0749 - val_accuracy: 0.9857\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9863 - val_loss: 0.0591 - val_accuracy: 0.9857\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9910 - val_loss: 0.0505 - val_accuracy: 0.9857\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0441 - accuracy: 0.9893 - val_loss: 0.0431 - val_accuracy: 0.9857\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.0382 - val_accuracy: 0.9857\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.9923 - val_loss: 0.0346 - val_accuracy: 0.9883\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.0312 - val_accuracy: 0.9857\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9902 - val_loss: 0.0270 - val_accuracy: 0.9883\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.0255 - val_accuracy: 0.9883\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.0224 - val_accuracy: 0.9896\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 0.0205 - val_accuracy: 0.9896\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.0162 - val_accuracy: 0.9935\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.0156 - val_accuracy: 0.9922\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9947 - val_loss: 0.0126 - val_accuracy: 0.9948\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.0113 - val_accuracy: 0.9987\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 0.0108 - val_accuracy: 0.9987\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.0095 - val_accuracy: 0.9974\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0089 - val_accuracy: 0.9987\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.0094 - val_accuracy: 0.9987\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9958 - val_loss: 0.0085 - val_accuracy: 0.9987\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.0063 - val_accuracy: 0.9987\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9973 - val_loss: 0.0061 - val_accuracy: 0.9987\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9982 - val_loss: 0.0064 - val_accuracy: 0.9987\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9987\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9974 - val_loss: 0.0037 - val_accuracy: 0.9987\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9987\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9989 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 00080: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 1 1 1]\n",
      "\n",
      "Training model with PCA and cross validate using fold #3...\n",
      " \n",
      "Model: \"sequential_127\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_324 (Dense)            (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 995\n",
      "Trainable params: 995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeapcl/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 5ms/step - loss: 1.0908 - accuracy: 0.5016 - val_loss: 0.9987 - val_accuracy: 0.9676\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.8759 - accuracy: 0.9720 - val_loss: 0.3970 - val_accuracy: 0.9676\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.9762 - val_loss: 0.1179 - val_accuracy: 0.9805\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9831 - val_loss: 0.0751 - val_accuracy: 0.9857\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9893 - val_loss: 0.0601 - val_accuracy: 0.9870\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9911 - val_loss: 0.0530 - val_accuracy: 0.9883\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9900 - val_loss: 0.0461 - val_accuracy: 0.9870\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9899 - val_loss: 0.0422 - val_accuracy: 0.9870\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9943 - val_loss: 0.0386 - val_accuracy: 0.9883\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9911 - val_loss: 0.0345 - val_accuracy: 0.9883\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9909 - val_loss: 0.0306 - val_accuracy: 0.9896\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0272 - val_accuracy: 0.9922\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 0.0252 - val_accuracy: 0.9922\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.9916 - val_loss: 0.0222 - val_accuracy: 0.9922\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9920 - val_loss: 0.0191 - val_accuracy: 0.9922\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.0159 - val_accuracy: 0.9935\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0122 - val_accuracy: 0.9961\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9992 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9974 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9999 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 9.9528e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 9.7573e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 9.6569e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 9.4955e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 9.3869e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 9.4040e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.2380e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 9.0151e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 8.8808e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.7867e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 8.8423e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 8.8155e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 8.7986e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 8.7936e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 8.7772e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 8.7662e-04 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 8.7593e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.7288e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 8.7195e-04 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.6925e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 8.6828e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 8.6576e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.6627e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9986 - val_loss: 8.6813e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9986 - val_loss: 8.6607e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 8.6573e-04 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 8.6393e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 8.6423e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 8.6225e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 8.6395e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 8.6230e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 8.6222e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 8.6199e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 8.6214e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.6193e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.6155e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 8.6153e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 8.6113e-04 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 8.6113e-04 - val_accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 8.6122e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 8.6098e-04 - val_accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.6055e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9990 - val_loss: 8.6027e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9987 - val_loss: 8.5985e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 8.5958e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 8.5936e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.5928e-04 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 8.5947e-04 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 8.5932e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 8.5922e-04 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 8.5896e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 8.5894e-04 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.5893e-04 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 8.5890e-04 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 8.5886e-04 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 8.5883e-04 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 8.5882e-04 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.5880e-04 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.5885e-04 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 8.5884e-04 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 8.5883e-04 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.5882e-04 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 8.5880e-04 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 8.5879e-04 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.5878e-04 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.5877e-04 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.5878e-04 - val_accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9979 - val_loss: 8.5881e-04 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 8.5879e-04 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.5877e-04 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9992 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 8.5876e-04 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 139/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9982 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 8.5875e-04 - val_accuracy: 1.0000\n",
      "Epoch 00147: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 1 1 1]\n",
      "\n",
      "Training model with PCA and cross validate using fold #4...\n",
      " \n",
      "Model: \"sequential_128\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_327 (Dense)            (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 995\n",
      "Trainable params: 995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeapcl/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 4ms/step - loss: 1.0899 - accuracy: 0.4866 - val_loss: 0.9951 - val_accuracy: 0.9287\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8798 - accuracy: 0.9472 - val_loss: 0.4188 - val_accuracy: 0.9728\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.9669 - val_loss: 0.1129 - val_accuracy: 0.9883\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1067 - accuracy: 0.9820 - val_loss: 0.0593 - val_accuracy: 0.9935\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9840 - val_loss: 0.0415 - val_accuracy: 0.9948\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9904 - val_loss: 0.0327 - val_accuracy: 0.9948\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9863 - val_loss: 0.0255 - val_accuracy: 0.9948\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.0209 - val_accuracy: 0.9974\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.0176 - val_accuracy: 0.9961\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.9911 - val_loss: 0.0147 - val_accuracy: 0.9961\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.9940 - val_loss: 0.0122 - val_accuracy: 0.9974\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.0099 - val_accuracy: 0.9987\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9961 - val_loss: 0.0076 - val_accuracy: 0.9987\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9935 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.0057 - val_accuracy: 0.9987\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9989 - val_loss: 0.0055 - val_accuracy: 0.9987\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9989 - val_loss: 0.0042 - val_accuracy: 0.9987\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9972 - val_loss: 0.0039 - val_accuracy: 0.9987\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9988 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9992 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9997 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9999 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9996 - val_loss: 9.9104e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 9.8463e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 9.6589e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9996 - val_loss: 9.1530e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 9.2074e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 9.1739e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 9.0464e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 9.0181e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 9.3686e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 9.0380e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 8.8669e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 8.9078e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 8.9295e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 8.9510e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 8.9736e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 8.9649e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9997 - val_loss: 8.9556e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 8.8846e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 8.8341e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 8.8273e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 8.8046e-04 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 8.8453e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 8.8806e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 8.8435e-04 - val_accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 8.8523e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 8.8807e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 8.8063e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 8.8690e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 8.8738e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 8.8782e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 8.8951e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 8.8985e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 8.8983e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 8.9022e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 8.8892e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 8.8829e-04 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 8.8834e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 8.8885e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9998 - val_loss: 8.8850e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 8.8819e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 8.8813e-04 - val_accuracy: 1.0000\n",
      "Epoch 00083: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 1 1 1]\n",
      "\n",
      "Training model with PCA and cross validate using fold #5...\n",
      " \n",
      "Model: \"sequential_129\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_330 (Dense)            (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_331 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 995\n",
      "Trainable params: 995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeapcl/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 4ms/step - loss: 1.0889 - accuracy: 0.3997 - val_loss: 0.9739 - val_accuracy: 0.4656\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.8694 - accuracy: 0.7467 - val_loss: 0.4966 - val_accuracy: 0.9442\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.9593 - val_loss: 0.1478 - val_accuracy: 0.9831\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9818 - val_loss: 0.0767 - val_accuracy: 0.9896\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9908 - val_loss: 0.0604 - val_accuracy: 0.9896\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9900 - val_loss: 0.0494 - val_accuracy: 0.9870\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.9917 - val_loss: 0.0435 - val_accuracy: 0.9870\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 0.0378 - val_accuracy: 0.9883\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9928 - val_loss: 0.0355 - val_accuracy: 0.9857\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9911 - val_loss: 0.0319 - val_accuracy: 0.9870\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9916 - val_loss: 0.0287 - val_accuracy: 0.9896\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0267 - val_accuracy: 0.9909\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9924 - val_loss: 0.0236 - val_accuracy: 0.9935\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.0212 - val_accuracy: 0.9948\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9968 - val_loss: 0.0194 - val_accuracy: 0.9935\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0182 - val_accuracy: 0.9948\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.0163 - val_accuracy: 0.9935\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 0.9967 - val_loss: 0.0162 - val_accuracy: 0.9948\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9979 - val_loss: 0.0135 - val_accuracy: 0.9974\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.0140 - val_accuracy: 0.9961\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9986 - val_loss: 0.0117 - val_accuracy: 0.9974\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 0.0103 - val_accuracy: 0.9974\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0098 - val_accuracy: 0.9974\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9990 - val_loss: 0.0084 - val_accuracy: 0.9974\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.0089 - val_accuracy: 0.9974\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.0067 - val_accuracy: 0.9974\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9992 - val_loss: 0.0061 - val_accuracy: 0.9974\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0054 - val_accuracy: 0.9987\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0055 - val_accuracy: 0.9987\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9976 - val_loss: 0.0058 - val_accuracy: 0.9974\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0050 - val_accuracy: 0.9987\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0051 - val_accuracy: 0.9987\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.0057 - val_accuracy: 0.9974\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0046 - val_accuracy: 0.9974\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.0052 - val_accuracy: 0.9974\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0034 - val_accuracy: 0.9987\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9991 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9978 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9982 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 00085: early stopping\n",
      "y_test\n",
      " [0 0 0 ... 2 2 2]\n",
      "\n",
      "mlp_pred\n",
      " [1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeapcl/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mlp_model, acc_scores, time_taken = perform_mlp(X_test, y_test, fold=5, pca=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - prediction accuracy of each fold:\n",
      " [0.30000001192092896, 0.30000001192092896, 0.30000001192092896, 0.30000001192092896, 0.30000001192092896]\n",
      "\n",
      "MLP - average accuracy in 5-fold = 0.30000001192092896 with std. deviation 0.0\n",
      "\n",
      "Time taken: 51.27 seconds\n"
     ]
    }
   ],
   "source": [
    "print('MLP - prediction accuracy of each fold:\\n {}'.format(acc_scores))\n",
    "print('')\n",
    "print('MLP - average accuracy in 5-fold = {} with std. deviation {}' .format(np.mean(acc_scores), np.std(acc_scores)))\n",
    "print('')\n",
    "print('Time taken: {:.2f} seconds' .format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "len(class_names) should be equal to number ofclasses in the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-e494bf6e27c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     plot_confusion_matrix(conf_mat=cm_hist[i], class_names=encoder.classes_, show_normed=True, \n\u001b[0m\u001b[1;32m      3\u001b[0m                           figsize=(7,7), hide_spines=False)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlxtend/plotting/plot_confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(conf_mat, hide_spines, hide_ticks, figsize, cmap, colorbar, show_absolute, show_normed, class_names, figure, axis)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Both show_absolute and show_normed are False'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         raise AssertionError('len(class_names) should be equal to number of'\n\u001b[0m\u001b[1;32m     73\u001b[0m                              'classes in the dataset')\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: len(class_names) should be equal to number ofclasses in the dataset"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    plot_confusion_matrix(conf_mat=cm_hist[i], class_names=encoder.classes_, show_normed=True, \n",
    "                          figsize=(7,7), hide_spines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         gun       0.96      0.98      0.97       432\n",
      "        hair       0.98      1.00      0.99       480\n",
      "    sidepump       1.00      0.98      0.99       688\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.99      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         gun       0.96      0.98      0.97       432\n",
      "        hair       0.98      1.00      0.99       480\n",
      "    sidepump       1.00      0.98      0.99       688\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         gun       0.96      0.98      0.97       432\n",
      "        hair       0.98      1.00      0.99       480\n",
      "    sidepump       1.00      0.98      0.99       688\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         gun       0.96      0.98      0.97       432\n",
      "        hair       0.98      1.00      0.99       480\n",
      "    sidepump       1.00      0.98      0.99       688\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.99      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Classification Report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         gun       0.96      0.98      0.97       432\n",
      "        hair       0.98      1.00      0.99       480\n",
      "    sidepump       1.00      0.98      0.99       688\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"\\nClassification Report for fold {}:\" .format(i+1))\n",
    "    print(classification_report_hist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAHwCAYAAACPE1g3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABkuUlEQVR4nO3deXhU5d3/8fc3GwkEEvYdAUVZkrDIooAsitbdolKxqEUftdSlrT5Sqb+qPNo+tVTFarXUti5VBKp1oUrtowICLpWASAURUcIue0JWQpL798eZCZNJAjMkk8nyeV3XXDNnmXO+52QIn7lzn/uYcw4REREREQlNTLQLEBERERFpSBSgRURERETCoAAtIiIiIhIGBWgRERERkTAoQIuIiIiIhEEBWkREREQkDArQIiIBzKynmTkziwth3almtqIu6moqwjn/IiLRogAtIg2WmWWZWbGZtQuav8YXwnpGqbTAWlqYWZ6ZLYp2LY2B72c+Idp1iEjTpgAtIg3dZuBq/4SZpQNJ0SunkiuBw8B5Zta5LnesVlwRkchQgBaRhu4F4LqA6R8Afw1cwcxSzOyvZrbXzLaY2S/MLMa3LNbMHjazfWb2DXBRFe/9i5ntMrMdZvZLM4sNo74fAHOAtcCUoG2PNrMPzSzbzLaZ2VTf/CQze8RXa46ZrfDNG2dm24O2Ud4ia2YzzewVM3vRzA4BU81suJl95NvHLjP7vZklBLx/gJm9Y2YHzGy3md1jZp3MrMDM2gasd7rv/MUHH2AI+3BmNs3MvjKzg2b2pJlZKOc/VGbWzMweM7OdvsdjZtbMt6ydmb3pq++AmS0P+Pnf7fu55prZl2Z2zonsX0SaFgVoEWnoPgZamVk/X7C9CngxaJ0ngBSgNzAWL3Bf71t2E3AxMBgYitdiHOh5oAQ4xbfOecCNoRRmZj2AccBc3+O6oGX/9NXWHhgErPEtfhg4HRgJtAF+BpSFsk/gMuAVINW3z1LgDqAdcCZwDnCLr4aWwLvA20AX3zG+55z7FlgKfC9gu9cA851zR6rYZ7X7CHAxMAwY6Nvud3zzj3f+Q/X/gDPwzuNAYDjwC9+y/wa2453njsA9gDOz04DbgGHOuZa+mrJOcP8i0oQoQItIY+BvhT4X2ADs8C8ICNU/d87lOueygEeAa32rfA94zDm3zTl3APh1wHs7AhcAP3XO5Tvn9gCzgckh1nUdsNY5tx6YBwwws8G+ZVOAd51z85xzR5xz+51za3wtozcAP3HO7XDOlTrnPnTOHQ5xnx855153zpU55wqdc6uccx8750p8x/5HvC8R4AXXb51zjzjninzn59++Zc/jhWb/Obwa7zxXcpx9+D3knMt2zm0FluAFXTjG+Q/TFOAB59we59xe4H84+jM+AnQGTvKd6+XOOYcX/JsB/c0s3jmX5Zz7+gT3LyJNiAK0iDQGLwDfB6YS1H0Dr1U0AdgSMG8L0NX3uguwLWiZ30lAPLDL9+f/bLxw2CHEuq7DawXGObcTeB+vSwdAd6CqsNYOSKxmWSgCjwUzO9XXfeFbX7eO//Xt41g1ALyBFyx7430xyXHOfVLVisfZh9+3Aa8LgGTf62Od/3B0ofLPuIvv9W+BTcD/mdk3ZjYDwDm3CfgpMBPYY2bzzawLIiLHoQAtIg2ec24L3sWEFwKvBi3eh9cCeVLAvB4cbaXehRckA5f5bcO7ALCdcy7V92jlnBtwvJrMbCTQB/i5L1h+C4wArvZd3LcNOLmKt+4DiqpZlg80D9hHLF63hEAuaPoPeK3yfZxzrfC6L1jA8VW1H5xzRcDf8Fp2r6Wa1ucQ9nE8xzr/4dhJ5Z/xTgBfy/p/O+d6A5cAd/r7OjvnXnLOjfa91wG/OcH9i0gTogAtIo3FfwFnO+fyA2c650rxguCvzKylmZ0E3MnRftJ/A35sZt3MrDUwI+C9u4D/Ax4xs1ZmFmNmJ5tZcPeEqvwAeAfoj9ddYRCQhheAL8BrmZ5gZt8zszgza2tmg5xzZcAzwKNm1sV3kd2ZvgviNgKJZnaR72K+X+B1QTiWlsAhIM/M+gI/Clj2JtDJzH7quwivpZmNCFj+V7xW/Uup3K881H0cT7Xn/xjizSwx4BGH10XmF2bW3rxhDe/z12xmF5vZKb4LFw/hdd0oNbPTzOxs37ktAgp9y0REjkkBWkQaBefc1865zGoW347XevsNsAJ4CS+kAvwJ+BfwGbCayi3Y1+F1AVkPHMS7QO+Yw9GZWSJe394nnHPfBjw247Xk/sDXF/hCvAvcDuBdQDjQt4m7gP8AK33LfgPEOOdy8C7O+zNeC3o+3sVxx3IXXveWXN+xLvAvcM7l4nXPuASvi8VXwPiA5R/gXby42te3Oex9hOB4578qi/DCrv8xE/glkIk32sl/fNv6pW/9PngXS+YBHwFPOeeW4n35eAiv1f9bvK4594RRu4g0UeZdRyEiIlKZmS0GXnLO/TnatYiI1BcK0CIiUiUzG4bXDaW7r7VaRERQFw4REamCmT2P1+3hpwrPIiIVqQVaRERERCQMaoEWEREREQmDArSIiIiISBjiol1AuNq1a+d69uwZ7TJEREREpJFbtWrVPudc8A2rGl6A7tmzJ5mZ1Q31KiIiIiJSO8xsS1Xz1YVDRERERCQMCtAiIiIiImFQgBYRERERCYMCtIiIiIhIGBSgRURERETCoAAtIiIiIhIGBWgRERERkTAoQIuIiIiIhEEBWkREREQkDArQIiIiIiJhUIAWEREREQmDArSIiIiISBgUoEVEREREwqAALSIiIiISBgVoEREREZEwKECLiIiIiIRBAVpEREREJAwRC9Bm9oyZ7TGzz6tZbmb2uJltMrO1ZjYkUrWIiIiIiNSWSLZAPwecf4zlFwB9fI+bgT9EsJaaKSiA55+HDRuiXYmIiIiIRFnEArRzbhlw4BirXAb81Xk+BlLNrHOk6qmRvDyYOhXeey/alYiIiIhIlMVFcd9dgW0B09t983ZFp5xjSEryngsLo1tHFBwuOUx2UTZ78vdwoOgAhUcK2ZqzlZU7VpJ3JI+C4gLyivMoKCng0lMv5ZQ2p/D5ns+Zkzmn0rZuH3E7p7U9jVW7VvHsp89WWj591HROSjmJD7d9yEv/eanS8l+M+QWdkjuxOGsxr65/lZiYGGIshliLJTYmlntG30NqYipLs5aydMtSDKPUlVJWVkapK+X+sfeTGJfI6xteZ3HWYsrKyips//ELHyeGGOZ/Pp8VW1fgcDjnKHNlxMTEcNWAqzhYdJAPt33IrtxdlLkyHA6AWIulS8suAOwr2EdhScXPSlxMHJ2Tve+He/L3cLj0cPkyw2iZ0JJRPUbROqk1y7cs52DhQWLMOz4MurXqxsS+EzlYeJAF6xaQcziHsrIyylwZZZSRGJdI++btAdiZu5NSV1ph/83jm9M2qS0AO3J3UOYqHnuL+Ba0SWoDwLZD2wjWKqEVPVv3pGVCS1buXFlem//8f+fk73B2r7PZfmg7j338mHfeXVn5o01SGxLjEikpK2FXXuV/4q0TW5OckMyR0iN8m/9tpeVtk9rSPL45h0sPsyd/T6Xl7Zq3IykuiaKSIvYW7K20vEPzDjSLa0bBkQL2F+6vtLxji44kxCaQX5zPgaLK3/s7J3cmLiaO3OJcsouyKy3v0rILsRbLocOHyDmcU2l515ZdibEYsouyyS3OrbS8e6vuABwsOkhecV6FZYbRrVU3APYX7qfgSEGF5TX57AHEx8TTKbkTALvzd1NcWlxhebPYZnRo0QGAXXm7KCkrqbA80p+9lgktSU1MxeHYfmh7peWtmrUipVkKpa6Unbk7Ky1PTUylZUJLffb02dNnL0hD+uwlxSWx+oeryz9v9YE55yK3cbOewJvOubQqlr0F/No5t8I3/R7wM+fcqirWvRmvmwc9evQ4fcuWLRGruUolJRAfDw88APfeW7f7rgNlroy1367l3c3vsnzrcr7Y+wU7cndwuORwpV9ITZVhtE5qTevE1hwpPUJxWTEJsQnEWiyGER8bz/CuwwH4Yu8XHCis+MsoMS6R07ucDsDnez4np8j7ZeNwlJSVEGMxtE5qzcHCg3yb922l/2j8Wia0BCA2JpaE2AQSYhOIj4mnTVIb+rfvD8DKHSsr/UfUrnk7Tmt3GgAfb/+Y0rKKP9eOyR05pc0pAHyw7QMI+rXQKbkTrZNac6DwAOv3rqe4tJji0uIqPx/+8+GvLyE2gd6te9O7dW+KSopYvXN1pfec3OZkOiV3Iq84j8++/azS8lPbnkr7Fu3JOZzD57srX1bRr30/2iS14UDhAb7Y+0Wl5Wkd0khJTGFvwV427ttYaXlGpwxaJrTk27xv+frA15WWD+48mObxzdmZu5PNBzdXWj6061CaxTZjW842tuZsrbR8RLcRxMXEkZWdxY5DOyotH9ljJIax6cAmduftrrAsxmI4s/uZAHy5/0v25e+rsPxEP3t+LRJaMKjTIAA++/azSv+RtWrWivSO6QCs3rWawiMVQ1LrpNYR/ex1admFXq17UepK+XjbxwTrltKNk1JOori0mJU7VlZaflLqSXRr1Y3CkkJ99vTZq7Bcn72G89lrndiauZfPpW3ztpW2E2lmtso5NzR4fjRboLcD3QOmuwGVv8IBzrmngacBhg4dGrnEX524OO9RVFTnu46E0rJS3vrqLV794lU+3PYhWdlZHCk7AsApbU7h1LankpWdRWpiKm2S2tChRQc6JXdiXM9xnN75dBJiE8grzqNls5Y0i20W5aOJrJbNWpa3IphZne23tKyUQ4cPkV2UTakrpXVia1ISU4iLieY/2cqKS4vJLsomuyibxLhEUhNTSU5I9lrORUREGqlo/m+8ELjNzOYDI4Ac51z9677hl5RUr7pw+LtV7CvYx978vewt2EvbpLZM7DcRgBnvzmBn7k6KSorIOZzDvoJ9tE5sTbvm7Xh/y/t8m3f0z0XtktqR3jGd/xr8X0zJmBKtQ5IAsTGxXot3Uutol3JMCbEJdGjRofzPrCIiIk1BxAK0mc0DxgHtzGw7cD8QD+CcmwMsAi4ENgEFwPWRqqVWfPghtGsXsc1/uutT3tv8HneNvAuA//fe/2PVrlUUlRRRWFJIUUkRJ7c+mVevehWAsc+NZe3utRW2cVaPsxjWdRif7vqUuf+Zy/6C/RwpO1Kh31j3Vt0Z33M8bZPacmGfC5nQewLxsfEROy4RERGRxiZiAdo5d/Vxljvg1kjtv9alVerGXWO5h3OZ//l85qyaw+pdq2mT2Ibi0mJaJ7bm3zv+zfZD22ke39y70KFlG3qm9CS7KJvNBzdzXu/zGNB+AHnFeRwoPMDu/N1k7syk+2yvV4xhnNr2VAZ3HszgTr5H58G0ax65LwEiIiIiTUFELyKMhKFDh7rMzMy63/G8eZCcDJdcUuNNFZcW85sVv+GXy39JcWkxhpWP5hCulGYp9Grdi56pPemV2ouTW5/MoE6DGNhpIMkJyTWuVURERKSpqo8XETYss2ZB9+4nFKCdc3y2+zN+ueyX7Mjdwdrda8uHAzq17alc3Odizu51NmeddBYJsQnlF2UdLDzoPRcdLJ9uFteMXqm96NW6F71Se9X7PrIiIiIijY0CdKjCvIjQOUfmzkxe/eJVXt3wKhv3e0PItE1qyw2DbuDsXmcztufY8jEoA3VK7lQ+NqaIiIiI1C8K0KEKIUCXlpWyYusKXv3iVV7b8BrbDm0j1mLp174fAD8e/mMeO/+xOh0OTURERERqlwJ0qJKSIDu72sVPr3qaXyz+BXsL9pIYl8h3Tv4Ovzz7l/Rr14+z/3o2Z/U4i0e+84jCs4iIiEgDpwAdqsTEalugS8pKuPvduzm59ck8eeGTXNDngvIL+G556xYSYhOYe/ncencTDBEREREJnxJdqJ58EqoZseTf2/9NdlE2d4+6m0kDJlVY9vgFj3Pb8NvontK9yveKiIiISMOi++2GqmNH6FT1hX2LvlpErMVy7snnls/7YOsH7M7bTVxMHP3b96+rKkVEREQkwhSgQ7V4MTz0UJWLFm1axKgeo0hNTAVg+6HtXDr/Uq5/o37fXFFEREREwqcAHap33oH77qs0e8ehHaz5dg0XnnIh4PWH/v7fv09xaTG/O/93dV2liIiIiESY+kCHKjERjhyB0lKIjS2f/famtwG4sI8XoH+57Jcs37qcFya+QJ+2faJSqoiIiIhEjlqgQ5WU5D0XFVWYvWjTIrq16kZahzSWbVnGg8se5AcDf8A1GddEoUgRERERiTQF6FD5A3TAUHbFpcW88/U7XHjKhZgZA9oP4Jaht/D7C38fpSJFREREJNLUhSNUVQToD7Z+QG5xbnn3jbbN2/LEhU9EozoRERERqSNqgQ7VNdd4dyLs2rV81qKvFhEfE885vc9hxrszeHHti9GrT0RERETqhAJ0qBITISUFYo6eskWbFjG251iSE5J5etXTfLTtoygWKCIiIiJ1QQE6VF99BdOnw+bNAGRlZ7F+73ouPOVCikuLOVh0kI7JHaNcpIiIiIhEmgJ0qHbsgIcfhqwsAP751T8Bb/i6Pfl7AOjYQgFaREREpLFTgA5V0EWEizYtonfr3pza9lR25+0GUAu0iIiISBOgAB2qgABdVFLEe9+8Vz58XW5xLi0TWqoFWkRERKQJ0DB2oQoI0O9nvU9hSWH58HXjeo7j0M8P4ZyLYoEiIiIiUhfUAh2qxETvuaiIRV8tIjEukXE9x1VYxczqvi4RERERqVNqgQ5Vt25w5AjExbHoiT6c3etskuK9Vuk5mXNYtXMVf7r0T1EuUkREREQiTS3QoTKDuDi+2v8Vmw5s4sJTLixftHzrchZnLY5icSIiIiJSV9QCHSrn4NZb2dCzGIAL+lxQvmh33m5dQCgiIiLSRKgFOlRm8MwzFCx/j77t+tK7de/yRbvzd2sIOxEREZEmQgE6DC4xkb37tlbovgFqgRYRERFpShSgw3A4IYaE4rLy4esAylwZHVp0oFdqryhWJiIiIiJ1RX2gw5AXU0LLsjhG9xhdPi/GYvj8ls+jWJWIiIiI1CW1QIfIOce3cUV0atWZZnHNol2OiIiIiESJAnSI1u9dT/pNR9j06L0V5n+07SPGPjeWDfs2RKkyEREREalLCtAhWvTVIqDi8HUAXx/8mmVblhFjOpUiIiIiTYFSX4gWbVrEI591ottv/lBh/u683QAahUNERESkiVCADkFOUQ4rtq7g/G+T4a23Kizbnb+bZrHNaNWsVZSqExEREZG6pAAdgrziPK7JuIaO7XpCYWGFZf6bqJhZdIoTERERkTqlAB2Crq268uxlz9K2bbdKAbpry64VhrUTERERkcZN40CHIzGxUoD+33P+N0rFiIiIiEg0qAU6HO3bQ5s20a5CRERERKJIATocDzwAX35ZPlnmyjjt96cxJ3NOFIsSERERkbqkAF0DBwoPsHH/Rg6XHI52KSIiIiJSRxSgw7FwIVxwAeTnAwFjQCdrDGgRERGRpkIBOhzbt8Pbb0NeHgB78vcAuomKiIiISFOiAB2OpCTv2TcSx+58tUCLiIiINDUK0OEICtBtk9pywSkX0Dm5cxSLEhEREZG6pHGgwxEUoM89+VzOPfncKBYkIiIiInVNLdDhaNMGTjsNYnTaRERERJoqtUCH46yzYMOG8skr/3YlBUcKWDRlURSLEhEREZG6pKbUGtiSswWHi3YZIiIiIlKHFKDDsWULjBkD774LeONAawg7ERERkaZFATocJSWwfDns2IFzjt35u+nQokO0qxIRERGROqQAHY6AUThyDudQXFqsFmgRERGRJkYBOhwBAbq0rJQbB9/I6V1Oj25NIiIiIlKnNApHOAICdNvmbfnTpX+Kbj0iIiIiUufUAh2OZs1g6FDo0IEjpUcoc2XRrkhERERE6pgCdDjMYOVKuPFGnl71NAkPJrA3f2+0qxIRERGROqQAfYJ25+/G4WiT1CbapYiIiIhIHVIf6HBdeCGcfjq7h++hXfN2xMbERrsiEREREalDCtDh+uorSE1l94ACDWEnIiIi0gSpC0e4kpKgsJDd+bvpmKwALSIiItLUqAU6XL4A/f2079OyWctoVyMiIiIidUwBOly+AH37iNujXYmIiIiIRIECdLiGD6esqIjdubto36I9cTE6hSIiIiJNifpAh2vWLDbP/AldHu3Ci2tfjHY1IiIiIlLHFKBPwJ78PQAahUNERESkCVKADtf/+3/0veg6AI3CISIiItIEKUCHKyeHpC07AbVAi4iIiDRFCtDhSkoitqgYgA4tOkS5GBERERGpawrQ4UpKIr64hIcn/Jb42PhoVyMiIiIidUwBOlxJSQD895Bbo1yIiIiIiESDAnS4+vfn0MXnsfvQrmhXIiIiIiJRoAAdrssuY+h5m/nJ8nuiXYmIiIiIRIEC9AnYnb9bFxCKiIiINFEK0GEqfuNVNv/PIQbstWiXIiIiIiJRoAAdpuzCbNoUQUdLjnYpIiIiIhIFEQ3QZna+mX1pZpvMbEYVy1PM7B9m9pmZrTOz6yNZT204YIUAtFeAFhEREWmSIhagzSwWeBK4AOgPXG1m/YNWuxVY75wbCIwDHjGzhEjVVBs6tOsJQN/knlGtQ0RERESiI5It0MOBTc65b5xzxcB84LKgdRzQ0swMSAYOACURrKnG2rTpAkBbax7lSkREREQkGiIZoLsC2wKmt/vmBfo90A/YCfwH+Ilzrix4Q2Z2s5llmlnm3r17I1VvSL6Jz2PPFRfgunSJah0iIiIiEh2RDNBVDVPhgqa/A6wBugCDgN+bWatKb3LuaefcUOfc0Pbt29d2nWH53bZX6DP0A2zYsKjWISIiIiLREckAvR3oHjDdDa+lOdD1wKvOswnYDPSNYE01Vj4GtAv+LiAiIiIiTUEkA/RKoI+Z9fJdGDgZWBi0zlbgHAAz6wicBnwTwZpq7NC+Hay782t49NFolyIiIiIiURAXqQ0750rM7DbgX0As8Ixzbp2ZTfMtnwM8CDxnZv/B6/Jxt3NuX6Rqqg3bi/eRUOKgsDDapYiIiIhIFEQsQAM45xYBi4LmzQl4vRM4L5I11LYdRXsojTFiFaBFREREmiTdiTBMf7vyb5CUpBZoERERkSYqoi3QjdE5vc+B5i0UoEVERESaKAXoMOzJ38P7We9z0fXX0Hzw8GiXIyIiIiJRoAAdhtW7VvO9V77HiltXMKrHqGiXIyIiIiJRoD7QYdidtxuAjolt1YVDREREpIlSgA7D7nwvQPe67Adw6aVRrkZEREREokEBOgy783aTFJdETPNktUCLiIiINFEK0GHYnb+bjskdMQ1jJyIiItJk6SLCMPzvOf/LvoJ9sOLXUFQU7XJEREREJAoUoMPQI6UHPVJ66EYqIiIiIk2YAnQY/pj5RwZ3Hszwyy6DjIxolyMiIiIiUaAAHaLSslJuWXQL94y+h+FXPBjtckREREQkSnQRYSi2bYOuXfne2jI6Jnf0um/s2hXtqkREREQkChSgQxEXR+y3u0kpgo4tOsKsWdClC5SWRrsyEREREaljCtChSErynkrwWqB90xqJQ0RERKTpUYAOhT9AH4EOLTocDdAaiUNERESkyVGADkVCAs6MuwbfysmtT1aAFhEREWnCNApHKMywxETaWBLExkNiojdfAVpERESkyVELdIgOJ8SybkumN3H66d6FhG3aRLcoEREREalzaoEOUW7METbt/A8DAPr18x4iIiIi0uSoBTpEhXGOlLIEb6KoCL76CvLyoluUiIiIiNQ5BegQ5ceW0dLFexOffQanngrLlkW3KBERERGpcwrQIXDOkRtbSnKpr8eLLiIUERERabIUoEOQXZRNQZyjRYl5M3QjFREREZEmSxcRhqB1UmtGnzoBsrO9GRoHWkRERKTJUgt0iGKbtyD2cLE3oQAtIiIi0mSpBTpUSUlHA3OrVvDUUzB6dHRrEhEREZE6pwAdqsAAnZAAP/pRdOsRERERkahQF45QJSZW7LKxdi1s3Rq9ekREREQkKhSgQ5WUVHHUjTFj4JFHolePiIiIiESFAnSo/F04nKs4LSIiIiJNigJ0qJKSoKwMjhw5Oq0ALSIiItLkKECHKvjugwrQIiIiIk2SAnSogu8+GHxRoYiIiIg0CRrGLlTBN0/51a+OzhMRERGRJkMBOlTBAfr886NXi4iIiIhEjbpwhCo4QK9fDx9/HL16RERERCQq1AIdKv9FhP4+0A88AGvWwIYNUStJREREROqeWqBDFdwCrYsIRURERJokBehQBQdoDWMnIiIi0iQpQIdKAVpEREREUIAOXXAfaAVoERERkSZJFxGGKrgF+tprYfRocA7MoleXiIiIiNQpBehQBQfovn29h4iIiIg0KerCEargAL11K7z2mrpxiIiIiDQxCtChatbMe/b3gX7vPbj8ctizJ3o1iYiIiEidU4AOVUyMF6IDR+EAKCiIXk0iIiIiUucUoMMROPJGcJcOEREREWkSFKDDERig/cPaKUCLiIiINCkK0OFITKw4DjQoQIuIiIg0MRrGLhyBLdADB8L770NGRnRrEhEREZE6pQAdjsAAnZICY8ZEtx4RERERqXPqwhGOwACdnw8vvghffRXdmkRERESkTilAhyOwD3ROjnc778WLo1uTiIiIiNQpBehwaBg7ERERkSZPATocCtAiIiIiTZ4CdDgCA3SzZmCmAC0iIiLSxChAhyOwD7SZN60ALSIiItKkaBi7cAS2QAN89BF06BC9ekRERESkzilAhyM4QA8cGL1aRERERCQq1IUjHElJUFLiPQDmz4d//jO6NYmIiIhInQorQJtZazNruveuTkz0nv39oH/1K/jTn6JXj4iIiIjUueMGaDNbamatzKwN8BnwrJk9GvnS6qHgoeuCu3SIiIiISKMXSgt0inPuEHA58Kxz7nRgQmTLqqeqCtAFBdGrR0RERETqXCgBOs7MOgPfA96McD31m1qgRURERJq8UAL0A8C/gE3OuZVm1hv4KrJl1VPBfaAVoEVERESanOMOY+ecexl4OWD6G+CKSBZVbwW3QD/1FDgXvXpEREREpM6FchHhLN9FhPFm9p6Z7TOza+qiuHonOEB37gxdukSvHhERERGpc6F04TjPdxHhxcB24FRgekSrqq+CA/R778Ejj0SvHhERERGpc6EE6Hjf84XAPOfcgQjWU7/5+0D7A/Tbb8O990avHhERERGpc6HcyvsfZrYBKARuMbP2QFFky6qn/C3QwRcROgdm0atLREREROrMcVugnXMzgDOBoc65I0A+cFmkC6uXqhrGDuDw4ejUIyIiIiJ17rgt0GYWD1wLjDGvlfV9YE6E66qfqgvQhYVHu3eIiIiISKMWSh/oPwCnA0/5HkN8847LzM43sy/NbJOZzahmnXFmtsbM1pnZ+6EWHhXBfaCDA7WIiIiINHqh9IEe5pwbGDC92Mw+O96bzCwWeBI4F2/0jpVmttA5tz5gnVS8UH6+c26rmXUIq/q6FtwH+tprYdIkSE2NWkkiIiIiUrdCaYEuNbOT/RO+OxGWhvC+4Xh3L/zGOVcMzKdy3+nvA68657YCOOf2hFZ2lMTGQnz80Rbn5s2hTRuICeU0ioiIiEhjEErymw4sMbOlvi4Wi4H/DuF9XYFtAdPbffMCnQq09m17lZldF0rRURV4++6NG2HGDNi6Nbo1iYiIiEidCeVW3u+ZWR/gNMCADXg3VTmeqsZ1C77vdRxe/+pzgCTgIzP72Dm3scKGzG4Gbgbo0aNHCLuOoMTEowF661b4zW/goosg2nWJiIiISJ0Iqe+Bc+6wc26tc+4z59xhYHYIb9sOdA+Y7gbsrGKdt51z+c65fcAyYGDQOjjnnnbODXXODW3fvn0oJUdOUlLFcaBBFxGKiIiINCEn2nk3lLuGrAT6mFkvM0sAJgMLg9Z5AzjLzOLMrDkwAvjiBGuqG4FdOBSgRURERJqcUEbhqEpwV4zKKzhXYma3Af8CYoFnnHPrzGyab/kc59wXZvY2sBYoA/7snPv8BGuqG1UF6IKC6NUjIiIiInWq2gBtZv+h6qBsQMdQNu6cWwQsCpo3J2j6t8BvQ9levRDYB1ot0CIiIiJNzrFaoEO5ULDpCewD3aOH9zohIbo1iYiIiEidqTZAO+e21GUhDUZSEmRne69jYqBZs6iWIyIiIiJ1S3cACVdgH+iyMrj9dnjzzejWJCIiIiJ1RgE6XIF9oGNi4A9/gI8+im5NIiIiIlJnFKDDFdgH2j+tiwhFREREmowTGYUDAOdcRkQqqu+CA3NwoBYRERGRRi2UUThu9T2/4HueAjTdgY+rCtBqgRYRERFpMo47CoeZjXLOjQpYNMPMPgAeiHRx9VJiIhQXQ2kpxMZCq1bgjntfGRERERFpJEK5E2ELMxvtnFsBYGYjgRaRLase89885fBhaN4c/vOf6NYjIiIiInUqlAD9X8AzZpbim84GbohYRfVd4N0HmzePbi0iIiIiUueOG6Cdc6uAgWbWCjDnXE7ky6rHgm/f/dvfQn4+zJwZtZJEREREpO4cN0CbWTPgCqAnEGdmADjnmm4faDgaoJcuhd27FaBFREREmohQunC8AeQAq4DDkS2nAfC3QPuHrtMoHCIiIiJNSigBuptz7vyIV9JQBHfhUIAWERERaVJCuRPhh2aWHvFKGgoFaBEREZEmLZQW6NHAVDPbjNeFwwDXpO9ECEdDc9u2kJJS/foiIiIi0qiEEqAviHgVDYn/IkJ/H+hf/9p7iIiIiEiTEMowdv47EnYAEiNeUX0X3AItIiIiIk3KcftAm9mlZvYVsBl4H8gC/hnhuuqv4AD9xhtwySUK1CIiIiJNRCgXET4InAFsdM71As4BPohoVfVZcIDeuhXefNO7mYqIiIiINHqhBOgjzrn9QIyZxTjnlgCDIltWPRbcBzp4XGgRERERadRCuYgw28ySgWXAXDPbA5REtqx6rKph7AKnRURERKRRC6UF+jKgALgDeBv4GrgkkkXVa/HxEBurAC0iIiLSRIUyCoe/c28Z8Hxky2kgAm+e0ro1nHJKdOsRERERkToTShcOCZaYeLTP8/jx8NVX0a1HREREROpMKF04JJhu3y0iIiLSZIUyDvTFZqagHSgwQG/e7LVCL10a1ZJEREREpG6EEownA1+Z2Swz6xfpghqEwAB95IgXnnfsiGpJIiIiIlI3jhugnXPXAIPxRt941sw+MrObzaxlxKurrwL7QGsUDhEREZEmJaSuGc65Q8DfgflAZ2AisNrMbo9gbfVXYAu0ArSIiIhIkxJKH+hLzOw1YDEQDwx3zl0ADATuinB99VNggPbfmVABWkRERKRJCGUYu0nAbOfcssCZzrkCM7shMmXVc8Et0AMHQtu20a1JREREROpEKAH6fmCXf8LMkoCOzrks59x7EausPgvsAx0bC2vWRLUcEREREak7ofSBfhnvLoR+pb55TZfGgRYRERFpskIJ0HHOuWL/hO91QuRKagCCA/SFF8IDD0SvHhERERGpM6EE6L1mdql/wswuA/ZFrqQGIDhAf/GFbuctIiIi0kSE0gd6GjDXzH4PGLANuC6iVdV3/j7QzoGZF6j9faJFREREpFE7boB2zn0NnGFmyYA553IjX1Y95x/7+fBhL0yrT7SIiIhIkxFKCzRmdhEwAEg0MwCcc02302/gzVMUoEVERESalOMGaDObAzQHxgN/Bq4EPolwXfVbYIBu3RqGDYOysmO/R0REREQahVBaoEc65zLMbK1z7n/M7BHg1UgXVq/57z7o7/c8e3b0ahERERGROhXKKBz+q+MKzKwLcAToFbmSGoDAFmgRERERaVJCCdD/MLNU4LfAaiALmBfBmuq/4AB9zz0wenT06hERERGROnPMLhxmFgO855zLBv5uZm8Cic65nLoort4KDtAHD8LGjdGrR0RERETqzDFboJ1zZcAjAdOHm3x4hsp9oDUKh4iIiEiTEUoXjv8zsyvMP36dVG6BTkxUgBYRERFpIkIZheNOoAVQYmZFeHcjdM65VhGtrD4LDtBJSVBaCkeOQHx89OoSERERkYgL5U6ELeuikAYlOED37w+XXQYlJQrQIiIiIo1cKDdSGVPVfOfcstovp4EI7gN9xRXeQ0REREQavVC6cEwPeJ0IDAdWAWdHpKKGQONAi4iIiDRZx72I0Dl3ScDjXCAN2B350uqx4AC9cCF06KCh7ERERESagFBG4Qi2HS9EN10JCWB2NECXlcHevZCfH926RERERCTiQukD/QTgfJMxwCDgswjWVP+Zef2gA8eBBnXpEBEREWkCQukDnRnwugSY55z7IEL1NByBN09RgBYRERFpMkIJ0K8ARc65UgAzizWz5s65gsiWVs8pQIuIiIg0SaH0gX4PSAqYTgLejUw5DUhggO7YEaZMgU6doluTiIiIiERcKC3Qic65PP+Ecy7PzJpHsKaGIbAPdI8e8OKL0a1HREREROpEKC3Q+WY2xD9hZqcD6qsQ2AItIiIiIk1GKAH6p8DLZrbczJYDC4DbIlpVQxAYoLOzoXlzePzxqJYkIiIiIpF33C4czrmVZtYXOA0wYINz7kjEK6vvkpLg4EHvdWKiF6YLmvZ1lSIiIiJNwXFboM3sVqCFc+5z59x/gGQzuyXypdVzgX2gmzXzntWlQ0RERKTRC6ULx03OuWz/hHPuIHBTxCpqKAK7cPhvrKIALSIiItLohRKgY8zM/BNmFgskRK6kBiL4IkJdVCgiIiLSJIQyjN2/gL+Z2Ry8W3pPA96OaFUNQXBgvukmGDw4evWIiIiISJ0IJUDfDdwM/AjvIsL/A/4UyaIahMA+0AC/+U30ahERERGROnPcLhzOuTLn3Bzn3JXOuSuAdcATkS+tnvO3QDvnTZeVQXFxdGsSERERkYgLpQ80ZjbIzH5jZlnAg8CGiFbVECQleaH5iG9EvzPPhMsui25NIiIiIhJx1XbhMLNTgcnA1cB+vBuomHNufB3VVr8lJXnPhYWQkAAtW0JubnRrEhEREZGIO1YL9AbgHOAS59xo59wTQGndlNUAJCZ6z/5+0Kmp3h0JRURERKRRO1aAvgL4FlhiZn8ys3PwLiIUqNgCDQrQIiIiIk1EtQHaOfeac+4qoC+wFLgD6GhmfzCz8+qovvpLAVpERESkSTruMHbOuXxgLjDXzNoAk4AZeMPZNV3BAfrcc6F5c29UDlNDvYiIiEhjFco40OWccweAP/oeTVtwH+jvfMd7iIiIiEijFtIwdifKzM43sy/NbJOZzTjGesPMrNTMroxkPbUquAX6yBHYvVtjQYuIiIg0chEL0GYWCzwJXAD0B642s/7VrPcbvFuGNxzBAfqf/4ROnWDt2ujVJCIiIiIRF8kW6OHAJufcN865YmA+UNWdRm4H/g7siWAtta+qiwgBcnKiUo6IiIiI1I1IBuiuwLaA6e2+eeXMrCswEZgTwToiwx+gA8eBBo3EISIiItLIRTJAVzUUhQuafgy42zl3zBu0mNnNZpZpZpl79+6trfpqxn8RYXALtAK0iIiISKMW1igcYdoOdA+Y7gbsDFpnKDDfvGHf2gEXmlmJc+71wJWcc08DTwMMHTo0OIRHR3AXjpQU71ldOEREREQatUgG6JVAHzPrBewAJgPfD1zBOdfL/9rMngPeDA7P9VZwgG7ZEv73f2H06OjVJCIiIiIRF7EA7ZwrMbPb8EbXiAWecc6tM7NpvuUNr99zoOBxoGNi4Oc/j149IiIiIlInItkCjXNuEbAoaF6Vwdk5NzWStdQ6M2jW7GgLNMD27VBWBj16RK8uEREREYmoiAboRi8pqWKAvuQS6N4dFi6MXk0iIiIiElERvRNhoxccoFNTdRGhiIiISCOnAF0TwQE6JUXD2ImIiIg0cgrQNZGYePQiQvBaoBWgRURERBo1BeiaUBcOERERkSZHFxHWRHCAnjwZBg+OXj0iIiIiEnEK0DWRlAS5uUenzzjDe4iIiIhIo6UuHDUR3Af64EH46CMoKIheTSIiIiISUQrQNRHchWPpUhg5EjZujFpJIiIiIhJZCtA1UdVFhKALCUVEREQaMQXomqhqHGjQUHYiIiIijZgCdE1UNQ40KECLiIiINGIK0DVRXRcOBWgRERGRRkvD2NVEUhKUlHiPuDivC8f8+XD66dGuTEREREQiRAG6JpKSvOfCQmjZEmJj4aqroluTiIiIiESUunDURGKi9xzYD/qjj2D16ujUIyIiIiIRpxbomghsgfa7+Wbo0wdefTU6NYmIiIhIRKkFuiaqCtCpqbqIUERERKQRU4CuieoCtG6kIiIiItJoKUDXRFV9oNUCLSIiItKoKUDXhLpwiIiIiDQ5CtA1UVWAvvVWeP31qJQjIiIiIpGnUThqoqoA3bev9xARERGRRkkt0DVRVR/orVth7lw4dCg6NYmIiIhIRClA10RVLdD//jdccw1s2RKdmkREREQkohSga6K6iwhBQ9mJiIiINFIK0DVxrACtkThEREREGiUF6JqobhxoUIAWERERaaQUoGsiNhbi49WFQ0RERKQJ0TB2NZWUVDFAt2njXUjYu3f0ahIRERGRiFGArqngAB0bC8OHR68eEREREYkodeGoqcTEin2gAebPh//7v+jUIyIiIiIRpQBdU8Et0AAPPAB/+lN06hERERGRiFKArqmqAnRqqi4iFBEREWmkFKBrqqoAnZKiYexEREREGikF6Jqqqg90aqoCtIiIiEgjpQBdU+rCISIiItKkKEDXVFUBeuZMWLUqKuWIiIiISGRpHOiaqipAd+wYnVpEREREJOLUAl1TVfWB/vxz+OUv1Q9aREREpBFSgK6pqlqgP/8c7r0Xdu2KTk0iIiIiEjEK0DVV3UWEoBZoERERkUZIAbqmkpKguBhKS4/OS0nxnhWgRURERBodBeiaSkz0ng8fPjrP3wKtoexEREREGh0F6JpKSvKeA7txqAuHiIiISKOlYexqqqoA3bEj7N17NEiLiIiISKOhAF1TVQXomBho1y469YiIiIhIRKkLR035+0AHjwX90EMwf37d1yMiIiIiEaUAXVNVtUADPPMMvPFG3dcjIiIiIhGlAF1T1QXo1FRdRCgiIiLSCClA15QCtIiIiEiTogBdU/4AHdwHOjVV40CLiIiINEIK0DXlv4iwqhbo3Nw6L0dEREREIkvD2NVUdV04nnwS4nR6RURERBobJbyaqi5Ax8fXfS0iIiIiEnHqwlFT1fWBXrECbrgBDh6s+5pEREREJGIUoGuquj7QWVnw7LPeLb1FREREpNFQgK6p+HiIja36IkLQSBwiIiIijYwCdG1ISqo+QGssaBEREZFGRQG6NiQlVT0ONChAi4iIiDQyCtC1ITGx6hboli2huDgqJYmIiIhIZGgYu9pQVReObt3g0KHo1CMiIiIiEaMW6NpQVYAWERERkUZJAbo2VNUHGmDaNHjqqbqvR0REREQiRgG6NlTVBxrgnXfgww/rvh4RERERiRgF6NpQXReO1FSNAy0iIiLSyChA14bqAnRKioaxExEREWlkFKBrQ3V9oFNTFaBFREREGhkF6NpQXR/ok06CNm3qvh4RERERiRiNA10bquvCMXt23dciIiIiIhGlFujaoHGgRURERJoMBeja4O8D7VzF+f/4B4wbp37QIiIiIo2IAnRtSEz0ng8frjj/wAF4/33vWUREREQaBQXo2pCU5D0Hd+NITfWe1QItIiIi0mhENECb2flm9qWZbTKzGVUsn2Jma32PD81sYCTriZjqAnRKivesAC0iIiLSaEQsQJtZLPAkcAHQH7jazPoHrbYZGOucywAeBJ6OVD0R5Q/QwWNBqwVaREREpNGJZAv0cGCTc+4b51wxMB+4LHAF59yHzrmDvsmPgW4RrCdy/H2gg1ug27WDjIyjy0VERESkwYvkONBdgW0B09uBEcdY/7+Af1a1wMxuBm4G6NGjR23VV3uq68LRrRt89lnd1yMiIiIiERPJFmirYp6rYh5mNh4vQN9d1XLn3NPOuaHOuaHt27evxRJrSXUBWkREREQanUgG6O1A94DpbsDO4JXMLAP4M3CZc25/BOuJnOr6QAOcey78+td1W4+IiIiIREwkA/RKoI+Z9TKzBGAysDBwBTPrAbwKXOuc2xjBWiKruj7QABs3wpdf1m09IiIiIhIxEesD7ZwrMbPbgH8BscAzzrl1ZjbNt3wOcB/QFnjKzABKnHNDI1VTxByrC0dqqkbhEBEREWlEInkRIc65RcCioHlzAl7fCNwYyRrqxLECdEqKArSIiIhII6I7EdaGY/WBVgu0iIiISKMS0RboJuNYfaCHDoU2beq2HhERERGJGAXo2nCsLhz33Ve3tYiIiIhIRKkLR21ISAAzjQMtIiIi0gQoQNcGM68Vuqo+0M8/DyedBDk5dV+XiIiIiNQ6BejakphYdQt0SQls3aoLCUVEREQaCQXo2pKUVP040KAALSIiItJIKEDXluMFaHXhEBEREWkUFKBrS3V9oNUCLSIiItKoaBi72pKUBHl5led36gTf/S60bVvnJYmIiETLkSNH2L59O0VVNS6J1DOJiYl069aN+Pj4kNZXgK4tp54K77wDznmjcvh17QqvvRa9ukRERKJg+/bttGzZkp49e2KB/y+K1DPOOfbv38/27dvp1atXSO9RF47aMnIk7N4NmzdHuxIREZGoKyoqom3btgrPUu+ZGW3btg3rryUK0LVl5Ejv+aOPKi/r3RtmzKjbekRERKJM4VkainA/qwrQtSUtDZKT4cMPKy8rLoa9e+u+JhERkSbutddew8zYsGFDtEupVdOnT2fAgAFMnz69wvylS5fyYVVZ5DgyMzP58Y9/fNz1RvobDGvZuHHjyMzMPOY6jz32GAUFBRHZf7gUoGtLbCyccUbVATo1VcPYiYiIRMG8efMYPXo08+fPj+h+SktLI7r9YH/84x9ZvXo1v/3tbyvMP1aALikpqXZ7Q4cO5fHHHz/ufk8knNcWBejGauRIWLsWcnMrzk9J0TB2IiIidSwvL48PPviAv/zlLxUCdGlpKXfddRfp6elkZGTwxBNPALBy5UpGjhzJwIEDGT58OLm5uTz33HPcdttt5e+9+OKLWbp0KQDJycncd999jBgxgo8++ogHHniAYcOGkZaWxs0334xzDoBNmzYxYcIEBg4cyJAhQ/j666+59tpreeONN8q3O2XKFBYuXFihfucc06dPJy0tjfT0dBYsWADApZdeSn5+PiNGjCifB5CVlcWcOXOYPXs2gwYNYvny5UydOpU777yT8ePHc/fdd/PJJ58wcuRIBg8ezMiRI/nyyy8BL3hffPHFAMycOZMbbriBcePG0bt37wrBOjk5uXz9cePGceWVV9K3b1+mTJlSfryLFi2ib9++jB49mh//+Mfl2w1UWFjI5MmTycjI4KqrrqIw4F4aP/rRjxg6dCgDBgzg/vvvB+Dxxx9n586djB8/nvHjx1e7Xl3RKBy1aeRIKCuDTz6Bc845Oj811bvAUEREpIka99y4SvO+N+B73DLsFgqOFHDh3AsrLZ86aCpTB01lX8E+rvzblRWWLZ269Lj7fP311zn//PM59dRTadOmDatXr2bIkCE8/fTTbN68mU8//ZS4uDgOHDhAcXExV111FQsWLGDYsGEcOnSIpKSkY24/Pz+ftLQ0HnjgAQD69+/PfffdB8C1117Lm2++ySWXXMKUKVOYMWMGEydOpKioiLKyMm688UZmz57NZZddRk5ODh9++CHPP/98he2/+uqrrFmzhs8++4x9+/YxbNgwxowZw8KFC0lOTmbNmjUV1u/ZsyfTpk0jOTmZu+66C4C//OUvbNy4kXfffZfY2FgOHTrEsmXLiIuL49133+Wee+7h73//e6Vj27BhA0uWLCE3N5fTTjuNH/3oR5WGePv0009Zt24dXbp0YdSoUXzwwQcMHTqUH/7whyxbtoxevXpx9dVXV3nu/vCHP9C8eXPWrl3L2rVrGTJkSPmyX/3qV7Rp04bS0lLOOecc1q5dy49//GMeffRRlixZQrt27apdLyMj45g/s9qiFujaNGKEN4Rd8J83LrwQLrooOjWJiIg0UfPmzWPy5MkATJ48mXnz5gHw7rvvMm3aNOLivHbENm3a8OWXX9K5c2eGDRsGQKtWrcqXVyc2NpYrrriifHrJkiWMGDGC9PR0Fi9ezLp168jNzWXHjh1MnDgR8MYbbt68OWPHjmXTpk3s2bOHefPmccUVV1Ta34oVK7j66quJjY2lY8eOjB07lpUrV4Z9HiZNmkRsbCwAOTk5TJo0ibS0NO644w7WrVtX5XsuuugimjVrRrt27ejQoQO7q2gIHD58ON26dSMmJoZBgwaRlZXFhg0b6N27d/lwcNUF6GXLlnHNNdcAkJGRUSH4/u1vf2PIkCEMHjyYdevWsX79+iq3Eep6kaAW6NqUmgoDBlQO0LfeGpVyRERE6otjtRg3j29+zOXtmrcLqcU50P79+1m8eDGff/45ZkZpaSlmxqxZs3DOVRp1oap5AHFxcZSVlZVPBw51lpiYWB5Mi4qKuOWWW8jMzKR79+7MnDmToqKi8m4NVbn22muZO3cu8+fP55lnnqm0/FjvDUeLFi3KX997772MHz+e1157jaysLMaNG1fle5o1a1b+OjY2tsr+01WtE07NVZ3vzZs38/DDD7Ny5Upat27N1KlTqxxeLtT1IkUt0LVt5EhvKLuAf2wAlJR4N1kRERGRiHvllVe47rrr2LJlC1lZWWzbto1evXqxYsUKzjvvPObMmVMeCg8cOEDfvn3ZuXNneQtvbm4uJSUl9OzZkzVr1lBWVsa2bdv45JNPqtyfP7y1a9eOvLw8XnnlFcBrye7WrRuvv/46AIcPHy6/EG7q1Kk89thjAAwYMKDSNseMGcOCBQsoLS1l7969LFu2jOHDhx/zuFu2bElu8LVYAXJycujatSsAzz333DG3dSL69u3LN998Q1ZWFkCFPtqBxowZw9y5cwH4/PPPWbt2LQCHDh2iRYsWpKSksHv3bv75z3+Wvyfw2I61Xl1QgK5tI0d6I2588cXRebNnQ3w81JMrR0VERBq7efPmlXeb8Lviiit46aWXuPHGG+nRowcZGRkMHDiQl156iYSEBBYsWMDtt9/OwIEDOffccykqKmLUqFH06tWL9PR07rrrrgp9dQOlpqZy0003kZ6ezne/+93yriAAL7zwAo8//jgZGRmMHDmSb7/9FoCOHTvSr18/rr/++iq3OXHixPIazz77bGbNmkWnTp2OedyXXHIJr732WvlFhMF+9rOf8fOf/5xRo0ZFZOSQpKQknnrqKc4//3xGjx5Nx44dSUlJqbTej370I/Ly8sjIyGDWrFnlXwwGDhzI4MGDGTBgADfccAOjRo0qf8/NN9/MBRdcwPjx44+5Xl2w2vrzQF0ZOnSoO944gVH11Vfebb2ffhpuusmb9/TT8MMfwvbt3q29RUREGrkvvviCfv36RbuMeq2goID09HRWr15dZchsqPLy8khOTsY5x6233kqfPn244447ol3WcVX1mTWzVc65ocHrqgW6tp1yCrRrV7EfdGqq96yh7ERERATvQsa+ffty++23N6rwDPCnP/2JQYMGMWDAAHJycvjhD38Y7ZJqnS4irG1mXjeOwADt/4ehm6mIiIgIMGHCBLZu3RrtMiLijjvuaBAtzjWhFuhIGDkSNm6Effu8abVAi4iIiDQaCtCR4L9P/Mcfe889esCdd3rPIiIiItKgKUBHwtChEBd3tBtH587wyCOQlhbdukRERESkxhSgIyEpCQYPrtgPOj8f8vKiV5OIiIiI1AoF6EgZORI++QSOHPGm27aFBx+Mbk0iIiJNzGuvvYaZsWHDhmiXUqumT5/OgAEDmD59eo2289xzz3HbbbcBMGfOHP76179WWicrK4u04/wVPSsri5deeql8OjMzkx//+Mc1qq0qgfVWZ+nSpXwYfFfoWqYAHSkjR0JhIXz2mTedmqqLCEVEROrYvHnzGD16NPPnz4/ofiJxU5Jj+eMf/8jq1av57W9/W2vbnDZtGtddd90JvTc4QA8dOpTHH3+8tkoLiwJ0Q+a/kND/A1SAFhERqVN5eXl88MEH/OUvf6kQoEtLS7nrrrtIT08nIyODJ554AoCVK1cycuRIBg4cyPDhw8nNza3U4nnxxRezdOlSAJKTk7nvvvsYMWIEH330EQ888ADDhg0jLS2Nm2++Gf/N6jZt2sSECRMYOHAgQ4YM4euvv+baa6/ljTfeKN/ulClTWLhwYYX6nXNMnz6dtLQ00tPTy2+Lfemll5Kfn8+IESMq3Cq7rKyMnj17kh2QN0455RR2797NP/7xD0aMGMHgwYOZMGECu3fvrnS+Zs6cycMPPwzAqlWrGDhwIGeeeSZPPvlk+TpZWVmcddZZDBkyhCFDhpQH1RkzZrB8+XIGDRrE7NmzWbp0KRdffDHg3Sr9u9/9LhkZGZxxxhnlt+2eOXMmN9xwA+PGjaN3797VBu5nn32WU089lbFjx/LBBx+Uz6/qmLKyspgzZw6zZ88uvxtjKMceLo0DHSndukH37l6A/vGPFaBFRKTJ+unbP2XNt2tqdZuDOg3isfMfO+Y6r7/+Oueffz6nnnoqbdq0YfXq1QwZMoSnn36azZs38+mnnxIXF8eBAwcoLi7mqquuYsGCBQwbNoxDhw6RlJR0zO3n5+eTlpbGAw88AED//v257777ALj22mt58803ueSSS5gyZQozZsxg4sSJFBUVUVZWxo033sjs2bO57LLLyMnJ4cMPP+T555+vsP1XX32VNWvW8Nlnn7Fv3z6GDRvGmDFjWLhwIcnJyaxZs6bC+jExMVx22WW89tprXH/99fz73/+mZ8+edOzYkdGjR/Pxxx9jZvz5z39m1qxZPPLII9Ue2/XXX88TTzzB2LFjK3QT6dChA++88w6JiYl89dVXXH311WRmZvLQQw/x8MMP8+abbwKUf8kAuP/++xk8eDCvv/46ixcv5rrrriuvfcOGDSxZsoTc3FxOO+00fvSjHxEfH1/+3l27dnH//fezatUqUlJSGD9+PIMHDwao9pimTZtGcnIyd911FwAHDx4M69hDoRboSAq8oUpKigK0iIhIHZo3bx6TJ08GYPLkycybNw/w7gI4bdo04uK8dsQ2bdrw5Zdf0rlzZ4YNGwZAq1atypdXJzY2liuuuKJ8esmSJYwYMYL09HQWL17MunXryM3NZceOHUycOBGAxMREmjdvztixY9m0aRN79uxh3rx5XHHFFZX2t2LFCq6++mpiY2Pp2LEjY8eOZeXKlcesyf8lAGD+/PlcddVVAGzfvp3vfOc7pKen89vf/pZ169ZVu42cnByys7MZO3Ys4H0Z8Dty5Ag33XQT6enpTJo0ifXr1x+zHv9x+Ldx9tlns3//fnJ8N5e76KKLaNasGe3ataNDhw6VWof//e9/M27cONq3b09CQkL58YRzTOEce6jUAh1JI0fCggWwbRtcfz0UFES7IhERkTp3vJbiSNi/fz+LFy/m888/x8woLS3FzJg1axbOOcyswvpVzQOIi4ujrKysfLqoqKj8dWJiIrGxseXzb7nlFjIzM+nevTszZ86kqKiovBtHVa699lrmzp3L/PnzeeaZZyotP9Z7q3PmmWeyadMm9u7dy+uvv84vfvELAG6//XbuvPNOLr30UpYuXcrMmTOr3UZ15wJg9uzZdOzYkc8++4yysjISExOPW1NVx+HffrNmzcrnxcbGUlJSUu26wUI9pnCOPVRqgY4kfz/ojz6CyZPhhhuiW4+IiEgT8corr3DdddexZcsWsrKy2LZtG7169WLFihWcd955zJkzpzysHThwgL59+7Jz587yFt7c3FxKSkro2bMna9asoaysjG3btvHJJ59UuT9/sG7Xrh15eXm88sorgNeS3a1bN15//XUADh8+TIGvQW3q1Kk89thjAAwYMKDSNseMGcOCBQsoLS1l7969LFu2jOHDhx/zuM2MiRMncuedd9KvXz/atm0LeK3KXbt2BajUVSRYamoqKSkprFixAoC5c+eWL8vJyaFz587ExMTwwgsvlF882bJlS3Jzc6vc3pgxY8q3sXTpUtq1a0erVq2OWYPfiBEjWLp0Kfv37+fIkSO8/PLLFWqp6piCawnn2EOlAB1JAwd6Y0J/+CHk5sKmTdGuSEREpEmYN29eebcJvyuuuIKXXnqJG2+8kR49epCRkcHAgQN56aWXSEhIYMGCBdx+++0MHDiQc889l6KiIkaNGkWvXr1IT0/nrrvuYsiQIVXuLzU1tbxrw3e/+93yriAAL7zwAo8//jgZGRmMHDmSb7/9FoCOHTvSr18/rr/++iq3OXHixPIazz77bGbNmkWnTp2Oe+xXXXUVL774YoXuDjNnzmTSpEmcddZZtGvX7rjbePbZZ7n11ls588wzK/QFv+WWW3j++ec544wz2LhxIy1atAAgIyODuLg4Bg4cyOzZsytsa+bMmWRmZpKRkcGMGTPCCrGdO3dm5syZnHnmmUyYMKHC+a/umC655BJee+218osIwz32UNiJ/HkgmoYOHeoyMzOjXUboxo3zum5MnAj33OO9Ps5FCSIiIg3dF198Qb9+/aJdRr1WUFBAeno6q1evJiUlJdrlNHlVfWbNbJVzbmjwumqBjrSRI+HTT+G007zpf/wjuvWIiIhI1L377rv07duX22+/XeG5AdJFhJE2ciSUlECbNnDyyfDYY/C970W7KhEREYmiCRMmsHXr1miXISdILdCRdsYZ3vO//+2NB/3RR95rEREREWmQFKAjrV07r/vGhx96Q9m1agUBV7OKiIiISMOiLhx1YeRIr+9zcrLXAu3vDy0iIiIiDY5aoOvCyJGwb583jF3//hAbCw1s9BMRERER8ShA14Uzz/Se/bf1/tvfvFbo/Pzo1SQiItLIJScnR23fjz/+OP369WPKlCkV5q9Zs4ZFixaFvb2dO3dy5ZVXHne9Cy+8kOzs7LC3fzxTp04tvzlMdZ577jl27txZ6/uujxSg60K/fpCScjRAd+sGX30FtXQ3HBEREalfnnrqKRYtWlThLn5w7ABd1W2s/bp06XLcAAuwaNEiUlNTw6q1tihAS+2KifFaof0B+swzYfhw+N3voKwsurWJiIg0IWvWrOGMM84gIyODiRMncvDgQcBrMe7fvz8ZGRlMnjwZgPfff59BgwYxaNAgBg8eXOWtqh999FHS0tJIS0srvy33tGnT+Oabb7j00ksr3JWvuLiY++67jwULFjBo0CAWLFjAzJkzufnmmznvvPO47rrryMrK4qyzzmLIkCEMGTKED33ZISsri7S0NMALqpdffjnnn38+ffr04Wc/+1n5Pnr27Mm+ffvIysqiX79+3HTTTQwYMIDzzjuPwsJCAFauXElGRgZnnnkm06dPL99uIOcct912G/379+eiiy5iz5495cseeOABhg0bRlpaGjfffDPOOV555RUyMzOZMmUKgwYNorCwsMr1Gg3nXIN6nH766a5BevBB58ycW73am37pJefAuTffjG5dIiIiEbB+/fqKM8aOrfx48klvWX5+1cuffdZbvndv5WUhaNGiRaV56enpbunSpc455+699173k5/8xDnnXOfOnV1RUZFzzrmDBw8655y7+OKL3YoVK5xzzuXm5rojR45U2FZmZqZLS0tzeXl5Ljc31/Xv39+t9v0/f9JJJ7m9e/dW2v+zzz7rbr311vLp+++/3w0ZMsQVFBT4TkW+KywsdM45t3HjRufPPZs3b3YDBgwo30avXr1cdna2KywsdD169HBbt26tsN/Nmze72NhY9+mnnzrnnJs0aZJ74YUXnHPODRgwwH3wwQfOOefuvvvu8u0G+vvf/+4mTJjgSkpK3I4dO1xKSop7+eWXnXPO7d+/v3y9a665xi1cuNA559zYsWPdypUry5dVt159Vekz65wDMl0VeVQt0HXlhz+ELl1g0iTIyYErr4SuXb0bq4iIiEjE5eTkkJ2dzdixYwH4wQ9+wLJlywDIyMhgypQpvPjii8TFeYOUjRo1ijvvvJPHH3+c7Ozs8vl+K1asYOLEibRo0YLk5GQuv/xyli9fHnZdl156KUlJSQAcOXKEm266ifT0dCZNmsT69eurfM8555xDSkoKiYmJ9O/fny1btlRap1evXgwaNAiA008/naysLLKzs8nNzWXkyJEAfP/7369y+8uWLePqq68mNjaWLl26cPbZZ5cvW7JkCSNGjCA9PZ3Fixezbt26KrcR6noNkYaxqyvt28OCBTB2LNxwA7zyCvz+99440SIiIo3d0qXVL2ve/NjL27U79vJa8NZbb7Fs2TIWLlzIgw8+yLp165gxYwYXXXQRixYt4owzzii//bafq6UuCS1atCh/PXv2bDp27Mhnn31GWVkZiYmJVb6nWbNm5a9jY2Or7D8dvE5hYWFYNZtZpXlFRUXccsstZGZm0r17d2bOnElRUdEJr9dQqQW6Lo0aBb/5Dbz6qtf/+bvfhdGjo12ViIhIk5CSkkLr1q3LW4lfeOEFxo4dS1lZGdu2bWP8+PHMmjWL7Oxs8vLy+Prrr0lPT+fuu+9m6NChbNiwocL2xowZw+uvv05BQQH5+fm89tprnHXWWcesoWXLllX2pfbLycmhc+fOxMTE8MILL1BaWlrzAw/QunVrWrZsyccffwzA/Pnzq1xvzJgxzJ8/n9LSUnbt2sWSJUsAykNwu3btyMvLq3BhY+CxHWu9xkAt0HXtzjth+XKYPh1GjPBG5HjoIbj/fujQIdrViYiINBoFBQV069atfPrOO+/k+eefZ9q0aRQUFNC7d2+effZZSktLueaaa8jJycE5xx133EFqair33nsvS5YsITY2lv79+3PBBRdU2P6QIUOYOnUqw4cPB+DGG29k8ODBx6xp/PjxPPTQQwwaNIif//znlZbfcsstXHHFFbz88suMHz++Qut0bfnLX/7CTTfdRIsWLRg3bhwpKSmV1pk4cSKLFy8mPT2dU089tbzbS2pqankXk549ezJs2LDy90ydOpVp06aRlJTERx99VO16jYHV1p8f6srQoUNdZmZmtMuomexsGDIEjhzxxoQeORL+53/gvvuiXZmIiEit+OKLL+jXr1+0y5Aq5OXllY+R/dBDD7Fr1y5+97vfRbmq6KvqM2tmq5xzQ4PXVReOaEhNhZdfhj174MEH4fzz4amn4PDhaFcmIiIijdxbb73FoEGDSEtLY/ny5fziF7+IdkkNjgJ0tJx+utcP+p//hE6dYPdumDcv2lWJiIhII3fVVVexZs0aPv/8c9566y3at28f7ZIaHAXoaPrhD+Hqq+Gvf4WePb0h7RpYlxoRERGRpkYBOprM4Omn4dRT4eBBSE8H312CRERERKR+UoCOtuRkrz90cTFs2wYJCbBli1qiRUREROopBej6IC0N5syB99+HQYMgI8O7Y2FeXrQrExEREZEgCtD1xXXXwUsveSNxHDoEf/+7F6w3bYp2ZSIiIg2Sf6i2aHj88cfp168fU6ZMqdF2li5dysUXXwzAwoULeeihh6pc73jHmp2dzVNPPVU+vXPnTq688soa1VaVwHqrs2bNGhYtWlTr+65LCtD1ydVXwxdfwDPPQMeOXleOvn3h179Wlw4REZEG5KmnnmLRokXMnTu31rZ56aWXMmPGjBN6b3CA7tKlS9TuDqgALbUvLg6uv97rD/2rX0FMDNxzj3fL7/feU5AWERGpgTVr1nDGGWeQkZHBxIkTOXjwIOC1GPfv35+MjAwmT54MwPvvv8+gQYMYNGgQgwcPrvIW3I8++ihpaWmkpaXx2GOPATBt2jS++eYbLr30UmbPnl1h/REjRrBu3bry6XHjxrFq1So++eQTRo4cyeDBgxk5ciRffvllpX0999xz3HbbbQBs3ryZM888k2HDhnHvvfeWr5OXl8c555zDkCFDSE9P54033gBgxowZfP311wwaNIjp06eTlZVFWloa4N12+/rrryc9PZ3BgweX37b7ueee4/LLL+f888+nT58+/OxnP6vynL799tv07duX0aNH8+qrr5bPr+qYiouLue+++1iwYAGDBg1iwYIFIR17veOca1CP008/3TUp+/Y59+ijznXt6hw4N3asc2vWRLsqERGRY1q/fv3RiZ/8xPv/qzYfP/nJcWto0aJFpXnp6elu6dKlzjnn7r33XvcT33Y6d+7sioqKnHPOHTx40Dnn3MUXX+xWrFjhnHMuNzfXHTlypMK2MjMzXVpamsvLy3O5ubmuf//+bvXq1c4550466SS3d+/eSvt/9NFH3X333eecc27nzp2uT58+zjnncnJyyrf/zjvvuMsvv9w559ySJUvcRRdd5Jxz7tlnn3W33nqrc865Sy65xD3//PPOOed+//vflx/rkSNHXE5OjnPOub1797qTTz7ZlZWVuc2bN7sBAwaU1xE4/fDDD7upU6c655z74osvXPfu3V1hYaF79tlnXa9evVx2drYrLCx0PXr0cFu3bq1wPIWFha5bt25u48aNrqyszE2aNKm83uqOKfA4jrVeXavwmfUBMl0VeVQt0PVd27Zwxx3w5ZfQrRssXw6DB8MNN3g3XxEREZGQ5OTkkJ2dzdixYwH4wQ9+wLJlywDIyMhgypQpvPjii8TFxQEwatQo7rzzTh5//HGys7PL5/utWLGCiRMn0qJFC5KTk7n88stZvnz5MWv43ve+x8svvwzA3/72NyZNmlRe26RJk0hLS+OOO+6o0EpdlQ8++ICrr74agGuvvbZ8vnOOe+65h4yMDCZMmMCOHTvYfZy8sGLFivJt9O3bl5NOOomNGzcCcM4555CSkkJiYiL9+/dny5YtFd67YcMGevXqRZ8+fTAzrrnmmvJloR5TuMdeH8QdfxWpF1q0gGefhbvvhtWrvdcvvQQ//7k3LzEx2hWKiIhUzde1oT576623WLZsGQsXLuTBBx9k3bp1zJgxg4suuohFixZxxhln8O6779K3b9/y97gT6FbZtWtX2rZty9q1a1mwYAF//OMfAbj33nsZP348r732GllZWYwbN+642zKzSvPmzp3L3r17WbVqFfHx8fTs2ZOioqJjbudYx9GsWbPy17GxsZSUlIRUB4R+TCdy7NGmFuiGZMIEyMz0+kKfeaY3YsfMmdC/P7zyivpHi4iIHENKSgqtW7cubyV+4YUXGDt2LGVlZWzbto3x48cza9YssrOzycvL4+uvvyY9PZ27776boUOHsmHDhgrbGzNmDK+//joFBQXk5+fz2muvcdZZZx23jsmTJzNr1ixycnJIT08HvFbYrl27Al7f4+MZNWoU8+fPB6hwoWJOTg4dOnQgPj6eJUuWlLcYt2zZsso+3P7j8G9j48aNbN26ldNOO+24NYDXYr1582a+/vprAObNm1ehlqqOKbiWcI+9PlCAbmjM4Oyz4cMPYeVK+Mc/vNbpSZO8Lh5vvKEgLSIiAhQUFNCtW7fyx6OPPsrzzz/P9OnTycjIYM2aNdx3332UlpZyzTXXlF9Ed8cdd5Camspjjz1GWloaAwcOJCkpiQsuuKDC9ocMGcLUqVMZPnw4I0aM4MYbb2Tw4MHHrevKK69k/vz5fO973yuf97Of/Yyf//znjBo1itLS0uNu43e/+x1PPvkkw4YNIycnp3z+lClTyMzMZOjQocydO7e8xbxt27aMGjWKtLQ0pk+fXmFbt9xyC6WlpaSnp3PVVVfx3HPPVWh5PpbExESefvppLrroIkaPHs1JJ5103GMaP34869evL7+IMNxjrw/sRP78EE1Dhw51mZmZ0S6jfikpge9+F956y5uOj4d+/WDiRLj5ZujSJarliYhI0/PFF1/Qr1+/aJchErKqPrNmtso5NzR4XbVANwZxcfDmm/D553DZZZCSAmvXwv/8D3TtCgMGwKWXwgsvQMC3VBEREREJny4ibEwGDIDXX/de79sHH3wAGzfCP//pdfX4xz+8ZSedBFdcAf/1X15LdTWd/0VERESkMrVAN1bt2nmt0dOnw7vvemF66lTo3Nm7w+Gjj3qB+5RT4Lbb4F//guLiaFctIiIiUu+pBbopiImBkSO9B8CuXfD8897rFSvg6afhySe9vtNDhsC118Lll3thW0RE5AQ556od4kykPgn3mkBdRCiwZAk8+KA3ssfhw0fnDxoEF10Ep58O48dDamq0KhQRkQZm8+bNtGzZkrZt2ypES73mnGP//v3k5ubSq1evCsuqu4hQAVqOKinxWqRfecUbIi8hAT76CPxDyrRsCWlpcN553lB6Y8ZEt14REam3jhw5wvbt2497Ew+R+iAxMZFu3boRHx9fYX5UArSZnQ/8DogF/uyceyhoufmWXwgUAFOdc6uPtU0F6Dp28KB3B6l//APWrz/aQh0TAyefDN27w44dXut0v35eF5CRI70+2M2aeSG8WTPvERurCxZFRESkwajzAG1mscBG4FxgO7ASuNo5tz5gnQuB2/EC9Ajgd865EcfargJ0FJWVwZo13qgeX38N+fmwbRusWhX6BYixsd4jORnat/datfPzvemWLb0h+FJTvZvCdOoESUne+m3aQKtW3k1jkpO954QEr9924CNG18WKiIhI7aguQEfyIsLhwCbn3De+AuYDlwHrA9a5DPir81L8x2aWamadnXO7IliXnKiYGK+FeciQyssOH/a6e3zwAXz2GXTo4HX3KCiAOXOgqOjo4/Bh6NEDTjvNG24vEl+ImjXzwrcZ5OV5z7Gx3jHExnrhPCUFCgth9+6jLeP+59RUL6CXlXkBPz7em/aH9lNOgbZtvW3v2OFt18x7jomBPn2geXM4cMC7aNP/RdU575Ge7m1r1y7Yvr3y8m7dvH0fOODtIz7eG+/b/+jRw5uXn+99eYmN9ebHxHjPPXt683JyvHPub/03O/r+mBjv/BcWevsuK/MeMTHeF5bSUti/33t/WdnR+mJivC8xzsGhQ3DkyNHPR0yMd1ytWnmv/efevyw+3vu5tG3rvS4o8GrzfwECr77mzb3t5+Ye3bf/55OU5H3Ziovztu//UhYXd3R5ixbe+/3jnvu/WJl5+2nW7OjPNnDb/s9O4M/ef94C3x8b6y2v6otjXJy3Tmmp1y0q+LOVkOC9v7TUO3fBjRj+L4JlZUfPbfD2zbzlgXfs8m8/cHnwuYOjn4XSUu9RVnb0c1dW5p17M+9zUVxcsX6zoz9b/79l/3z/IznZW//w4aP7D1yekODN8+83mP9nVd3y2Fjv2X/sgf92/D8fM68+/7LAY/fvP/DcBh5j4P6DBX4WgmsL/Ota8LLA6ZgYb7qq+v2/n5zzPjvB2/AvLyurvNx/fP7l1X124uK8fQf/bKHiZ9P/2fX/bgtcN7DmwOfqfnbBn6FjNdyFsrym/J/34PrjfJHoyJGKn104+rsVKn42Amv1fzaDfzb+4/afn8D3BH82At8f/Nnw7z/ws+3n/73j3NGfvb92/+/1+Pijv9cC/937f28mJnrzi4oq/+z9/3/6f08E86/r/71U1bn1fzb9n/3gf3fH+9l36nT0HNQDkaykK7AtYHo7Xivz8dbpCihANzTNmsG4cd4j2J13Vv++0lLYudP7B+1/HDrk3T2xSxcvYL74ojcvL88LVPn5MHiw14Vkzx5YuLBiAHDOG6KvY0dv+Ucfeb9QSkq8R2mpFxBTU72Amp1d8b3Oedvu0AG2boV16yrX/c033nYKCipeeOnn/0VR3S+DhQuPfT7btfP+M8vL8449WGLi0eMRERFp7LZu9bqN1hORDNBVfU0MThOhrIOZ3Qzc7JvMM7Mva1jbiWoH7IvSvhubmp3LpUuPvbyq0Bvo44+PvfzDD4+9fN9xSg9sGTwRx9t+xYty9LmsXTqftUfnsnbpfNYencvaUzfnskePiO+iGidVNTOSAXo7EPhVoRuw8wTWwTn3NPB0bRcYLjPLrKofjIRP57L26FzWLp3P2qNzWbt0PmuPzmXtaarnMpJXXK0E+phZLzNLACYDwX+3XghcZ54zgBz1fxYRERGR+ixiLdDOuRIzuw34F94wds8459aZ2TTf8jnAIrwRODbhDWN3faTqERERERGpDRG9nNE5twgvJAfOmxPw2gG3RrKGWhb1biSNiM5l7dG5rF06n7VH57J26XzWHp3L2tMkz2WDuxOhiIiIiEg06a4TIiIiIiJhUIAOgZmdb2ZfmtkmM5sR7XoaGjN7xsz2mNnnAfPamNk7ZvaV77l1NGtsKMysu5ktMbMvzGydmf3EN1/nM0xmlmhmn5jZZ75z+T+++TqXJ8jMYs3sUzN70zetc3mCzCzLzP5jZmvMLNM3T+fzBPhu0vaKmW3w/e48U+fyxJjZab7PpP9xyMx+2hTPpwL0cfhuSf4kcAHQH7jazPpHt6oG5zng/KB5M4D3nHN9gPd803J8JcB/O+f6AWcAt/o+jzqf4TsMnO2cGwgMAs73jQakc3nifgJ8ETCtc1kz451zgwKGCNP5PDG/A952zvUFBuJ9RnUuT4Bz7kvfZ3IQcDreABCv0QTPpwL08ZXfktw5Vwz4b0kuIXLOLQMOBM2+DHje9/p54Lt1WVND5Zzb5Zxb7Xudi/cfQVd0PsPmPHm+yXjfw6FzeULMrBtwEfDngNk6l7VL5zNMZtYKGAP8BcA5V+ycy0bnsjacA3ztnNtCEzyfCtDHV93txqVmOvrH/PY9d4hyPQ2OmfUEBgP/RufzhPi6HKwB9gDvOOd0Lk/cY8DPgLKAeTqXJ84B/2dmq3x34wWdzxPRG9gLPOvrXvRnM2uBzmVtmAzM871ucudTAfr4QrrduEhdMrNk4O/AT51zh6JdT0PlnCv1/SmyGzDczNKiXFKDZGYXA3ucc6uiXUsjMso5NwSv++CtZjYm2gU1UHHAEOAPzrnBQD5NoHtBpPlukHcp8HK0a4kWBejjC+l24xK23WbWGcD3vCfK9TQYZhaPF57nOude9c3W+awB3590l+L11de5DN8o4FIzy8Lr5na2mb2IzuUJc87t9D3vwetjOhydzxOxHdju++sSwCt4gVrnsmYuAFY753b7ppvc+VSAPr5Qbkku4VsI/MD3+gfAG1GspcEwM8Pry/eFc+7RgEU6n2Eys/Zmlup7nQRMADagcxk259zPnXPdnHM98X5HLnbOXYPO5QkxsxZm1tL/GjgP+Bydz7A5574FtpnZab5Z5wDr0bmsqas52n0DmuD51I1UQmBmF+L17/PfkvxX0a2oYTGzecA4oB2wG7gfeB34G9AD2ApMcs4FX2goQcxsNLAc+A9H+5reg9cPWuczDGaWgXexSyxeY8LfnHMPmFlbdC5PmJmNA+5yzl2sc3lizKw3XqszeF0QXnLO/Urn88SY2SC8i1sTgG+A6/H9m0fnMmxm1hzv2rDezrkc37wm99lUgBYRERERCYO6cIiIiIiIhEEBWkREREQkDArQIiIiIiJhUIAWEREREQmDArSIiIiISBgUoEVEapmZtTWzNb7Ht2a2I2A64TjvHWpmj4ewjw9rqdZxZpYTUN8aM5tQG9v2bX+qmf2+trYnIlIfxEW7ABGRxsY5tx8YBGBmM4E859zD/uVmFuecK6nmvZlAZgj7GFkrxXqWO+cursXtiYg0amqBFhGpA2b2nJk9amZLgN+Y2XAz+9DMPvU9n+Zbb5yZvel7PdPMnjGzpWb2jZn9OGB7eQHrLzWzV8xsg5nN9d2xEjO70DdvhZk97t9uiPX29L33eTNb69t+c9+yc3x1/8dXXzPf/GG+Y/nMzD7x300P6GJmb5vZV2Y2qzbOp4hINClAi4jUnVOBCc65/8a7bfgY59xg4D7gf6t5T1/gO8Bw4H4zi69incHAT4H+QG9glJklAn8ELnDOjQbaH6Ous4K6cJzsm38a8LRzLgM4BNzi2+5zwFXOuXS8v2T+yNc1ZQHwE+fcQLxboxf6tjMIuApIB64ys+7HqEVEpN5TgBYRqTsvO+dKfa9TgJfN7HNgNjCgmve85Zw77JzbB+wBOlaxzifOue3OuTJgDdATL3h/45zb7Ftn3jHqWu6cGxTw+No3f5tz7gPf6xeB0XiherNzbqNv/vPAGN/8Xc65lQDOuUMB3VTec87lOOeKgPXASceoRUSk3lOAFhGpO/kBrx8Eljjn0oBLgMRq3nM44HUpVV+7UtU6VoM6/VwV09Vt16pY3y+UYxARaTAUoEVEoiMF2OF7PTUC298A9Daznr7pq05gGz3M7Ezf66uBFb7t9jSzU3zzrwXe983vYmbDAMyspZkpKItIo6QALSISHbOAX5vZB0BsbW/cOVcI3AK8bWYrgN1ATjWrB/eBvtI3/wvgB2a2FmgD/MHXDeN6vO4n/wHKgDnOuWK8kP6EmX0GvEP1reoiIg2aOVfdX9xERKQhM7Nk51yeb1SOJ4GvnHOzQ3xvT+BNXxcTEREJoBZoEZHG6yYzWwOsw+sy8sfoliMi0jioBVpEREREJAxqgRYRERERCYMCtIiIiIhIGBSgRURERETCoAAtIiIiIhIGBWgRERERkTAoQIuIiIiIhOH/A1lbBOJ1ZRPxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for history in train_histories:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(history['accuracy'], \"g--\", label=\"Accuracy of training data\")\n",
    "    plt.plot(history['val_accuracy'], \"g\", label=\"Accuracy of validation data\")\n",
    "    plt.plot(history['loss'], \"r--\", label=\"Loss of training data\")\n",
    "    plt.plot(history['val_loss'], \"r\", label=\"Loss of validation data\")\n",
    "    plt.title('Model Accuracy and Loss')\n",
    "    plt.ylabel('Accuracy and Loss')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylim(0)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.088730</td>\n",
       "      <td>-0.127596</td>\n",
       "      <td>-1.385538</td>\n",
       "      <td>-0.892889</td>\n",
       "      <td>0.194735</td>\n",
       "      <td>-1.746761</td>\n",
       "      <td>1.421869</td>\n",
       "      <td>-0.235242</td>\n",
       "      <td>1.229765</td>\n",
       "      <td>1.241227</td>\n",
       "      <td>-0.456665</td>\n",
       "      <td>0.323502</td>\n",
       "      <td>0.954254</td>\n",
       "      <td>1.084440</td>\n",
       "      <td>0.343333</td>\n",
       "      <td>0.105532</td>\n",
       "      <td>0.202017</td>\n",
       "      <td>-0.993289</td>\n",
       "      <td>-0.429881</td>\n",
       "      <td>-1.105154</td>\n",
       "      <td>0.279719</td>\n",
       "      <td>0.688370</td>\n",
       "      <td>0.176828</td>\n",
       "      <td>0.445442</td>\n",
       "      <td>-1.543907</td>\n",
       "      <td>1.005676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.069072</td>\n",
       "      <td>-0.127857</td>\n",
       "      <td>-1.381500</td>\n",
       "      <td>-0.891119</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>-1.752683</td>\n",
       "      <td>1.419024</td>\n",
       "      <td>-0.256786</td>\n",
       "      <td>1.230025</td>\n",
       "      <td>1.256185</td>\n",
       "      <td>-0.486099</td>\n",
       "      <td>0.323691</td>\n",
       "      <td>0.949866</td>\n",
       "      <td>1.080554</td>\n",
       "      <td>0.289763</td>\n",
       "      <td>0.160513</td>\n",
       "      <td>0.201982</td>\n",
       "      <td>-0.996781</td>\n",
       "      <td>-0.430804</td>\n",
       "      <td>-1.087847</td>\n",
       "      <td>0.279674</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>0.179340</td>\n",
       "      <td>0.447299</td>\n",
       "      <td>-1.531123</td>\n",
       "      <td>0.991554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.049455</td>\n",
       "      <td>-0.117360</td>\n",
       "      <td>-1.377789</td>\n",
       "      <td>-0.889368</td>\n",
       "      <td>0.197791</td>\n",
       "      <td>-1.758113</td>\n",
       "      <td>1.413791</td>\n",
       "      <td>-0.277053</td>\n",
       "      <td>1.222976</td>\n",
       "      <td>1.269993</td>\n",
       "      <td>-0.515316</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.945764</td>\n",
       "      <td>1.051407</td>\n",
       "      <td>0.211843</td>\n",
       "      <td>0.154404</td>\n",
       "      <td>0.201628</td>\n",
       "      <td>-1.003071</td>\n",
       "      <td>-0.430832</td>\n",
       "      <td>-1.051580</td>\n",
       "      <td>0.279797</td>\n",
       "      <td>0.735881</td>\n",
       "      <td>0.179590</td>\n",
       "      <td>0.461162</td>\n",
       "      <td>-1.523140</td>\n",
       "      <td>0.993164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.026340</td>\n",
       "      <td>-0.108389</td>\n",
       "      <td>-1.375465</td>\n",
       "      <td>-0.887544</td>\n",
       "      <td>0.200528</td>\n",
       "      <td>-1.761438</td>\n",
       "      <td>1.411215</td>\n",
       "      <td>-0.294904</td>\n",
       "      <td>1.216957</td>\n",
       "      <td>1.278377</td>\n",
       "      <td>-0.551185</td>\n",
       "      <td>0.309433</td>\n",
       "      <td>0.942908</td>\n",
       "      <td>1.004772</td>\n",
       "      <td>0.143664</td>\n",
       "      <td>0.123859</td>\n",
       "      <td>0.200865</td>\n",
       "      <td>-1.008354</td>\n",
       "      <td>-0.430931</td>\n",
       "      <td>-1.014338</td>\n",
       "      <td>0.281028</td>\n",
       "      <td>0.763925</td>\n",
       "      <td>0.180692</td>\n",
       "      <td>0.483217</td>\n",
       "      <td>-1.513884</td>\n",
       "      <td>1.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000713</td>\n",
       "      <td>-0.108898</td>\n",
       "      <td>-1.374981</td>\n",
       "      <td>-0.885748</td>\n",
       "      <td>0.200356</td>\n",
       "      <td>-1.762112</td>\n",
       "      <td>1.413854</td>\n",
       "      <td>-0.308670</td>\n",
       "      <td>1.217324</td>\n",
       "      <td>1.280049</td>\n",
       "      <td>-0.592113</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.942246</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.099834</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.200321</td>\n",
       "      <td>-1.011405</td>\n",
       "      <td>-0.431585</td>\n",
       "      <td>-0.984460</td>\n",
       "      <td>0.281763</td>\n",
       "      <td>0.778641</td>\n",
       "      <td>0.184395</td>\n",
       "      <td>0.501677</td>\n",
       "      <td>-1.504520</td>\n",
       "      <td>1.012496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.471750</td>\n",
       "      <td>1.741507</td>\n",
       "      <td>0.841135</td>\n",
       "      <td>1.524948</td>\n",
       "      <td>-1.316009</td>\n",
       "      <td>0.262825</td>\n",
       "      <td>-1.472785</td>\n",
       "      <td>-0.954595</td>\n",
       "      <td>-0.794735</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>-1.176676</td>\n",
       "      <td>-1.374970</td>\n",
       "      <td>-1.118399</td>\n",
       "      <td>-1.688394</td>\n",
       "      <td>-0.830329</td>\n",
       "      <td>-0.254896</td>\n",
       "      <td>-1.285691</td>\n",
       "      <td>1.526608</td>\n",
       "      <td>1.086842</td>\n",
       "      <td>1.789111</td>\n",
       "      <td>1.567759</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>1.602927</td>\n",
       "      <td>-2.130023</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-1.913764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.810128</td>\n",
       "      <td>2.243463</td>\n",
       "      <td>0.575408</td>\n",
       "      <td>1.524948</td>\n",
       "      <td>-1.316009</td>\n",
       "      <td>0.262825</td>\n",
       "      <td>-1.259638</td>\n",
       "      <td>-0.954595</td>\n",
       "      <td>-0.794735</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>-1.176676</td>\n",
       "      <td>-1.374970</td>\n",
       "      <td>-1.118399</td>\n",
       "      <td>-1.688394</td>\n",
       "      <td>-0.830329</td>\n",
       "      <td>-0.254896</td>\n",
       "      <td>-1.285691</td>\n",
       "      <td>1.526608</td>\n",
       "      <td>1.086842</td>\n",
       "      <td>1.789111</td>\n",
       "      <td>1.567759</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>1.602927</td>\n",
       "      <td>-2.130023</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-1.913764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1.397171</td>\n",
       "      <td>2.578076</td>\n",
       "      <td>0.296587</td>\n",
       "      <td>1.524948</td>\n",
       "      <td>-1.316009</td>\n",
       "      <td>0.262825</td>\n",
       "      <td>-0.894243</td>\n",
       "      <td>-0.954595</td>\n",
       "      <td>-0.794735</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>-1.176676</td>\n",
       "      <td>-1.374970</td>\n",
       "      <td>-1.118399</td>\n",
       "      <td>-1.688394</td>\n",
       "      <td>-0.830329</td>\n",
       "      <td>-0.254896</td>\n",
       "      <td>-1.285691</td>\n",
       "      <td>1.526608</td>\n",
       "      <td>1.086842</td>\n",
       "      <td>1.789111</td>\n",
       "      <td>1.567759</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>1.602927</td>\n",
       "      <td>-2.130023</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-1.913764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1.615547</td>\n",
       "      <td>2.954857</td>\n",
       "      <td>0.107144</td>\n",
       "      <td>1.524948</td>\n",
       "      <td>-1.316009</td>\n",
       "      <td>0.262825</td>\n",
       "      <td>-0.707935</td>\n",
       "      <td>-0.954595</td>\n",
       "      <td>-0.794735</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>-1.176676</td>\n",
       "      <td>-1.374970</td>\n",
       "      <td>-1.118399</td>\n",
       "      <td>-1.688394</td>\n",
       "      <td>-0.830329</td>\n",
       "      <td>-0.254896</td>\n",
       "      <td>-1.285691</td>\n",
       "      <td>1.526608</td>\n",
       "      <td>1.086842</td>\n",
       "      <td>1.789111</td>\n",
       "      <td>1.567759</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>1.602927</td>\n",
       "      <td>-2.130023</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-1.913764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.649198</td>\n",
       "      <td>3.051131</td>\n",
       "      <td>0.064969</td>\n",
       "      <td>1.524948</td>\n",
       "      <td>-1.316009</td>\n",
       "      <td>0.262825</td>\n",
       "      <td>-0.670417</td>\n",
       "      <td>-0.954595</td>\n",
       "      <td>-0.794735</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>-1.176676</td>\n",
       "      <td>-1.374970</td>\n",
       "      <td>-1.118399</td>\n",
       "      <td>-1.688394</td>\n",
       "      <td>-0.830329</td>\n",
       "      <td>-0.254896</td>\n",
       "      <td>-1.285691</td>\n",
       "      <td>1.526608</td>\n",
       "      <td>1.086842</td>\n",
       "      <td>1.789111</td>\n",
       "      <td>1.567759</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>1.602927</td>\n",
       "      <td>-2.130023</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-1.913764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -1.088730 -0.127596 -1.385538 -0.892889  0.194735 -1.746761  1.421869   \n",
       "1   -1.069072 -0.127857 -1.381500 -0.891119  0.194600 -1.752683  1.419024   \n",
       "2   -1.049455 -0.117360 -1.377789 -0.889368  0.197791 -1.758113  1.413791   \n",
       "3   -1.026340 -0.108389 -1.375465 -0.887544  0.200528 -1.761438  1.411215   \n",
       "4   -1.000713 -0.108898 -1.374981 -0.885748  0.200356 -1.762112  1.413854   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "235  0.471750  1.741507  0.841135  1.524948 -1.316009  0.262825 -1.472785   \n",
       "236  0.810128  2.243463  0.575408  1.524948 -1.316009  0.262825 -1.259638   \n",
       "237  1.397171  2.578076  0.296587  1.524948 -1.316009  0.262825 -0.894243   \n",
       "238  1.615547  2.954857  0.107144  1.524948 -1.316009  0.262825 -0.707935   \n",
       "239  1.649198  3.051131  0.064969  1.524948 -1.316009  0.262825 -0.670417   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -0.235242  1.229765  1.241227 -0.456665  0.323502  0.954254  1.084440   \n",
       "1   -0.256786  1.230025  1.256185 -0.486099  0.323691  0.949866  1.080554   \n",
       "2   -0.277053  1.222976  1.269993 -0.515316  0.315999  0.945764  1.051407   \n",
       "3   -0.294904  1.216957  1.278377 -0.551185  0.309433  0.942908  1.004772   \n",
       "4   -0.308670  1.217324  1.280049 -0.592113  0.309804  0.942246  0.965909   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "235 -0.954595 -0.794735  0.063399 -1.176676 -1.374970 -1.118399 -1.688394   \n",
       "236 -0.954595 -0.794735  0.063399 -1.176676 -1.374970 -1.118399 -1.688394   \n",
       "237 -0.954595 -0.794735  0.063399 -1.176676 -1.374970 -1.118399 -1.688394   \n",
       "238 -0.954595 -0.794735  0.063399 -1.176676 -1.374970 -1.118399 -1.688394   \n",
       "239 -0.954595 -0.794735  0.063399 -1.176676 -1.374970 -1.118399 -1.688394   \n",
       "\n",
       "           14        15        16        17        18        19        20  \\\n",
       "0    0.343333  0.105532  0.202017 -0.993289 -0.429881 -1.105154  0.279719   \n",
       "1    0.289763  0.160513  0.201982 -0.996781 -0.430804 -1.087847  0.279674   \n",
       "2    0.211843  0.154404  0.201628 -1.003071 -0.430832 -1.051580  0.279797   \n",
       "3    0.143664  0.123859  0.200865 -1.008354 -0.430931 -1.014338  0.281028   \n",
       "4    0.099834  0.081096  0.200321 -1.011405 -0.431585 -0.984460  0.281763   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "235 -0.830329 -0.254896 -1.285691  1.526608  1.086842  1.789111  1.567759   \n",
       "236 -0.830329 -0.254896 -1.285691  1.526608  1.086842  1.789111  1.567759   \n",
       "237 -0.830329 -0.254896 -1.285691  1.526608  1.086842  1.789111  1.567759   \n",
       "238 -0.830329 -0.254896 -1.285691  1.526608  1.086842  1.789111  1.567759   \n",
       "239 -0.830329 -0.254896 -1.285691  1.526608  1.086842  1.789111  1.567759   \n",
       "\n",
       "           21        22        23        24        25  \n",
       "0    0.688370  0.176828  0.445442 -1.543907  1.005676  \n",
       "1    0.702085  0.179340  0.447299 -1.531123  0.991554  \n",
       "2    0.735881  0.179590  0.461162 -1.523140  0.993164  \n",
       "3    0.763925  0.180692  0.483217 -1.513884  1.001204  \n",
       "4    0.778641  0.184395  0.501677 -1.504520  1.012496  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "235  0.794153  1.602927 -2.130023 -0.051556 -1.913764  \n",
       "236  0.794153  1.602927 -2.130023 -0.051556 -1.913764  \n",
       "237  0.794153  1.602927 -2.130023 -0.051556 -1.913764  \n",
       "238  0.794153  1.602927 -2.130023 -0.051556 -1.913764  \n",
       "239  0.794153  1.602927 -2.130023 -0.051556 -1.913764  \n",
       "\n",
       "[240 rows x 26 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler2 = StandardScaler()\n",
    "# X_train = scaler2.fit_transform(X_train)\n",
    "X_val2 = scaler2.fit_transform(X_val)\n",
    "hhdds = pd.DataFrame(X_val2)\n",
    "hhdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val2 = to_categorical(y_val, 3)\n",
    "ds22 = mlp_model.evaluate(X_val, y_val2, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000791842641774565, 1.0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred2 = np.argmax(mlp_model.predict(X_val), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.to_csv('dataset/extracted_features/features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", 100)\n",
    "np.set_printoptions(threshold=1000) # 'sys.maxsize' for max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_acc_mean_train = pd.DataFrame(get_mean(xyz_acc_train), columns=['acc_X_mean', 'acc_Y_mean', 'acc_Z_mean'])\n",
    "xyz_acc_std_train = pd.DataFrame(get_std(xyz_acc_train), columns=['acc_X_std', 'acc_Y_std', 'acc_Z_std'])\n",
    "xyz_acc_mag_train = pd.DataFrame(get_mag(xyz_acc_mean_train), columns=['acc_mag'])\n",
    "xyz_acc_kurtosis_train = pd.DataFrame(get_kurtosis(xyz_acc_train), columns=['acc_X_kurtosis', 'acc_Y_kurtosis', 'acc_Z_kurtosis'])\n",
    "xyz_acc_skew_train = pd.DataFrame(get_skew(xyz_acc_train), columns=['acc_X_skew', 'acc_Y_skew', 'acc_Z_skew'])\n",
    "# xyz_acc_ft = pd.DataFrame(fourier_transform(xyz_acc_mean), columns=['acc_X_fourier', 'acc_Y_fourier', 'acc_Z_fourier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_acc_mean_test = pd.DataFrame(get_mean(xyz_acc_test), columns=['acc_X_mean', 'acc_Y_mean', 'acc_Z_mean'])\n",
    "xyz_acc_std_test = pd.DataFrame(get_std(xyz_acc_test), columns=['acc_X_std', 'acc_Y_std', 'acc_Z_std'])\n",
    "xyz_acc_mag_test = pd.DataFrame(get_mag(xyz_acc_mean_test), columns=['acc_mag'])\n",
    "xyz_acc_kurtosis_test = pd.DataFrame(get_kurtosis(xyz_acc_test), columns=['acc_X_kurtosis', 'acc_Y_kurtosis', 'acc_Z_kurtosis'])\n",
    "xyz_acc_skew_test = pd.DataFrame(get_skew(xyz_acc_test), columns=['acc_X_skew', 'acc_Y_skew', 'acc_Z_skew'])\n",
    "# xyz_acc_ft = pd.DataFrame(fourier_transform(xyz_acc_mean), columns=['acc_X_fourier', 'acc_Y_fourier', 'acc_Z_fourier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_gyro_mean_train = pd.DataFrame(get_mean(xyz_gyro_train), columns=['gyro_X_mean', 'gyro_Y_mean', 'gyro_Z_mean'])\n",
    "xyz_gyro_std_train = pd.DataFrame(get_std(xyz_gyro_train), columns=['gyro_X_std', 'gyro_Y_std', 'gyro_Z_std'])\n",
    "xyz_gyro_mag_train = pd.DataFrame(get_mag(xyz_gyro_mean_train), columns=['gyro_mag'])\n",
    "xyz_gyro_kurtosis_train = pd.DataFrame(get_kurtosis(xyz_gyro_train), columns=['gyro_X_kurtosis', 'gyro_Y_kurtosis', 'gyro_Z_kurtosis'])\n",
    "xyz_gyro_skew_train = pd.DataFrame(get_skew(xyz_gyro_train), columns=['gyro_X_skew', 'gyro_Y_skew', 'gyro_Z_skew'])\n",
    "# xyz_gyro_ft = pd.DataFrame(fourier_transform(xyz_gyro), columns=['gyro_X_fourier', 'gyro_Y_fourier', 'gyro_Z_fourier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_gyro_mean_test = pd.DataFrame(get_mean(xyz_gyro_test), columns=['gyro_X_mean', 'gyro_Y_mean', 'gyro_Z_mean'])\n",
    "xyz_gyro_std_test = pd.DataFrame(get_std(xyz_gyro_test), columns=['gyro_X_std', 'gyro_Y_std', 'gyro_Z_std'])\n",
    "xyz_gyro_mag_test = pd.DataFrame(get_mag(xyz_gyro_mean_test), columns=['gyro_mag'])\n",
    "xyz_gyro_kurtosis_test = pd.DataFrame(get_kurtosis(xyz_gyro_test), columns=['gyro_X_kurtosis', 'gyro_Y_kurtosis', 'gyro_Z_kurtosis'])\n",
    "xyz_gyro_skew_test = pd.DataFrame(get_skew(xyz_gyro_test), columns=['gyro_X_skew', 'gyro_Y_skew', 'gyro_Z_skew'])\n",
    "# xyz_gyro_ft = pd.DataFrame(fourier_transform(xyz_gyro), columns=['gyro_X_fourier', 'gyro_Y_fourier', 'gyro_Z_fourier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data883 = []\n",
    "# data_temp883 = []\n",
    "# for col in range(xyz_acc.shape[1]):\n",
    "#     for row in range(xyz_acc.shape[0]):\n",
    "#         print('count: ', row)\n",
    "# #         print(xyz_acc.iloc[row, col])\n",
    "#         dsd2 = np.std(xyz_acc.iloc[row, col])\n",
    "#         print(dsd2)\n",
    "#         data_temp883.append(dsd2)\n",
    "# data_temp883 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in range(xyz_acc_mean.shape[1]):\n",
    "#     for row in range(xyz_acc_mean.shape[0]):\n",
    "#         print(xyz_acc_mean.values[row][col])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features_df = pd.concat([xyz_acc_mean, xyz_acc_std], axis=1)\n",
    "features_train = pd.concat([xyz_acc_mean_train, xyz_acc_std_train, xyz_acc_mag_train,\n",
    "                         xyz_acc_kurtosis_train, xyz_acc_skew_train,\n",
    "                         xyz_gyro_mean_train, xyz_gyro_std_train, xyz_gyro_mag_train,\n",
    "                         xyz_gyro_kurtosis_train, xyz_gyro_skew_train], axis=1)\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df = pd.concat([xyz_acc_mean, xyz_acc_std], axis=1)\n",
    "features_test = pd.concat([xyz_acc_mean_test, xyz_acc_std_test, xyz_acc_mag_test,\n",
    "                         xyz_acc_kurtosis_test, xyz_acc_skew_test,\n",
    "                         xyz_gyro_mean_test, xyz_gyro_std_test, xyz_gyro_mag_test,\n",
    "                         xyz_gyro_kurtosis_test, xyz_gyro_skew_test], axis=1)\n",
    "features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train['Dancer'] = 'Alex'\n",
    "features_train['Move'] = train_danceMove_Alex.movename\n",
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_test['Dancer'] = 'Alex'\n",
    "features_test['Move'] = test_danceMove_Alex.movename\n",
    "features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_train.iloc[:, :-2]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_X_mean</th>\n",
       "      <th>acc_Y_mean</th>\n",
       "      <th>acc_Z_mean</th>\n",
       "      <th>acc_X_std</th>\n",
       "      <th>acc_Y_std</th>\n",
       "      <th>acc_Z_std</th>\n",
       "      <th>acc_mag</th>\n",
       "      <th>acc_X_kurtosis</th>\n",
       "      <th>acc_Y_kurtosis</th>\n",
       "      <th>acc_Z_kurtosis</th>\n",
       "      <th>acc_X_skew</th>\n",
       "      <th>acc_Y_skew</th>\n",
       "      <th>acc_Z_skew</th>\n",
       "      <th>gyro_X_mean</th>\n",
       "      <th>gyro_Y_mean</th>\n",
       "      <th>gyro_Z_mean</th>\n",
       "      <th>gyro_X_std</th>\n",
       "      <th>gyro_Y_std</th>\n",
       "      <th>gyro_Z_std</th>\n",
       "      <th>gyro_mag</th>\n",
       "      <th>gyro_X_kurtosis</th>\n",
       "      <th>gyro_Y_kurtosis</th>\n",
       "      <th>gyro_Z_kurtosis</th>\n",
       "      <th>gyro_X_skew</th>\n",
       "      <th>gyro_Y_skew</th>\n",
       "      <th>gyro_Z_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.256790</td>\n",
       "      <td>-0.462554</td>\n",
       "      <td>-0.784829</td>\n",
       "      <td>0.130087</td>\n",
       "      <td>0.591135</td>\n",
       "      <td>0.371541</td>\n",
       "      <td>0.946495</td>\n",
       "      <td>-0.224492</td>\n",
       "      <td>2.002373</td>\n",
       "      <td>-0.555826</td>\n",
       "      <td>-0.382008</td>\n",
       "      <td>0.578464</td>\n",
       "      <td>0.735185</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>-2.285156</td>\n",
       "      <td>136.164216</td>\n",
       "      <td>27.500832</td>\n",
       "      <td>49.382057</td>\n",
       "      <td>6.376557</td>\n",
       "      <td>-0.071116</td>\n",
       "      <td>3.116190</td>\n",
       "      <td>2.278384</td>\n",
       "      <td>-0.150386</td>\n",
       "      <td>-1.422626</td>\n",
       "      <td>0.155111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257769</td>\n",
       "      <td>-0.462569</td>\n",
       "      <td>-0.784113</td>\n",
       "      <td>0.130428</td>\n",
       "      <td>0.591126</td>\n",
       "      <td>0.371121</td>\n",
       "      <td>0.946176</td>\n",
       "      <td>-0.240044</td>\n",
       "      <td>2.002696</td>\n",
       "      <td>-0.551996</td>\n",
       "      <td>-0.398504</td>\n",
       "      <td>0.578550</td>\n",
       "      <td>0.733888</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>-6.152344</td>\n",
       "      <td>-2.109375</td>\n",
       "      <td>136.162653</td>\n",
       "      <td>27.423676</td>\n",
       "      <td>49.367088</td>\n",
       "      <td>6.515626</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>3.138564</td>\n",
       "      <td>2.282634</td>\n",
       "      <td>-0.149532</td>\n",
       "      <td>-1.414567</td>\n",
       "      <td>0.144675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.258747</td>\n",
       "      <td>-0.461937</td>\n",
       "      <td>-0.783456</td>\n",
       "      <td>0.130765</td>\n",
       "      <td>0.591316</td>\n",
       "      <td>0.370737</td>\n",
       "      <td>0.945589</td>\n",
       "      <td>-0.254675</td>\n",
       "      <td>1.993933</td>\n",
       "      <td>-0.548460</td>\n",
       "      <td>-0.414879</td>\n",
       "      <td>0.575078</td>\n",
       "      <td>0.732676</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-6.464844</td>\n",
       "      <td>-2.128906</td>\n",
       "      <td>136.146788</td>\n",
       "      <td>27.284667</td>\n",
       "      <td>49.366636</td>\n",
       "      <td>6.807054</td>\n",
       "      <td>-0.071051</td>\n",
       "      <td>3.193697</td>\n",
       "      <td>2.283057</td>\n",
       "      <td>-0.143155</td>\n",
       "      <td>-1.409535</td>\n",
       "      <td>0.145865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259899</td>\n",
       "      <td>-0.461397</td>\n",
       "      <td>-0.783044</td>\n",
       "      <td>0.131116</td>\n",
       "      <td>0.591479</td>\n",
       "      <td>0.370501</td>\n",
       "      <td>0.945301</td>\n",
       "      <td>-0.267562</td>\n",
       "      <td>1.986450</td>\n",
       "      <td>-0.546314</td>\n",
       "      <td>-0.434982</td>\n",
       "      <td>0.572115</td>\n",
       "      <td>0.731832</td>\n",
       "      <td>-0.371094</td>\n",
       "      <td>-6.738281</td>\n",
       "      <td>-2.226562</td>\n",
       "      <td>136.112690</td>\n",
       "      <td>27.167927</td>\n",
       "      <td>49.365032</td>\n",
       "      <td>7.106316</td>\n",
       "      <td>-0.070031</td>\n",
       "      <td>3.239446</td>\n",
       "      <td>2.284921</td>\n",
       "      <td>-0.133009</td>\n",
       "      <td>-1.403701</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.261176</td>\n",
       "      <td>-0.461428</td>\n",
       "      <td>-0.782958</td>\n",
       "      <td>0.131461</td>\n",
       "      <td>0.591469</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.945596</td>\n",
       "      <td>-0.277500</td>\n",
       "      <td>1.986906</td>\n",
       "      <td>-0.545886</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>0.572282</td>\n",
       "      <td>0.731636</td>\n",
       "      <td>-0.761719</td>\n",
       "      <td>-6.914062</td>\n",
       "      <td>-2.363281</td>\n",
       "      <td>136.088361</td>\n",
       "      <td>27.100509</td>\n",
       "      <td>49.354425</td>\n",
       "      <td>7.346399</td>\n",
       "      <td>-0.069421</td>\n",
       "      <td>3.263453</td>\n",
       "      <td>2.291185</td>\n",
       "      <td>-0.124516</td>\n",
       "      <td>-1.397798</td>\n",
       "      <td>0.160152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.334560</td>\n",
       "      <td>-0.350046</td>\n",
       "      <td>-0.390334</td>\n",
       "      <td>0.595231</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>-0.743796</td>\n",
       "      <td>-0.514374</td>\n",
       "      <td>-0.857406</td>\n",
       "      <td>-0.785536</td>\n",
       "      <td>-0.188064</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>-27.441406</td>\n",
       "      <td>-10.644531</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>69.624660</td>\n",
       "      <td>83.184039</td>\n",
       "      <td>73.982519</td>\n",
       "      <td>29.633650</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>3.288758</td>\n",
       "      <td>4.690904</td>\n",
       "      <td>-1.335196</td>\n",
       "      <td>-0.481920</td>\n",
       "      <td>-2.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.351424</td>\n",
       "      <td>-0.319832</td>\n",
       "      <td>-0.437412</td>\n",
       "      <td>0.595231</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.645849</td>\n",
       "      <td>-0.743796</td>\n",
       "      <td>-0.514374</td>\n",
       "      <td>-0.857406</td>\n",
       "      <td>-0.785536</td>\n",
       "      <td>-0.188064</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>-27.441406</td>\n",
       "      <td>-10.644531</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>69.624660</td>\n",
       "      <td>83.184039</td>\n",
       "      <td>73.982519</td>\n",
       "      <td>29.633650</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>3.288758</td>\n",
       "      <td>4.690904</td>\n",
       "      <td>-1.335196</td>\n",
       "      <td>-0.481920</td>\n",
       "      <td>-2.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.380681</td>\n",
       "      <td>-0.299691</td>\n",
       "      <td>-0.486810</td>\n",
       "      <td>0.595231</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.686817</td>\n",
       "      <td>-0.743796</td>\n",
       "      <td>-0.514374</td>\n",
       "      <td>-0.857406</td>\n",
       "      <td>-0.785536</td>\n",
       "      <td>-0.188064</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>-27.441406</td>\n",
       "      <td>-10.644531</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>69.624660</td>\n",
       "      <td>83.184039</td>\n",
       "      <td>73.982519</td>\n",
       "      <td>29.633650</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>3.288758</td>\n",
       "      <td>4.690904</td>\n",
       "      <td>-1.335196</td>\n",
       "      <td>-0.481920</td>\n",
       "      <td>-2.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.391565</td>\n",
       "      <td>-0.277011</td>\n",
       "      <td>-0.520373</td>\n",
       "      <td>0.595231</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.707705</td>\n",
       "      <td>-0.743796</td>\n",
       "      <td>-0.514374</td>\n",
       "      <td>-0.857406</td>\n",
       "      <td>-0.785536</td>\n",
       "      <td>-0.188064</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>-27.441406</td>\n",
       "      <td>-10.644531</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>69.624660</td>\n",
       "      <td>83.184039</td>\n",
       "      <td>73.982519</td>\n",
       "      <td>29.633650</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>3.288758</td>\n",
       "      <td>4.690904</td>\n",
       "      <td>-1.335196</td>\n",
       "      <td>-0.481920</td>\n",
       "      <td>-2.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.393242</td>\n",
       "      <td>-0.271216</td>\n",
       "      <td>-0.527845</td>\n",
       "      <td>0.595231</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>0.711911</td>\n",
       "      <td>-0.743796</td>\n",
       "      <td>-0.514374</td>\n",
       "      <td>-0.857406</td>\n",
       "      <td>-0.785536</td>\n",
       "      <td>-0.188064</td>\n",
       "      <td>0.122606</td>\n",
       "      <td>-27.441406</td>\n",
       "      <td>-10.644531</td>\n",
       "      <td>-3.437500</td>\n",
       "      <td>69.624660</td>\n",
       "      <td>83.184039</td>\n",
       "      <td>73.982519</td>\n",
       "      <td>29.633650</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>3.288758</td>\n",
       "      <td>4.690904</td>\n",
       "      <td>-1.335196</td>\n",
       "      <td>-0.481920</td>\n",
       "      <td>-2.002434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acc_X_mean  acc_Y_mean  acc_Z_mean  acc_X_std  acc_Y_std  acc_Z_std  \\\n",
       "0      0.256790   -0.462554   -0.784829   0.130087   0.591135   0.371541   \n",
       "1      0.257769   -0.462569   -0.784113   0.130428   0.591126   0.371121   \n",
       "2      0.258747   -0.461937   -0.783456   0.130765   0.591316   0.370737   \n",
       "3      0.259899   -0.461397   -0.783044   0.131116   0.591479   0.370501   \n",
       "4      0.261176   -0.461428   -0.782958   0.131461   0.591469   0.370453   \n",
       "..          ...         ...         ...        ...        ...        ...   \n",
       "235    0.334560   -0.350046   -0.390334   0.595231   0.501368   0.513918   \n",
       "236    0.351424   -0.319832   -0.437412   0.595231   0.501368   0.513918   \n",
       "237    0.380681   -0.299691   -0.486810   0.595231   0.501368   0.513918   \n",
       "238    0.391565   -0.277011   -0.520373   0.595231   0.501368   0.513918   \n",
       "239    0.393242   -0.271216   -0.527845   0.595231   0.501368   0.513918   \n",
       "\n",
       "      acc_mag  acc_X_kurtosis  acc_Y_kurtosis  acc_Z_kurtosis  acc_X_skew  \\\n",
       "0    0.946495       -0.224492        2.002373       -0.555826   -0.382008   \n",
       "1    0.946176       -0.240044        2.002696       -0.551996   -0.398504   \n",
       "2    0.945589       -0.254675        1.993933       -0.548460   -0.414879   \n",
       "3    0.945301       -0.267562        1.986450       -0.546314   -0.434982   \n",
       "4    0.945596       -0.277500        1.986906       -0.545886   -0.457919   \n",
       "..        ...             ...             ...             ...         ...   \n",
       "235  0.621951       -0.743796       -0.514374       -0.857406   -0.785536   \n",
       "236  0.645849       -0.743796       -0.514374       -0.857406   -0.785536   \n",
       "237  0.686817       -0.743796       -0.514374       -0.857406   -0.785536   \n",
       "238  0.707705       -0.743796       -0.514374       -0.857406   -0.785536   \n",
       "239  0.711911       -0.743796       -0.514374       -0.857406   -0.785536   \n",
       "\n",
       "     acc_Y_skew  acc_Z_skew  gyro_X_mean  gyro_Y_mean  gyro_Z_mean  \\\n",
       "0      0.578464    0.735185     0.429688    -5.937500    -2.285156   \n",
       "1      0.578550    0.733888     0.390625    -6.152344    -2.109375   \n",
       "2      0.575078    0.732676     0.097656    -6.464844    -2.128906   \n",
       "3      0.572115    0.731832    -0.371094    -6.738281    -2.226562   \n",
       "4      0.572282    0.731636    -0.761719    -6.914062    -2.363281   \n",
       "..          ...         ...          ...          ...          ...   \n",
       "235   -0.188064    0.122606   -27.441406   -10.644531    -3.437500   \n",
       "236   -0.188064    0.122606   -27.441406   -10.644531    -3.437500   \n",
       "237   -0.188064    0.122606   -27.441406   -10.644531    -3.437500   \n",
       "238   -0.188064    0.122606   -27.441406   -10.644531    -3.437500   \n",
       "239   -0.188064    0.122606   -27.441406   -10.644531    -3.437500   \n",
       "\n",
       "     gyro_X_std  gyro_Y_std  gyro_Z_std   gyro_mag  gyro_X_kurtosis  \\\n",
       "0    136.164216   27.500832   49.382057   6.376557        -0.071116   \n",
       "1    136.162653   27.423676   49.367088   6.515626        -0.071154   \n",
       "2    136.146788   27.284667   49.366636   6.807054        -0.071051   \n",
       "3    136.112690   27.167927   49.365032   7.106316        -0.070031   \n",
       "4    136.088361   27.100509   49.354425   7.346399        -0.069421   \n",
       "..          ...         ...         ...        ...              ...   \n",
       "235   69.624660   83.184039   73.982519  29.633650         0.997362   \n",
       "236   69.624660   83.184039   73.982519  29.633650         0.997362   \n",
       "237   69.624660   83.184039   73.982519  29.633650         0.997362   \n",
       "238   69.624660   83.184039   73.982519  29.633650         0.997362   \n",
       "239   69.624660   83.184039   73.982519  29.633650         0.997362   \n",
       "\n",
       "     gyro_Y_kurtosis  gyro_Z_kurtosis  gyro_X_skew  gyro_Y_skew  gyro_Z_skew  \n",
       "0           3.116190         2.278384    -0.150386    -1.422626     0.155111  \n",
       "1           3.138564         2.282634    -0.149532    -1.414567     0.144675  \n",
       "2           3.193697         2.283057    -0.143155    -1.409535     0.145865  \n",
       "3           3.239446         2.284921    -0.133009    -1.403701     0.151807  \n",
       "4           3.263453         2.291185    -0.124516    -1.397798     0.160152  \n",
       "..               ...              ...          ...          ...          ...  \n",
       "235         3.288758         4.690904    -1.335196    -0.481920    -2.002434  \n",
       "236         3.288758         4.690904    -1.335196    -0.481920    -2.002434  \n",
       "237         3.288758         4.690904    -1.335196    -0.481920    -2.002434  \n",
       "238         3.288758         4.690904    -1.335196    -0.481920    -2.002434  \n",
       "239         3.288758         4.690904    -1.335196    -0.481920    -2.002434  \n",
       "\n",
       "[240 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = features_test.iloc[:, :-2]\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsd2 = features_df.iloc[0:, 0:3]\n",
    "dsd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsd2.values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdh2 = train_danceMove_Alex.a_xList[0]\n",
    "sdh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag33 = []\n",
    "# for row in range(dsd2.shape[0]):\n",
    "for i in range(dsd2.shape[0]):\n",
    "    x, y, z = dsd2.values[i]\n",
    "#     print(x, y, z)\n",
    "    mag33.append(sqrt(pow(x, 2) + pow(y, 2) + pow(z, 2)))\n",
    "mag33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis(sdh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew(train_danceMove_Alex.a_xList[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew([1,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data22 = []\n",
    "data_temp22 = []\n",
    "for col in range(x.shape[1]):\n",
    "    for row in range(x.shape[0]):\n",
    "        data_temp.append(np.fft.fft(x.iloc[row, col]))\n",
    "        if row == x.shape[0]-1:\n",
    "            data.append(data_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['Dancer'] = 'Alex'\n",
    "features_df['Move'] = train_danceMove_Alex.movename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(train_danceMove_Alex.g_zList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df.insert(, 'Dancer', 'Alex')\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.insert(4, 'Move', train_danceMove_Alex.movename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = features_df.drop('Dancer', 1)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup - working and good!\n",
    "def perform_mlp(X_val, y_val, fold, pca):\n",
    "    start_time = timer()\n",
    "    k = fold\n",
    "    perform_pca = pca\n",
    "    number_of_classes = 3\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    #kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    \n",
    "# # Trial 3: ave accuracy 81%\n",
    "#     mlp_adam = nn.MLPClassifier(hidden_layer_sizes=(200, 150), max_iter=500, activation='tanh', solver='adam',\n",
    "#                                       batch_size = minSamples, validation_fraction=0.2, n_iter_no_change=20,\n",
    "#                                       alpha=1e-4, early_stopping=True, verbose=0, tol=1e-6, random_state=None, \n",
    "#                                       learning_rate_init=0.001, shuffle=False) \n",
    "\n",
    "    acc_scores = []\n",
    "    cv_iteration = 1\n",
    "    cv_pca_iteration = 1\n",
    "    train_histories.clear()\n",
    "    cm_hist.clear()\n",
    "    classification_report_hist.clear()\n",
    "#     train_histories_mlp_sgd.clear()\n",
    "#     train_histories_mlp_adam.clear()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        if perform_pca == False:\n",
    "            print('\\nTraining model and cross validate using fold #{}...\\n ' .format(cv_iteration))\n",
    "            cv_iteration += 1\n",
    "        \n",
    "        X_train , X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train , y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         print('X_train', X_train)\n",
    "#         print('X_test', X_test)\n",
    "\n",
    "#         print('X_train and X_test shape before standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "\n",
    "        y_test_without_transform = y_test\n",
    "\n",
    "#         print('X_train before standard scaler: ', X_train)\n",
    "#         print('X_test before standard scaler: ', X_test)        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_val_std_scaler = scaler.transform(X_val)\n",
    "        \n",
    "#         print('X_train and X_test shape after standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#         X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)        \n",
    "\n",
    "        \n",
    "        y_train = to_categorical(y_train, number_of_classes)\n",
    "        y_test = to_categorical(y_test, number_of_classes)\n",
    "        y_val_categorical = to_categorical(y_val, number_of_classes)\n",
    "        \n",
    "#         X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)     \n",
    "        \n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "        if perform_pca == True:\n",
    "            print('\\nTraining model with PCA and cross validate using fold #{}...\\n ' .format(cv_pca_iteration))\n",
    "            cv_pca_iteration += 1\n",
    "            pca = PCA(n_components = 4) # n=100 is the usual consensus in HAR\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "            pca.explained_variance_ratio_\n",
    "\n",
    "        def mlp_model():\n",
    "            model = Sequential()\n",
    "#             model.add(Flatten(input_shape=X_train[0].shape))\n",
    "            model.add(Dense(units=64, kernel_initializer='uniform', activation='relu', input_shape=X_train[0].shape))\n",
    "#             model.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(units=number_of_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            return model\n",
    "        \n",
    "        mlp = mlp_model()\n",
    "        print(mlp.summary())        \n",
    "        \n",
    "#         checkpoint_filepath=\"MLP_weights_checkpoint.hdf5\"\n",
    "                \n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.00001, patience=20, verbose=1),\n",
    "#             ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only=True, monitor='val_accuracy',\n",
    "#                             verbose=1, save_best_only=True, mode='max')  \n",
    "        ] \n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test, y_test),\n",
    "                                  callbacks=[my_callbacks])\n",
    "        \n",
    "        mlp_pred = np.argmax(mlp.predict(X_test), axis=-1)\n",
    "        scores = mlp.evaluate(X_test, y_test, batch_size=64, verbose=0)\n",
    "        acc_scores.append(scores[1])\n",
    "        train_histories.append(history.history)\n",
    "        \n",
    "        mlp_weights = mlp.get_weights()\n",
    "        print(\"MLP Weights:\", mlp_weights)\n",
    "        \n",
    "#         mlp.save('saved_models/MLP_99.6_accuracy')\n",
    "        \n",
    "        print('y_test\\n', y_test_without_transform)\n",
    "        print('')\n",
    "        print('mlp_pred\\n', mlp_pred)\n",
    "        \n",
    "        cm_hist.append(confusion_matrix(y_test_without_transform, mlp_pred))\n",
    "        classification_report_hist.append(classification_report(y_test_without_transform, mlp_pred, target_names=encoder.classes_))\n",
    "        \n",
    "\n",
    "    end_time = timer()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return mlp, acc_scores, time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% accuracy -- bug exists somewhere\n",
    "def perform_mlp(X_test, y_test, fold, pca):\n",
    "    start_time = timer()\n",
    "    k = fold\n",
    "    perform_pca = pca\n",
    "    number_of_classes = 3\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    \n",
    "    #kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    \n",
    "# # Trial 3: ave accuracy 81%\n",
    "#     mlp_adam = nn.MLPClassifier(hidden_layer_sizes=(200, 150), max_iter=500, activation='tanh', solver='adam',\n",
    "#                                       batch_size = minSamples, validation_fraction=0.2, n_iter_no_change=20,\n",
    "#                                       alpha=1e-4, early_stopping=True, verbose=0, tol=1e-6, random_state=None, \n",
    "#                                       learning_rate_init=0.001, shuffle=False) \n",
    "\n",
    "    acc_scores = []\n",
    "    cv_iteration = 1\n",
    "    cv_pca_iteration = 1\n",
    "    train_histories.clear()\n",
    "    cm_hist.clear()\n",
    "    classification_report_hist.clear()\n",
    "#     train_histories_mlp_sgd.clear()\n",
    "#     train_histories_mlp_adam.clear()\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        \n",
    "        if perform_pca == False:\n",
    "            print('\\nTraining model and cross validate using fold #{}...\\n ' .format(cv_iteration))\n",
    "            cv_iteration += 1\n",
    "        \n",
    "        X_train , X_val = X.iloc[train_index,:], X.iloc[val_index,:]\n",
    "        y_train , y_val = y[train_index], y[val_index]\n",
    "        \n",
    "#         print('X_train', X_train)\n",
    "#         print('X_test', X_test)\n",
    "\n",
    "#         print('X_train and X_test shape before standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "\n",
    "        y_val_without_transform = y_val\n",
    "\n",
    "#         print('X_train before standard scaler: ', X_train)\n",
    "#         print('X_test before standard scaler: ', X_test)        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test_std_scaler = scaler.transform(X_test)\n",
    "        \n",
    "#         print('X_train and X_test shape after standard scaler:')\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#         X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)        \n",
    "\n",
    "        \n",
    "        y_train = to_categorical(y_train, number_of_classes)\n",
    "        y_val = to_categorical(y_val, number_of_classes)\n",
    "        y_test_categorical = to_categorical(y_test, number_of_classes)\n",
    "        \n",
    "#         X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "#         X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)     \n",
    "        \n",
    "#         print('X_train shape:', X_train.shape)\n",
    "#         print('X_test shape:', X_test.shape)\n",
    "        \n",
    "        if perform_pca == True:\n",
    "            print('\\nTraining model with PCA and cross validate using fold #{}...\\n ' .format(cv_pca_iteration))\n",
    "            cv_pca_iteration += 1\n",
    "            pca = PCA(n_components = 4) # n=100 is the usual consensus in HAR\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_val = pca.transform(X_val)\n",
    "            pca.explained_variance_ratio_\n",
    "\n",
    "        def mlp_model():\n",
    "            model = Sequential()\n",
    "#             model.add(Flatten(input_shape=X_train[0].shape))\n",
    "            model.add(Dense(units=32, kernel_initializer='uniform', activation='relu', input_shape=X_train[0].shape))\n",
    "#             model.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "#             model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(units=number_of_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            return model\n",
    "        \n",
    "        mlp = mlp_model()\n",
    "        print(mlp.summary())        \n",
    "        \n",
    "#         checkpoint_filepath=\"MLP_weights_checkpoint.hdf5\"\n",
    "                \n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20),\n",
    "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=0.00001, patience=20, verbose=1),\n",
    "#             ModelCheckpoint(filepath = checkpoint_filepath, save_weights_only=True, monitor='val_accuracy',\n",
    "#                             verbose=1, save_best_only=True, mode='max')  \n",
    "        ] \n",
    "        \n",
    "        history = mlp.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_val, y_val),\n",
    "                                  callbacks=[my_callbacks])\n",
    "        \n",
    "        mlp_pred = np.argmax(mlp.predict(X_test_std_scaler), axis=-1)\n",
    "        scores = mlp.evaluate(X_test_std_scaler, y_test_categorical, batch_size=64, verbose=0)\n",
    "        acc_scores.append(scores[1])\n",
    "        train_histories.append(history.history)\n",
    "        \n",
    "#         mlp_weights = mlp.get_weights()\n",
    "#         print(\"MLP Weights:\", mlp_weights)\n",
    "        \n",
    "#         mlp.save('saved_models/MLP_99.6_accuracy')\n",
    "        \n",
    "        print('y_test\\n', y_test)\n",
    "        print('')\n",
    "        print('mlp_pred\\n', mlp_pred)\n",
    "        \n",
    "        cm_hist.append(confusion_matrix(y_test, mlp_pred))\n",
    "        classification_report_hist.append(classification_report(y_test, mlp_pred, target_names=encoder.classes_))\n",
    "        \n",
    "\n",
    "    end_time = timer()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return mlp, acc_scores, time_taken"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
